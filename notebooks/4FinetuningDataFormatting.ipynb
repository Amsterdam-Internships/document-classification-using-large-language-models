{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "Goal: turn dataframe with txt files, into dataset format, to finetune GEITje.\n",
    "The dataset is save to huggingface.\n",
    "\n",
    "Funtion allows to truncate text. txtfiles_tokenizer, should already indlue columns MistralTokens and LlamaTokens, with the tokens split using the models tokenizer.\n",
    "\n",
    "*Previous notebook: text_truncation*\n",
    "\n",
    "*Next notebook: finetuning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load example data**\n",
    "\n",
    "Datasets used in Finetuning.py example of the GitHub of GEITje. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test_sft: Dataset({\n",
      "        features: ['prompt', 'prompt_id', 'messages', 'category', 'messages_nl'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    train_sft: Dataset({\n",
      "        features: ['prompt', 'prompt_id', 'messages', 'category', 'messages_nl'],\n",
      "        num_rows: 9500\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    test_sft: Dataset({\n",
      "        features: ['prompt', 'prompt_id', 'messages', 'messages_nl'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    train_sft: Dataset({\n",
      "        features: ['prompt', 'prompt_id', 'messages', 'messages_nl'],\n",
      "        num_rows: 9500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets,Dataset\t\n",
    "\n",
    "\n",
    "no_robots_nl = load_dataset('Rijgersberg/no_robots_nl')\n",
    "print(no_robots_nl)\n",
    "no_robots_nl[\"train_sft\"]=no_robots_nl[\"train_sft\"].select(range(2))\n",
    "no_robots_nl[\"test_sft\"]=no_robots_nl[\"test_sft\"].select(range(2))\n",
    "\n",
    "ultrachat_nl = load_dataset('Rijgersberg/ultrachat_10k_nl')\n",
    "print(ultrachat_nl)\n",
    "ultrachat_nl[\"train_sft\"]=ultrachat_nl[\"train_sft\"].select(range(2))\n",
    "ultrachat_nl[\"test_sft\"]=ultrachat_nl[\"test_sft\"].select(range(2))\n",
    "\n",
    "chat_dataset = DatasetDict({\n",
    "    'train_sft': concatenate_datasets([no_robots_nl['train_sft'],\n",
    "                                        ultrachat_nl['train_sft']]).shuffle(seed=42),\n",
    "    'test_sft': concatenate_datasets([no_robots_nl['test_sft'],\n",
    "                                        ultrachat_nl['test_sft']]).shuffle(seed=42),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspection**: we only need the columns prompt_id (equal to doc id) and messages_nl. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text using GEITje/Mistral tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load amsterdam data     \n",
    "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets,Dataset\t\n",
    "import sys\n",
    "sys.path.append('../src/') \n",
    "import prompt_template as pt\n",
    "\n",
    "def format_message(input_txt, label):\n",
    "    message_user = {\n",
    "        \"content\":pt.zeroshot_prompt_mistral_llama(input_txt, remove_template=True),\n",
    "        'role':'user'\n",
    "    }\n",
    "\n",
    "    message_model = {\n",
    "        \"content\":f\"{{'categorie': {label}}}\",\n",
    "        'role':'assistant'\n",
    "    }\n",
    "\n",
    "    return [message_user, message_model]\n",
    "\n",
    "\n",
    "\n",
    "def format_data(df, text_col, model_token_col, label_col, split_col,  token_threshold='full_text'):\n",
    "    format_df = pd.DataFrame(columns=['prompt_id', 'message', split_col])\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        # select whole text\n",
    "        if token_threshold == 'full_text':\n",
    "            input_txt = row[text_col]\n",
    "\n",
    "        # else select text according to the token threshold\n",
    "        else:\n",
    "            first_tokens_threshold = token_threshold[0]\n",
    "            last_tokens_threshold = token_threshold[1]\n",
    "\n",
    "            # select first tokens\n",
    "            tokens = row[model_token_col][0:first_tokens_threshold]\n",
    "\n",
    "            # combine tokens into txt\n",
    "            tokens_txt = ''.join(tokens)\n",
    "\n",
    "            # \\n is converted by tokenizer to <0x0A>, we reverse this to get original length\n",
    "            len_char = len(tokens_txt.replace(\"<0x0A>\", \"\\n\")) # get character length\n",
    "\n",
    "            # select the same amount of characters as the tokens\n",
    "            front_txt = row[text_col][0:len_char]\n",
    "\n",
    "            # Check if back of document also given as input\n",
    "            if last_tokens_threshold != 0:\n",
    "                # select LAST n (= token_theshold) tokens using the model tokenizer\n",
    "                tokens = row[model_token_col][-last_tokens_threshold:]\n",
    "\n",
    "                # combine tokens into txt\n",
    "                tokens_txt = ''.join(tokens)\n",
    "\n",
    "                # \\n is converted by tokenizer to <0x0A>, we reverse this to get original length\n",
    "                len_char = len(tokens_txt.replace(\"<0x0A>\", \"\\n\")) # get character length\n",
    "\n",
    "                # select the same amount of characters as the tokens\n",
    "                back_txt = row[text_col][-len_char:]\n",
    "\n",
    "                # combine front and back text\n",
    "                input_txt = front_txt + ' ' + back_txt\n",
    "\n",
    "            else:\n",
    "                input_txt = front_txt\n",
    "        \n",
    "        # format message\n",
    "        label = row[label_col]\n",
    "        message = format_message(input_txt, label)\n",
    "\n",
    "        # save in dataframe\n",
    "        format_df.loc[len(format_df)] = {'prompt_id':row['id'], 'message':message, split_col:row[split_col]}\n",
    "\n",
    "    # split data\n",
    "    train_set = format_df.loc[format_df[split_col]=='train'].drop(columns=[split_col])\n",
    "    test_set = format_df.loc[format_df[split_col]=='test'].drop(columns=[split_col])\n",
    "\n",
    "\n",
    "    if split_col == '4split':\n",
    "        dev_set = format_df.loc[format_df[split_col]=='dev'].drop(columns=[split_col])\n",
    "        val_set = format_df.loc[format_df[split_col]=='val'].drop(columns=[split_col])\n",
    "\n",
    "        chat_dataset = DatasetDict({\n",
    "            'train': Dataset.from_pandas(train_set).remove_columns('__index_level_0__'),\n",
    "            'test': Dataset.from_pandas(test_set).remove_columns('__index_level_0__'),\n",
    "            'dev': Dataset.from_pandas(dev_set).remove_columns('__index_level_0__'),\n",
    "            'val': Dataset.from_pandas(val_set).remove_columns('__index_level_0__')\n",
    "        })\n",
    "\n",
    "    elif split_col == '2split':\n",
    "            chat_dataset = DatasetDict({\n",
    "            'train': Dataset.from_pandas(train_set).remove_columns('__index_level_0__'),\n",
    "            'test': Dataset.from_pandas(test_set).remove_columns('__index_level_0__'),\n",
    "        })\n",
    "\n",
    "    elif split_col == 'balanced_split':\n",
    "        val_set = format_df.loc[format_df[split_col]=='val'].drop(columns=[split_col])\n",
    "        discarded_set = format_df.loc[format_df[split_col]=='discard'].drop(columns=[split_col])\n",
    "\n",
    "        chat_dataset = DatasetDict({\n",
    "            'train': Dataset.from_pandas(train_set).remove_columns('__index_level_0__'),\n",
    "            'test': Dataset.from_pandas(test_set).remove_columns('__index_level_0__'),\n",
    "            'val': Dataset.from_pandas(val_set).remove_columns('__index_level_0__'),\n",
    "            'discard': Dataset.from_pandas(discarded_set).remove_columns('__index_level_0__')\n",
    "        })\n",
    "\n",
    "\n",
    "    return chat_dataset\n",
    "\n",
    "\n",
    "data = format_data(df, 'text', 'LlamaTokens', 'label', 'balanced_split', [200,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.push_to_hub(\"FemkeBakker/AmsterdamBalancedFirst200Tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dataset('FemkeBakker/AmsterdamBalancedFirst200Tokens')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
