{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Select where to run notebook: \"azure\" or \"local\"\n",
        "my_run = \"azure\"\n",
        "\n",
        "import my_secrets as sc\n",
        "import settings as st\n",
        "\n",
        "if my_run == \"azure\":\n",
        "    import config_azure as cf\n",
        "elif my_run == \"local\":\n",
        "    import config as cf\n",
        "\n",
        "\n",
        "import os\n",
        "if my_run == \"azure\":\n",
        "    if not os.path.exists(cf.HUGGING_CACHE):\n",
        "        os.mkdir(cf.HUGGING_CACHE)\n",
        "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
        "\n",
        "\n",
        "# setup environment GEITje-7B Finetuning\n",
        "# - pip install torch\n",
        "# - pip install datasets\n",
        "# - pip install transformers\n",
        "# - pip install trl\n",
        "# - pip install accelerate (restart after)\n",
        "# - switch device_map='auto' to avaoid memory error\n",
        "\n",
        "# - pip install sentencepiece\n",
        "# - pip install jupyter\n",
        "# - pip install protobuf \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook overview\n",
        "This notebook creates predictions for the baseline models. In total, five models are tried out.\n",
        "- Training function. Given a baseline model, will return scores.\n",
        "- Load Data. Load all the documents, and set parameters.\n",
        "- save predictions\n",
        "\n",
        "\n",
        "Kernel: Pytorch and Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load file with training funcation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../scripts/') \n",
        "import baseline as bf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n",
        "df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'train': 16445, 'test': 4373})\n",
            "Counter({'train': 15613, 'test': 4164, 'dev': 832, 'val': 209})\n",
            "Counter({'train': 9900, 'discard': 8718, 'test': 1100, 'val': 1100})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "sys.path.append('../scripts/') \n",
        "import baseline as bf\n",
        "from truncation import add_truncation_column\n",
        "\n",
        "print(Counter(df['2split']))\n",
        "print(Counter(df['4split']))\n",
        "print(Counter(df['balanced_split']))\n",
        "\n",
        "#set  variables, same for each model\n",
        "SPLIT_COLUMN = 'balanced_split' #column that has the data split saved. must be either 2split, 4split or balanced_split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "TRAIN_SET = 'train' # must be dev or train\n",
        "TEST_SET = 'test' # must be val or test\n",
        "# this split column, train_set and test_set might be a bit confusing. The split_column need to have values about the split, so a row either belongs, in my case, to 'train', 'test', 'dev' or 'val'.\n",
        "# Then the train_set indates which rows will be selected based on the filtering of the split column. \n",
        "# Thus if TRAIN_SET = 'train', then all rows where split_col is 'train', will be selected as the training set.\n",
        "# The same goes for TEST_SET    \n",
        "\n",
        "\n",
        "TEXT_COLUMN = 'text' # column where the text is\n",
        "LABEL_COLUMN = 'label' # column with truth label\n",
        "DATAFRAME = df.copy() # df where each rows is a doc. \n",
        "PATH = f\"{cf.output_path}/predictionsFinal/baselines/predictions.pkl\" # path where each individual prediction is saved\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/baselines/overview.pkl\" # path where score and extra data about run is saved\n",
        "\n",
        "# needed for truncation experiment on baselines\n",
        "TRUNC_COLUMN = 'trunc_txt'\n",
        "TOKENS_COL = 'LlamaTokens'\n",
        "THRESHOLD_COMBINATIONS =[(100,0), (200,0), (100,100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_truncation_on_baselines(baseline_function, model_name):\n",
        "    for thresholds in THRESHOLD_COMBINATIONS:\n",
        "\n",
        "        # select thresholds\n",
        "        front_threshold = thresholds[0]\n",
        "        back_threshold = thresholds[1]\n",
        "\n",
        "        # set run_id\n",
        "        run_id = f\"{model_name}_first{front_threshold}_last{back_threshold}\"\n",
        "\n",
        "        # get df with truncated text column\n",
        "        trunc = add_truncation_column(DATAFRAME, TEXT_COLUMN, TOKENS_COL, front_threshold,back_threshold)\n",
        "\n",
        "        # train and get predictions\n",
        "        bf.run_baseline(baseline_function, model_name, trunc, SPLIT_COLUMN, TRAIN_SET, TEST_SET, TRUNC_COLUMN, LABEL_COLUMN, PATH, OVERVIEW_PATH, run_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Baseline 1: linear SVM+tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "        Actualiteit       1.00      0.79      0.88       100\n",
            "             Agenda       0.95      0.99      0.97       100\n",
            "            Besluit       0.98      0.97      0.97       100\n",
            "              Brief       0.95      0.98      0.97       100\n",
            "          Factsheet       1.00      0.46      0.63       100\n",
            "              Motie       0.97      0.95      0.96       100\n",
            "  Onderzoeksrapport       0.64      0.94      0.76       100\n",
            "         Raadsadres       0.81      1.00      0.89       100\n",
            "       Raadsnotulen       1.00      1.00      1.00       100\n",
            "Schriftelijke Vraag       0.96      0.93      0.94       100\n",
            "         Voordracht       0.96      0.99      0.98       100\n",
            "\n",
            "           accuracy                           0.91      1100\n",
            "          macro avg       0.93      0.91      0.91      1100\n",
            "       weighted avg       0.93      0.91      0.91      1100\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "        Actualiteit       0.94      0.79      0.86       100\n",
            "             Agenda       0.93      0.99      0.96       100\n",
            "            Besluit       0.97      0.96      0.96       100\n",
            "              Brief       0.97      0.96      0.96       100\n",
            "          Factsheet       1.00      0.28      0.44       100\n",
            "              Motie       0.94      0.94      0.94       100\n",
            "  Onderzoeksrapport       0.56      0.95      0.71       100\n",
            "         Raadsadres       0.76      0.94      0.84       100\n",
            "       Raadsnotulen       1.00      0.98      0.99       100\n",
            "Schriftelijke Vraag       1.00      0.94      0.97       100\n",
            "         Voordracht       1.00      0.99      0.99       100\n",
            "\n",
            "           accuracy                           0.88      1100\n",
            "          macro avg       0.92      0.88      0.88      1100\n",
            "       weighted avg       0.92      0.88      0.88      1100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "model_name = 'LinearSVC'\n",
        "baseline_function = LinearSVC()\n",
        "run_id = f\"{model_name}_fulltext\"\n",
        "\n",
        "linear_svm = bf.run_baseline(baseline_function, model_name , DATAFRAME, SPLIT_COLUMN, TRAIN_SET, TEST_SET,TEXT_COLUMN, LABEL_COLUMN, PATH, OVERVIEW_PATH, run_id)\n",
        "\n",
        "run_truncation_on_baselines(baseline_function, model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Baseline 2: Naive Bayes+tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "        Actualiteit       0.00      0.00      0.00       100\n",
            "             Agenda       0.76      0.99      0.86       100\n",
            "            Besluit       1.00      0.27      0.43       100\n",
            "              Brief       1.00      0.11      0.20       100\n",
            "          Factsheet       0.00      0.00      0.00       100\n",
            "              Motie       0.83      0.89      0.86       100\n",
            "  Onderzoeksrapport       0.40      0.87      0.55       100\n",
            "         Raadsadres       0.43      0.92      0.59       100\n",
            "       Raadsnotulen       0.00      0.00      0.00       100\n",
            "Schriftelijke Vraag       0.34      0.96      0.51       100\n",
            "         Voordracht       0.85      0.99      0.91       100\n",
            "\n",
            "           accuracy                           0.55      1100\n",
            "          macro avg       0.51      0.55      0.45      1100\n",
            "       weighted avg       0.51      0.55      0.45      1100\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "naive_bayes = bf.run_baseline(MultinomialNB(), 'MultinomialNB' , DATAFRAME, SPLIT_COLUMN, TRAIN_SET, TEST_SET,TEXT_COLUMN, LABEL_COLUMN, PATH, OVERVIEW_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Baseline 3: Logistic Regression + tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "        Actualiteit       0.96      0.70      0.81       100\n",
            "             Agenda       0.94      0.99      0.97       100\n",
            "            Besluit       0.98      0.94      0.96       100\n",
            "              Brief       0.93      0.97      0.95       100\n",
            "          Factsheet       1.00      0.29      0.45       100\n",
            "              Motie       0.90      0.94      0.92       100\n",
            "  Onderzoeksrapport       0.62      0.92      0.74       100\n",
            "         Raadsadres       0.69      0.99      0.81       100\n",
            "       Raadsnotulen       1.00      0.96      0.98       100\n",
            "Schriftelijke Vraag       0.95      0.93      0.94       100\n",
            "         Voordracht       0.96      0.99      0.98       100\n",
            "\n",
            "           accuracy                           0.87      1100\n",
            "          macro avg       0.90      0.87      0.86      1100\n",
            "       weighted avg       0.90      0.87      0.86      1100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = bf.run_baseline(LogisticRegression(), 'LogisticRegression' , DATAFRAME, SPLIT_COLUMN, TRAIN_SET, TEST_SET,TEXT_COLUMN, LABEL_COLUMN, PATH, OVERVIEW_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Baseline 4: k Nearest Neigbors + tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "        Actualiteit       0.90      0.45      0.60       100\n",
            "             Agenda       0.89      0.98      0.93       100\n",
            "            Besluit       0.84      0.81      0.82       100\n",
            "              Brief       0.50      0.32      0.39       100\n",
            "          Factsheet       1.00      0.35      0.52       100\n",
            "              Motie       0.74      0.31      0.44       100\n",
            "  Onderzoeksrapport       0.25      0.88      0.39       100\n",
            "         Raadsadres       0.85      0.22      0.35       100\n",
            "       Raadsnotulen       0.55      1.00      0.71       100\n",
            "Schriftelijke Vraag       0.64      0.32      0.43       100\n",
            "         Voordracht       0.92      0.87      0.89       100\n",
            "\n",
            "           accuracy                           0.59      1100\n",
            "          macro avg       0.73      0.59      0.59      1100\n",
            "       weighted avg       0.73      0.59      0.59      1100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = bf.run_baseline(KNeighborsClassifier(), 'KNeighborsClassifier' , DATAFRAME, SPLIT_COLUMN, TRAIN_SET, TEST_SET,TEXT_COLUMN, LABEL_COLUMN, PATH, OVERVIEW_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Baseline 5: RandomForest + tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "        Actualiteit       0.99      0.70      0.82       100\n",
            "             Agenda       0.95      0.99      0.97       100\n",
            "            Besluit       1.00      0.90      0.95       100\n",
            "              Brief       0.94      0.95      0.95       100\n",
            "          Factsheet       1.00      0.18      0.31       100\n",
            "              Motie       0.94      0.90      0.92       100\n",
            "  Onderzoeksrapport       0.65      0.95      0.77       100\n",
            "         Raadsadres       0.54      1.00      0.70       100\n",
            "       Raadsnotulen       1.00      0.94      0.97       100\n",
            "Schriftelijke Vraag       0.99      0.91      0.95       100\n",
            "         Voordracht       0.97      0.99      0.98       100\n",
            "\n",
            "           accuracy                           0.86      1100\n",
            "          macro avg       0.91      0.86      0.84      1100\n",
            "       weighted avg       0.91      0.86      0.84      1100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest = bf.run_baseline(RandomForestClassifier(), 'RandomForestClassifier' , DATAFRAME, SPLIT_COLUMN, TRAIN_SET, TEST_SET,TEXT_COLUMN, LABEL_COLUMN, PATH, OVERVIEW_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overview of all runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "overview = pd.read_pickle(OVERVIEW_PATH)\n",
        "display(overview)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "2AmsterdamLLM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
