{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Select where to run notebook: \"azure\" or \"local\"\n",
        "my_run = \"azure\"\n",
        "\n",
        "import my_secrets as sc\n",
        "import settings as st\n",
        "\n",
        "if my_run == \"azure\":\n",
        "    import config_azure as cf\n",
        "elif my_run == \"local\":\n",
        "    import config as cf\n",
        "\n",
        "\n",
        "import os\n",
        "if my_run == \"azure\":\n",
        "    if not os.path.exists(cf.HUGGING_CACHE):\n",
        "        os.mkdir(cf.HUGGING_CACHE)\n",
        "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
        "\n",
        "\n",
        "# setup environment GEITje-7B Finetuning\n",
        "# - pip install torch\n",
        "# - pip install datasets\n",
        "# - pip install transformers\n",
        "# - pip install trl\n",
        "# - pip install accelerate (restart after)\n",
        "# - switch device_map='auto' to avaoid memory error\n",
        "\n",
        "# - pip install sentencepiece\n",
        "# - pip install jupyter\n",
        "# - pip install protobuf \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook overview\n",
        "This notebook creates predictions for the baseline models. In total, five model are tried out.\n",
        "- Functions to split the data. One function save the split as a column in txtfiles, the other loads the split.\n",
        "- Training function. Given a baseline model, will return scores.\n",
        "- Load Data. Load all the documents, and set parameters.\n",
        "- TODO: save predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data split functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "\"\"\"Function takes a dataframe and splits the data into train, test, val and dev set and save it.\n",
        "Only need to run it once, load_data_spli is used to get the right subsets.\n",
        "\n",
        "\"\"\"\n",
        "def save_split(df, save_to_path):\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Splitting temp into test (20%) and val_dev (5%)\n",
        "    test_df, val_dev_df = train_test_split(temp_df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Splitting val_dev into validation (1%) and development (4%)\n",
        "    val_df, dev_df = train_test_split(val_dev_df, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_df['split'] = 'train'\n",
        "    test_df['split'] = 'test'\n",
        "    val_df['split'] = 'val'\n",
        "    dev_df['split'] = 'dev'\n",
        "\n",
        "    # Combining the DataFrames\n",
        "    final_df = pd.concat([train_df, test_df, val_df, dev_df])\n",
        "\n",
        "    final_df.to_pickle(save_to_path)\n",
        "\n",
        "# txtfiles = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n",
        "# save_split(txtfiles, f\"{cf.output_path}/txtfiles.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function returns X and y set for either the train, val, test or dev set.\"\"\"\n",
        "def load_data_split(df, split_col,subset, label_col):\n",
        "    subdf = df.loc[df[split_col]==subset]\n",
        "    X = subdf.drop(columns=[label_col])\n",
        "    y = subdf[label_col]\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "  \n",
        "import sys\n",
        "sys.path.append('../scripts/') \n",
        "import prediction_helperfunctions as ph\n",
        "\n",
        "\n",
        "def run_baseline(baseline_function, dataframe,split_col, subset_train, subset_test, text_col, label_col):\n",
        "    ph.check_data_split_input(subset_train, subset_test)\n",
        "    X_train, y_train = load_data_split(dataframe,split_col,subset_train,label_col) \n",
        "    X_test, y_test = load_data_split(dataframe,split_col,subset_test,label_col) \n",
        "\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_tfidf_bin = vectorizer.fit_transform(X_train[text_col])\n",
        "    X_test_tfidf_bin = vectorizer.transform(X_test[text_col])\n",
        "\n",
        "    model = baseline_function\n",
        "\n",
        "    # Train the classifier on the training data\n",
        "    model.fit(X_train_tfidf_bin, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test_tfidf_bin)\n",
        "\n",
        "    # Calculate the accuracy of the classifier\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    print(report)\n",
        "\n",
        "    predictions = X_test.copy()\n",
        "    predictions[label_col] = y_test\n",
        "    predictions['prediction'] = y_pred\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n",
        "\n",
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = 'split'\n",
        "TEXT_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'\n",
        "DATAFRAME = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Baseline 1: linear SVM+tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "         Actualiteit       1.00      0.06      0.11        35\n",
            "              Agenda       0.79      0.87      0.83       134\n",
            "             Besluit       0.93      0.56      0.70        25\n",
            "               Brief       0.75      0.87      0.80        89\n",
            "          Factsheets       1.00      0.14      0.25         7\n",
            "               Motie       0.87      0.95      0.91       349\n",
            "   Onderzoeksrapport       0.52      0.41      0.45        37\n",
            "          Raadsadres       0.84      0.75      0.79        83\n",
            "        Raadsnotulen       1.00      0.54      0.70        13\n",
            "Schriftelijke Vragen       0.77      0.98      0.86       127\n",
            "       Termijnagenda       0.50      0.24      0.32        42\n",
            "          Voordracht       0.99      0.98      0.99       127\n",
            "\n",
            "            accuracy                           0.83      1068\n",
            "           macro avg       0.83      0.61      0.64      1068\n",
            "        weighted avg       0.83      0.83      0.81      1068\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "linear_svm = run_baseline(LinearSVC(), DATAFRAME, SPLIT_COLUMN, TRAIN_SET, TEST_SET,TEXT_COLUMN, LABEL_COLUMN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Baseline 2: Naive Bayes+tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "         Actualiteit       0.00      0.00      0.00        35\n",
            "              Agenda       0.80      0.71      0.75       134\n",
            "             Besluit       0.00      0.00      0.00        25\n",
            "               Brief       0.00      0.00      0.00        89\n",
            "          Factsheets       0.00      0.00      0.00         7\n",
            "               Motie       0.37      1.00      0.54       349\n",
            "   Onderzoeksrapport       0.00      0.00      0.00        37\n",
            "          Raadsadres       0.00      0.00      0.00        83\n",
            "        Raadsnotulen       0.00      0.00      0.00        13\n",
            "Schriftelijke Vragen       0.00      0.00      0.00       127\n",
            "       Termijnagenda       0.00      0.00      0.00        42\n",
            "          Voordracht       1.00      0.09      0.17       127\n",
            "\n",
            "            accuracy                           0.43      1068\n",
            "           macro avg       0.18      0.15      0.12      1068\n",
            "        weighted avg       0.34      0.43      0.29      1068\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "naive_bayes = run_baseline(MultinomialNB(), DATAFRAME, SPLIT_COLUMN, TRAIN_SET, TEST_SET,TEXT_COLUMN, LABEL_COLUMN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "precision, recall and f1-score equal to zero occurs if there are not True Positives. Meaning for those classes not one document is correctly predicted. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Baseline 3: Logistic Regression + tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "         Actualiteit       0.00      0.00      0.00        35\n",
            "              Agenda       0.82      0.89      0.85       134\n",
            "             Besluit       0.00      0.00      0.00        25\n",
            "               Brief       0.71      0.06      0.10        89\n",
            "          Factsheets       0.00      0.00      0.00         7\n",
            "               Motie       0.55      1.00      0.71       349\n",
            "   Onderzoeksrapport       0.75      0.16      0.27        37\n",
            "          Raadsadres       1.00      0.23      0.37        83\n",
            "        Raadsnotulen       0.00      0.00      0.00        13\n",
            "Schriftelijke Vragen       0.77      0.81      0.79       127\n",
            "       Termijnagenda       0.00      0.00      0.00        42\n",
            "          Voordracht       1.00      0.91      0.95       127\n",
            "\n",
            "            accuracy                           0.67      1068\n",
            "           macro avg       0.47      0.34      0.34      1068\n",
            "        weighted avg       0.65      0.67      0.59      1068\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = run_baseline(LogisticRegression(), DATAFRAME, SPLIT_COLUMN, TRAIN_SET, TEST_SET,TEXT_COLUMN, LABEL_COLUMN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Baseline 4: k Nearest Neigbors + tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "         Actualiteit       0.50      0.06      0.10        35\n",
            "              Agenda       0.73      0.68      0.70       134\n",
            "             Besluit       0.83      0.40      0.54        25\n",
            "               Brief       0.33      0.76      0.46        89\n",
            "          Factsheets       0.00      0.00      0.00         7\n",
            "               Motie       0.90      0.18      0.30       349\n",
            "   Onderzoeksrapport       0.15      0.73      0.24        37\n",
            "          Raadsadres       1.00      0.07      0.13        83\n",
            "        Raadsnotulen       0.00      0.00      0.00        13\n",
            "Schriftelijke Vragen       0.37      0.94      0.53       127\n",
            "       Termijnagenda       0.15      0.12      0.13        42\n",
            "          Voordracht       0.98      0.77      0.86       127\n",
            "\n",
            "            accuracy                           0.46      1068\n",
            "           macro avg       0.49      0.39      0.33      1068\n",
            "        weighted avg       0.70      0.46      0.43      1068\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = run_baseline(KNeighborsClassifier(), DATAFRAME, SPLIT_COLUMN, TRAIN_SET, TEST_SET,TEXT_COLUMN, LABEL_COLUMN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Baseline 5: RandomForest + tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "         Actualiteit       0.00      0.00      0.00        35\n",
            "              Agenda       0.75      0.92      0.83       134\n",
            "             Besluit       0.00      0.00      0.00        25\n",
            "               Brief       0.76      0.87      0.81        89\n",
            "          Factsheets       0.00      0.00      0.00         7\n",
            "               Motie       0.80      0.95      0.87       349\n",
            "   Onderzoeksrapport       0.72      0.70      0.71        37\n",
            "          Raadsadres       0.74      0.73      0.74        83\n",
            "        Raadsnotulen       0.00      0.00      0.00        13\n",
            "Schriftelijke Vragen       0.87      0.93      0.90       127\n",
            "       Termijnagenda       0.29      0.05      0.08        42\n",
            "          Voordracht       0.96      0.98      0.97       127\n",
            "\n",
            "            accuracy                           0.81      1068\n",
            "           macro avg       0.49      0.51      0.49      1068\n",
            "        weighted avg       0.73      0.81      0.76      1068\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "random_forest = run_baseline(RandomForestClassifier(), DATAFRAME, SPLIT_COLUMN, TRAIN_SET, TEST_SET,TEXT_COLUMN, LABEL_COLUMN)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
