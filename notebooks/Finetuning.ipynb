{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "# import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# setup environment GEITje-7B Finetuning\n",
    "# - pip install torch\n",
    "# - pip install datasets\n",
    "# - pip install transformers\n",
    "# - pip install trl\n",
    "# - pip install accelerate (restart after)\n",
    "# - switch device_map='auto' to avaoid memory error\n",
    "\n",
    "# - pip install sentencepiece\n",
    "# - pip install jupyter\n",
    "# - pip install protobuf \n",
    "# pip install bitsandbytes\n",
    "# pip install bnb\n",
    "# pip install wandb==0.13.3 --upgrade\n",
    "#pip install tensorboardX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84abb8720d64f059f4e3fc8ab44e431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning GEITje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GEITje formatted data\n",
    "from datasets import load_dataset\n",
    "chat_dataset = load_dataset('FemkeBakker/AmsterdamBalancedFirst200Tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Classificeer het document in Ã©Ã©n van de categoriÃ«n. CategoriÃ«n: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']. Categoriseer dit document: X Gemeente Amsterdam W B\\n% Raadscommissie voor Bouwen, Wonen, Wijkaanpak en Dierenwelzijn\\n% Agenda, woensdag 19 april 2017\\nHierbij wordt u uitgenodigd voor de openbare vergadering van de Raadscommissie\\nvoor Bouwen, Wonen, Wijkaanpak en Dierenwelzijn\\nTijd 09.00 uur tot 12.30 uur\\nLocatie De Rooszaal 0239, Stadhuis\\nAlgemeen\\n1 Opening procedureel gedeelte\\n2 Mededelingen\\n3 Vaststellen agenda\\n4 Conceptverslag van de openbare vergadering van de Raadscommissie WB d.d.\\n29 maart 2017\\ne Tekstuele \\nGeef de output in de vorm van een JSON file: {'categorie': categorie van het document}. \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_dataset['val']['message'][0][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_dataset[\"train\"]=chat_dataset[\"train\"].select(range(2))\n",
    "# chat_dataset[\"test\"]=chat_dataset[\"test\"].select(range(2))\n",
    "# chat_dataset[\"val\"]=chat_dataset[\"val\"].select(range(2))\n",
    "# chat_dataset[\"dev\"]=chat_dataset[\"dev\"].select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# basemodel_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
    "# basemodel_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "# basemodel_name = \"stabilityai/stablelm-2-1_6b\"\n",
    "basemodel_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel_name, torch_dtype=torch.bfloat16,\n",
    "                                                low_cpu_mem_usage=True, attn_implementation=\"sdpa\",\n",
    "                                                device_map='cpu')\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(basemodel_name)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "model.config.pad_token_id = tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Classificeer het document in Ã©Ã©n van de categoriÃ«n. CategoriÃ«n: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']. Categoriseer dit document: > Gemeente\n",
      "Amsterdam\n",
      "Motie\n",
      "Datum raadsvergadering 19 juli 2023\n",
      "Ingekomen onder nummer 456\n",
      "Status Aangenomen\n",
      "Onderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\n",
      "Kabamba en Krom inzake een grote schoonmaak voor iedereen op de\n",
      "wachtlijst Hulp bij Huishouden\n",
      "Onderwerp\n",
      "Een grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\n",
      "Aan de gemeenteraad\n",
      "Ondergetekenden hebben de eer voor te stellen:\n",
      "De Raad,\n",
      "Gehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\n",
      "Constaterende dat:\n",
      "e \n",
      "Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}. </s>\n",
      "<|assistant|>\n",
      "{'categorie': Motie}</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# geitje base\n",
    "print(tokenizer.apply_chat_template(chat_dataset['train']['message'][0], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Classificeer het document in Ã©Ã©n van de categoriÃ«n. CategoriÃ«n: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']. Categoriseer dit document: > Gemeente\n",
      "Amsterdam\n",
      "Motie\n",
      "Datum raadsvergadering 19 juli 2023\n",
      "Ingekomen onder nummer 456\n",
      "Status Aangenomen\n",
      "Onderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\n",
      "Kabamba en Krom inzake een grote schoonmaak voor iedereen op de\n",
      "wachtlijst Hulp bij Huishouden\n",
      "Onderwerp\n",
      "Een grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\n",
      "Aan de gemeenteraad\n",
      "Ondergetekenden hebben de eer voor te stellen:\n",
      "De Raad,\n",
      "Gehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\n",
      "Constaterende dat:\n",
      "e \n",
      "Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}. [/INST] {'categorie': Motie} </s>\n"
     ]
    }
   ],
   "source": [
    "# llama\n",
    "print(tokenizer.apply_chat_template(chat_dataset['train']['message'][0], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Classificeer het document in Ã©Ã©n van de categoriÃ«n. CategoriÃ«n: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']. Categoriseer dit document: > Gemeente\n",
      "Amsterdam\n",
      "Motie\n",
      "Datum raadsvergadering 19 juli 2023\n",
      "Ingekomen onder nummer 456\n",
      "Status Aangenomen\n",
      "Onderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\n",
      "Kabamba en Krom inzake een grote schoonmaak voor iedereen op de\n",
      "wachtlijst Hulp bij Huishouden\n",
      "Onderwerp\n",
      "Een grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\n",
      "Aan de gemeenteraad\n",
      "Ondergetekenden hebben de eer voor te stellen:\n",
      "De Raad,\n",
      "Gehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\n",
      "Constaterende dat:\n",
      "e \n",
      "Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}. </s>\n",
      "<|assistant|>\n",
      "{'categorie': Motie}</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# geitje chat\n",
    "print(tokenizer.apply_chat_template(chat_dataset['train']['message'][0], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def combine_and_save_df(model_df, save_to_path):\n",
    "    \n",
    "    # combine with earlier runs if exists\n",
    "    if os.path.exists(save_to_path):\n",
    "        original = pd.read_pickle(save_to_path)\n",
    "        model_df = pd.concat([original, model_df])\n",
    "\n",
    "    model_df.to_pickle(save_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import prediction_helperfunctions as ph\n",
    "\n",
    "def train(model, model_name, tokenizer, chat_dataset, chat_dataset_name, new_model_name, train_set, test_set, run_id='No_id', save_to_hub=True, resume=False):\n",
    "    start_time = time.time()\n",
    "    def format(examples):\n",
    "        return [tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "                for conversation in examples['message']]\n",
    "\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    steps_per_epoch = len(chat_dataset[train_set])\\\n",
    "                // (torch.cuda.device_count() * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = steps_per_epoch // 5\n",
    "\n",
    "    output_directory = f'{cf.output_path}/finetuning_output/AmsterdamDocClassificationGEITje200T'\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        optim='adamw_bnb_8bit',\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy='epoch',\n",
    "        bf16=False, #bf16=True require CUDA 11 -> original code bf16=True\n",
    "        output_dir=output_directory,\n",
    "        report_to=[\"tensorboard\", 'wandb'],\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        hub_model_id=new_model_name,\n",
    "        push_to_hub=True,\n",
    "        hub_private_repo=True,\n",
    "        hub_strategy='all_checkpoints',\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=8192,\n",
    "        train_dataset=chat_dataset[train_set],\n",
    "        eval_dataset=chat_dataset[test_set],\n",
    "        formatting_func=format,\n",
    "        neftune_noise_alpha=5,\n",
    "    )\n",
    "\n",
    "  \n",
    "    dict_info = {\n",
    "        'model':new_model_name,\n",
    "        'base_model':model_name,\n",
    "        'chat_dataset':chat_dataset_name,\n",
    "        'train_set':train_set,\n",
    "        'test_set': test_set,\n",
    "        'training_args': training_args,\n",
    "        'resume_from_checkpoint':resume,\n",
    "        'date':ph.get_datetime(),\n",
    "        'runtime': False,\n",
    "        'Error': False,\n",
    "        'run_id':run_id,\n",
    "        'save_to_hub':save_to_hub,\n",
    "        'output_dir': output_directory\n",
    "        }\n",
    "\n",
    "    data = pd.DataFrame(columns=dict_info.keys())\n",
    "\n",
    "    # trainer.train(resume_from_checkpoint=resume)\n",
    "    # if save_to_hub == True:\n",
    "    #     trainer.push_to_hub()\n",
    "    # return trainer\n",
    "\n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=resume)\n",
    "        if save_to_hub == True:\n",
    "            trainer.push_to_hub()\n",
    "            \n",
    "        dict_info['runtime'] = time.time()-start_time\n",
    "\n",
    "        data.loc[len(data)] = dict_info\n",
    "        combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "        print(\"Finished without error!\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        dict_info['Error'] = 'KeyboardInterrupt'        \n",
    "        dict_info['runtime'] = time.time()-start_time\n",
    "\n",
    "        data.loc[len(data)] = dict_info\n",
    "        combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "    except Exception  as e:\n",
    "        print(e)\n",
    "        dict_info['Error'] = e\n",
    "        dict_info['runtime'] = time.time()-start_time\n",
    "\n",
    "        data.loc[len(data)] = dict_info\n",
    "        combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "\n",
    "        model_df = pd.DataFrame(dict_info)\n",
    "        combine_and_save_df(model_df, f'{cf.output_path}/overview_models.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "resume_from_checkpoitn, gives error if last epoch was not fully run, because then files are missing but the folder exists, thus it throws an error. Removing the last checkpoint folder solves this, does mean the epoch need to restart completely. If this is a viable solution  depends on how long an epoch takes to train. Might be quite long. However if all files are in folder then it is fine. How long does it take before all files are saved in folder? Does this need the epoch to be completed?\n",
    "\n",
    "\n",
    "MAKE SURE: no previous checkpints of runs unrelated to current run are in output folder!!\n",
    "\n",
    "MAKE SURE: run_id is unique, for each seperate run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:257: UserWarning: You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override the one in the `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfemkebakker\u001b[0m (\u001b[33mthesisamsterdam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/femke-gpu-24cores-220ram/code/Users/f.bakker/document-classification-using-large-language-models/notebooks/wandb/run-20240603_071156-0v7i4pdh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/thesisamsterdam/huggingface/runs/0v7i4pdh\" target=\"_blank\">/home/azureuser/cloudfiles/code/blobfuse/raadsinformatie/processed_data/woo_document_classification/finetuning_output/AmsterdamDocClassificationMistral200T</a></strong> to <a href=\"https://wandb.ai/thesisamsterdam/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1236' max='1236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1236/1236 : < :, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1a431882af4e858f10016e62953d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86486f0fd6464a858a89aaa157df1f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a22d2ac4954c418b0337a10d8474b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1717398714.femke-gpu-24cores-220ram:   0%|          | 0.00/5.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished without error!\n"
     ]
    }
   ],
   "source": [
    "train(model, basemodel_name, tokenizer, chat_dataset, 'FemkeBakker/AmsterdamBalancedFirst200Tokens',\n",
    "          'FemkeBakker/AmsterdamDocClassificationGEITje200T3Epochs', 'train', 'val',  run_id=21, save_to_hub=True, resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888acfad41f44de5a33a9a0abf7d9cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/667 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6ed77dcea14a4f938bd03a7bf1c6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fe0bb726e04d2495efa16da19ee3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f10124a13b4d4ba75bd96a2f019219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, Conversation\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconversational\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFemkeBakker/AmsterdamDocClassificationGEITje200T2Epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moffload_buffers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/pipelines/base.py:283\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    285\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/modeling_utils.py:3511\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3510\u001b[0m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3511\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3522\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3523\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3524\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3527\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3530\u001b[0m ):\n\u001b[1;32m   3531\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/utils/hub.py:1040\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/utils/hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1884\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:539\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    537\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/urllib3/response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/urllib3/response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/anaconda/envs/2AmsterdamLLM/lib/python3.9/ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "\n",
    "model = pipeline(task='conversational', model='FemkeBakker/AmsterdamDocClassificationGEITje200T2Epochs',\n",
    "                   device_map='cpu', model_kwargs={'offload_buffers':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "260 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>base_model</th>\n",
       "      <th>chat_dataset</th>\n",
       "      <th>train_set</th>\n",
       "      <th>test_set</th>\n",
       "      <th>training_args</th>\n",
       "      <th>resume_from_checkpoint</th>\n",
       "      <th>date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>Error</th>\n",
       "      <th>run_id</th>\n",
       "      <th>save_to_hub</th>\n",
       "      <th>output_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-25 13:21:46.201696+02:00</td>\n",
       "      <td>0.876791</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-25 13:24:59.884223+02:00</td>\n",
       "      <td>0.274652</td>\n",
       "      <td>Error(s) in loading state_dict for MistralForC...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-25 13:25:57.527833+02:00</td>\n",
       "      <td>0.362344</td>\n",
       "      <td>No valid checkpoint found in output directory ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-25 13:26:24.369492+02:00</td>\n",
       "      <td>0.319212</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 112.00 M...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 08:47:21.945291+02:00</td>\n",
       "      <td>0.392476</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 08:47:57.151755+02:00</td>\n",
       "      <td>0.600505</td>\n",
       "      <td>string longer than 2147483647 bytes</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryoutGeitje2</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-26 08:56:52.283214+02:00</td>\n",
       "      <td>0.449522</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryoutGeitje2</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 09:26:12.329799+02:00</td>\n",
       "      <td>3.251497</td>\n",
       "      <td>[Errno 2] No such file or directory: '/home/az...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryoutGeitje2</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 09:29:06.636546+02:00</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 09:46:03.908296+02:00</td>\n",
       "      <td>0.364137</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 09:48:18.644406+02:00</td>\n",
       "      <td>0.791530</td>\n",
       "      <td>Expected input batch_size (2) to match target ...</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 11:28:02.756504+02:00</td>\n",
       "      <td>1.342644</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 11:48:12.624523+02:00</td>\n",
       "      <td>4.499735</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 11:53:35.436951+02:00</td>\n",
       "      <td>0.519014</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistral-community/Mistral-7B-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-30 13:32:55.123342+02:00</td>\n",
       "      <td>1.238507</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 22.00 Mi...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistral-community/Mistral-7B-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 10:14:42.103986+02:00</td>\n",
       "      <td>4.744335</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/LlamaSmallData200Tokens</td>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 10:22:35.180386+02:00</td>\n",
       "      <td>0.644924</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistral-community/Mistral-7B-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 11:30:37.119100+02:00</td>\n",
       "      <td>1.784342</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 112.00 M...</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 13:29:55.638421+02:00</td>\n",
       "      <td>4.780315</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 14:02:55.215348+02:00</td>\n",
       "      <td>0.597784</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 14:06:39.657276+02:00</td>\n",
       "      <td>1467.518128</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B-chat-v2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 14:33:40.318320+02:00</td>\n",
       "      <td>1367.711304</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/LlamaSmallData200Tokens</td>\n",
       "      <td>meta-llama/Llama-2-7b-chat-hf</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 15:01:21.038330+02:00</td>\n",
       "      <td>1040.267759</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-28 09:33:15.666775+02:00</td>\n",
       "      <td>1274.191825</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-05-28 09:54:46.992883+02:00</td>\n",
       "      <td>2.428063</td>\n",
       "      <td>No valid checkpoint found in output directory ...</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-05-28 09:57:30.580525+02:00</td>\n",
       "      <td>0.332646</td>\n",
       "      <td>No valid checkpoint found in output directory ...</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-28 09:57:47.191583+02:00</td>\n",
       "      <td>6866.744590</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-28 11:53:36.216185+02:00</td>\n",
       "      <td>15681.023710</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-05-28 16:17:16.520661+02:00</td>\n",
       "      <td>189.865374</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/AmsterdamDocClassificationGEITje200T</td>\n",
       "      <td>Rijgersberg/GEITje-7B-chat-v2</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-29 14:07:04.642821+02:00</td>\n",
       "      <td>2926.216714</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/AmsterdamDocClassificationMistral200T</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-29 15:36:45.085202+02:00</td>\n",
       "      <td>2879.482625</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/AmsterdamDocClassificationLlama200T</td>\n",
       "      <td>meta-llama/Llama-2-7b-chat-hf</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-29 16:44:49.778730+02:00</td>\n",
       "      <td>2459.635145</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/AmsterdamDocClassificationLlama200T</td>\n",
       "      <td>meta-llama/Llama-2-7b-chat-hf</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-05-29 17:32:34.996990+02:00</td>\n",
       "      <td>2435.851116</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/AmsterdamDocClassificationMistrall...</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-05-29 18:17:40.189426+02:00</td>\n",
       "      <td>2909.715319</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/AmsterdamDocClassificationMistral200T</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-05-29 19:20:43.997402+02:00</td>\n",
       "      <td>19.168457</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/AmsterdamDocClassificationMistral200T</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-05-29 19:21:12.049071+02:00</td>\n",
       "      <td>370.747607</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/AmsterdamDocClassificationGEITje200T</td>\n",
       "      <td>Rijgersberg/GEITje-7B-chat-v2</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-05-29 19:39:27.535325+02:00</td>\n",
       "      <td>2821.834527</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  \\\n",
       "0                 FemkeBakker/TryOutFinetuningGeitje   \n",
       "0                 FemkeBakker/TryOutFinetuningGeitje   \n",
       "0                 FemkeBakker/TryOutFinetuningGeitje   \n",
       "0                 FemkeBakker/TryOutFinetuningGeitje   \n",
       "0                 FemkeBakker/TryOutFinetuningGeitje   \n",
       "0                 FemkeBakker/TryOutFinetuningGeitje   \n",
       "0                          FemkeBakker/TryoutGeitje2   \n",
       "0                          FemkeBakker/TryoutGeitje2   \n",
       "0                          FemkeBakker/TryoutGeitje2   \n",
       "0                         FemkeBakker/tryoutstablelm   \n",
       "0                         FemkeBakker/tryoutstablelm   \n",
       "0               FemkeBakker/GEITjeSmallData200Tokens   \n",
       "0               FemkeBakker/GEITjeSmallData200Tokens   \n",
       "0               FemkeBakker/GEITjeSmallData200Tokens   \n",
       "0              FemkeBakker/MistralSmallData200Tokens   \n",
       "0              FemkeBakker/MistralSmallData200Tokens   \n",
       "0                FemkeBakker/LlamaSmallData200Tokens   \n",
       "0              FemkeBakker/MistralSmallData200Tokens   \n",
       "0              FemkeBakker/MistralSmallData200Tokens   \n",
       "0              FemkeBakker/MistralSmallData200Tokens   \n",
       "0              FemkeBakker/MistralSmallData200Tokens   \n",
       "0               FemkeBakker/GEITjeSmallData200Tokens   \n",
       "0                FemkeBakker/LlamaSmallData200Tokens   \n",
       "0                         FemkeBakker/tryoutstablelm   \n",
       "0                         FemkeBakker/tryoutstablelm   \n",
       "0                         FemkeBakker/tryoutstablelm   \n",
       "0                         FemkeBakker/tryoutstablelm   \n",
       "0                         FemkeBakker/tryoutstablelm   \n",
       "0                         FemkeBakker/tryoutstablelm   \n",
       "0   FemkeBakker/AmsterdamDocClassificationGEITje200T   \n",
       "0  FemkeBakker/AmsterdamDocClassificationMistral200T   \n",
       "0    FemkeBakker/AmsterdamDocClassificationLlama200T   \n",
       "0    FemkeBakker/AmsterdamDocClassificationLlama200T   \n",
       "0  FemkeBakker/AmsterdamDocClassificationMistrall...   \n",
       "0  FemkeBakker/AmsterdamDocClassificationMistral200T   \n",
       "0  FemkeBakker/AmsterdamDocClassificationMistral200T   \n",
       "0   FemkeBakker/AmsterdamDocClassificationGEITje200T   \n",
       "\n",
       "                           base_model  \\\n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0         stabilityai/stablelm-2-1_6b   \n",
       "0         stabilityai/stablelm-2-1_6b   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0               Rijgersberg/GEITje-7B   \n",
       "0   mistral-community/Mistral-7B-v0.2   \n",
       "0   mistral-community/Mistral-7B-v0.2   \n",
       "0            meta-llama/Llama-2-7b-hf   \n",
       "0   mistral-community/Mistral-7B-v0.2   \n",
       "0  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "0  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "0  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "0       Rijgersberg/GEITje-7B-chat-v2   \n",
       "0       meta-llama/Llama-2-7b-chat-hf   \n",
       "0         stabilityai/stablelm-2-1_6b   \n",
       "0         stabilityai/stablelm-2-1_6b   \n",
       "0         stabilityai/stablelm-2-1_6b   \n",
       "0         stabilityai/stablelm-2-1_6b   \n",
       "0         stabilityai/stablelm-2-1_6b   \n",
       "0         stabilityai/stablelm-2-1_6b   \n",
       "0       Rijgersberg/GEITje-7B-chat-v2   \n",
       "0  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "0       meta-llama/Llama-2-7b-chat-hf   \n",
       "0       meta-llama/Llama-2-7b-chat-hf   \n",
       "0  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "0  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "0  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "0       Rijgersberg/GEITje-7B-chat-v2   \n",
       "\n",
       "                                  chat_dataset train_set test_set  \\\n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train     test   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train     test   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train     test   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train     test   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train      val   \n",
       "\n",
       "                                       training_args  resume_from_checkpoint  \\\n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "\n",
       "                              date       runtime  \\\n",
       "0 2024-04-25 13:21:46.201696+02:00      0.876791   \n",
       "0 2024-04-25 13:24:59.884223+02:00      0.274652   \n",
       "0 2024-04-25 13:25:57.527833+02:00      0.362344   \n",
       "0 2024-04-25 13:26:24.369492+02:00      0.319212   \n",
       "0 2024-04-26 08:47:21.945291+02:00      0.392476   \n",
       "0 2024-04-26 08:47:57.151755+02:00      0.600505   \n",
       "0 2024-04-26 08:56:52.283214+02:00      0.449522   \n",
       "0 2024-04-26 09:26:12.329799+02:00      3.251497   \n",
       "0 2024-04-26 09:29:06.636546+02:00      0.543515   \n",
       "0 2024-04-29 09:46:03.908296+02:00      0.364137   \n",
       "0 2024-04-29 09:48:18.644406+02:00      0.791530   \n",
       "0 2024-04-29 11:28:02.756504+02:00      1.342644   \n",
       "0 2024-04-29 11:48:12.624523+02:00      4.499735   \n",
       "0 2024-04-29 11:53:35.436951+02:00      0.519014   \n",
       "0 2024-04-30 13:32:55.123342+02:00      1.238507   \n",
       "0 2024-05-02 10:14:42.103986+02:00      4.744335   \n",
       "0 2024-05-02 10:22:35.180386+02:00      0.644924   \n",
       "0 2024-05-02 11:30:37.119100+02:00      1.784342   \n",
       "0 2024-05-02 13:29:55.638421+02:00      4.780315   \n",
       "0 2024-05-02 14:02:55.215348+02:00      0.597784   \n",
       "0 2024-05-02 14:06:39.657276+02:00   1467.518128   \n",
       "0 2024-05-02 14:33:40.318320+02:00   1367.711304   \n",
       "0 2024-05-02 15:01:21.038330+02:00   1040.267759   \n",
       "0 2024-05-28 09:33:15.666775+02:00   1274.191825   \n",
       "0 2024-05-28 09:54:46.992883+02:00      2.428063   \n",
       "0 2024-05-28 09:57:30.580525+02:00      0.332646   \n",
       "0 2024-05-28 09:57:47.191583+02:00   6866.744590   \n",
       "0 2024-05-28 11:53:36.216185+02:00  15681.023710   \n",
       "0 2024-05-28 16:17:16.520661+02:00    189.865374   \n",
       "0 2024-05-29 14:07:04.642821+02:00   2926.216714   \n",
       "0 2024-05-29 15:36:45.085202+02:00   2879.482625   \n",
       "0 2024-05-29 16:44:49.778730+02:00   2459.635145   \n",
       "0 2024-05-29 17:32:34.996990+02:00   2435.851116   \n",
       "0 2024-05-29 18:17:40.189426+02:00   2909.715319   \n",
       "0 2024-05-29 19:20:43.997402+02:00     19.168457   \n",
       "0 2024-05-29 19:21:12.049071+02:00    370.747607   \n",
       "0 2024-05-29 19:39:27.535325+02:00   2821.834527   \n",
       "\n",
       "                                               Error  run_id save_to_hub  \\\n",
       "0                                  KeyboardInterrupt       0         NaN   \n",
       "0  Error(s) in loading state_dict for MistralForC...       0         NaN   \n",
       "0  No valid checkpoint found in output directory ...       0         NaN   \n",
       "0  CUDA out of memory. Tried to allocate 112.00 M...       0         NaN   \n",
       "0                                  KeyboardInterrupt       0         NaN   \n",
       "0                string longer than 2147483647 bytes       0         NaN   \n",
       "0                                              False       1         NaN   \n",
       "0  [Errno 2] No such file or directory: '/home/az...       1         NaN   \n",
       "0                                              False       1         NaN   \n",
       "0                                  KeyboardInterrupt       2         NaN   \n",
       "0  Expected input batch_size (2) to match target ...       2       False   \n",
       "0                                  KeyboardInterrupt       3        True   \n",
       "0                                  KeyboardInterrupt       4        True   \n",
       "0                                              False       4        True   \n",
       "0  CUDA out of memory. Tried to allocate 22.00 Mi...       5        True   \n",
       "0                                  KeyboardInterrupt       6        True   \n",
       "0                                              False       7        True   \n",
       "0  CUDA out of memory. Tried to allocate 112.00 M...       8        True   \n",
       "0                                              False       9        True   \n",
       "0                                  KeyboardInterrupt      10        True   \n",
       "0                                              False      10        True   \n",
       "0                                              False      11        True   \n",
       "0                                              False      12        True   \n",
       "0                                  KeyboardInterrupt      13        True   \n",
       "0  No valid checkpoint found in output directory ...      13        True   \n",
       "0  No valid checkpoint found in output directory ...      13        True   \n",
       "0                                  KeyboardInterrupt      13        True   \n",
       "0                                              False      14        True   \n",
       "0                                  KeyboardInterrupt      14        True   \n",
       "0                                              False      15        True   \n",
       "0                                              False      16        True   \n",
       "0                                              False      17        True   \n",
       "0                                              False      17        True   \n",
       "0                                              False      16        True   \n",
       "0                                  KeyboardInterrupt      16        True   \n",
       "0                                              False      16        True   \n",
       "0                                              False      15        True   \n",
       "\n",
       "                                          output_dir  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yeet = pd.read_pickle(f'{cf.output_path}/overview_models.pkl')\n",
    "display(yeet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import prediction_helperfunctions as ph\n",
    "\n",
    "def train(model, model_name, tokenizer, dataset, chat_dataset_name, new_model_name, train_set, test_set, run_id='No_id', save_to_hub=True, resume=False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    # tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_data = dataset[train_set].shuffle(seed=42).select(range(100))\n",
    "    train_data = train_data.map(tokenize_function, batched=True)\n",
    "    test_data = dataset[test_set].shuffle(seed=42).select(range(100))    \n",
    "    test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    steps_per_epoch = len(train_data)\\\n",
    "                // (torch.cuda.device_count() * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = steps_per_epoch // 5\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        optim='adamw_bnb_8bit',\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy='epoch',\n",
    "        bf16=False, #bf16=True require CUDA 11 -> original code bf16=True\n",
    "        output_dir=f'{cf.output_path}/finetuning_output/stablelm_finetuning_output',\n",
    "        report_to=[\"tensorboard\", 'wandb'],\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        hub_model_id=new_model_name,\n",
    "        push_to_hub=True,\n",
    "        hub_private_repo=True,\n",
    "        hub_strategy='all_checkpoints',\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=8192,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        packing=True\n",
    "        # formatting_func=format,\n",
    "        # neftune_noise_alpha=5,\n",
    "    )\n",
    "\n",
    "  \n",
    "    dict_info = {\n",
    "        'model':new_model_name,\n",
    "        'base_model':model_name,\n",
    "        'chat_dataset':chat_dataset_name,\n",
    "        'train_set':train_set,\n",
    "        'test_set': test_set,\n",
    "        'training_args': training_args,\n",
    "        'resume_from_checkpoint':resume,\n",
    "        'date':ph.get_datetime(),\n",
    "        'runtime': time.time()-start_time,\n",
    "        'Error': False,\n",
    "        'run_id':run_id,\n",
    "        'save_to_hub':save_to_hub\n",
    "        }\n",
    "\n",
    "    data = pd.DataFrame(columns=dict_info.keys())\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume)\n",
    "    if save_to_hub == True:\n",
    "        trainer.push_to_hub()\n",
    "    return trainer\n",
    "\n",
    "    # try:\n",
    "    #     trainer.train(resume_from_checkpoint=resume)\n",
    "    #     if save_to_hub == True:\n",
    "    #         trainer.push_to_hub()\n",
    "    #     data.loc[len(data)] = dict_info\n",
    "    #     combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "    # except KeyboardInterrupt:\n",
    "    #     dict_info['Error'] = 'KeyboardInterrupt'\n",
    "    #     data.loc[len(data)] = dict_info\n",
    "    #     combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "    # except Exception  as e:\n",
    "    #     print(e)\n",
    "    #     dict_info['Error'] = e\n",
    "    #     data.loc[len(data)] = dict_info\n",
    "    #     combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "\n",
    "    #     model_df = pd.DataFrame(dict_info)\n",
    "    #     combine_and_save_df(model_df, f'{cf.output_path}/overview_models.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "basemodel_name = \"stabilityai/stablelm-2-1_6b\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(basemodel_name,num_labels=5, torch_dtype=torch.bfloat16,\n",
    "                                                low_cpu_mem_usage=True, attn_implementation=\"sdpa\",\n",
    "                                                device_map='cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(basemodel_name)\n",
    "# tokenizer.pad_token = tokenizer.unk_token\n",
    "# tokenizer.padding_side = 'right'\n",
    "\n",
    "# model.config.pad_token_id = tokenizer.unk_token_id\n",
    "model.config.problem_type = \"multi_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_class = train(model, basemodel_name, tokenizer, dataset, 'FemkeBakker/AmsterdamGEITjeFormat200Tokens',\n",
    "          'FemkeBakker/tryoutstablelm', 'train', 'test',  run_id=2, save_to_hub=False, resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"FemkeBakker/TryoutGeitje2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize input text\n",
    "input_text = \"This is a sample sentence.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Model inference\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=1)\n",
    "\n",
    "# Post-processing (e.g., convert predictions to labels)\n",
    "predicted_label = predictions.item()\n",
    "print(\"Predicted label:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tryout finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## WORKING VERSION OF GEITJE-Chat using AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\n",
    "                                             low_cpu_mem_usage=True, attn_implementation=\"eager\",\n",
    "                                             device_map='balanced')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate(conversation, temperature=0.2, top_k=50, max_new_tokens=1_000):\n",
    "    tokenized = tokenizer.apply_chat_template(conversation, add_generation_prompt=True,\n",
    "                                              return_tensors='pt').to(device)\n",
    "    outputs = model.generate(tokenized, do_sample=True, temperature=temperature,\n",
    "                             top_k=top_k, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Xx Gemeente Amsterdam F l N\\n% Raadscommissie voor FinanciÃ«n, CoÃ¶rdinatie 3d, CoÃ¶rdinatie Aanpak Subsidies,\\nAanpak Belastingen, Waterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\n% Agenda, woensdag 3 juni 2015\\nHierbij wordt u uitgenodigd voor de openbare vergadering van de Raadscommissie\\nvoor FinanciÃ«n, CoÃ¶rdinatie 3d, CoÃ¶rdinatie Aanpak Subsidies, Aanpak Belastingen,\\nWaterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\nTijd 09.00 tot 12.30 uur en van 13.30 tot 17',\n",
    "    }\n",
    "]\n",
    "print(generate(conversation))\n",
    "# <|user|>\n",
    "# Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"? \n",
    "# <|assistant|>\n",
    "# Het woord dat niet op zijn plaats staat is 'geit'. Een geit zou niet tussen een lijst van vervoersmiddelen moeten staan. Het past beter bij een boerderijthema of dierenlijst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation = [\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': f\"\"\"\n",
    "         Classificeer het document in Ã©Ã©n van de categoriÃ«n.\n",
    "        Houd het kort, geef enkel de naam van de categorie als response.\n",
    "    \n",
    "    CategoriÃ«n:   ['Voordracht', 'Besluit', 'Schriftelijke Vragen', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheets']\n",
    "    \n",
    "    Document: \n",
    "\n",
    "        'Xx Gemeente Amsterdam F l N\\n% Raadscommissie voor FinanciÃ«n, CoÃ¶rdinatie 3d, CoÃ¶rdinatie Aanpak Subsidies,\\nAanpak Belastingen, Waterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\n% Agenda, woensdag 3 juni 2015\\nHierbij wordt u uitgenodigd voor de openbare vergadering van de Raadscommissie\\nvoor FinanciÃ«n, CoÃ¶rdinatie 3d, CoÃ¶rdinatie Aanpak Subsidies, Aanpak Belastingen,\\nWaterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\nTijd 09.00 tot 12.30 uur en van 13.30 tot 17'\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "print(generate(conversation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dataset['val'][0]['message'][0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code works for GEITje example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel_name, torch_dtype=torch.bfloat16,\n",
    "                                                low_cpu_mem_usage=True, attn_implementation=\"sdpa\",\n",
    "                                                device_map='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "def train(model, tokenizer, chat_dataset, new_model_name):\n",
    "\n",
    "    def format(examples):\n",
    "        return [tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "                for conversation in examples['messages_nl']]\n",
    "\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    steps_per_epoch = len(chat_dataset['train_sft'])\\\n",
    "                 // (torch.cuda.device_count() * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = steps_per_epoch // 5\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        optim='adamw_bnb_8bit',\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        output_dir=f'{cf.output_path}/geitje_finetuning_output',\n",
    "        report_to=[\"tensorboard\", 'wandb'],\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        hub_model_id=new_model_name,\n",
    "        push_to_hub=True,\n",
    "        hub_private_repo=True,\n",
    "        hub_strategy='all_checkpoints',\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=8192,\n",
    "        train_dataset=chat_dataset['train_sft'],\n",
    "        eval_dataset=chat_dataset['test_sft'],\n",
    "        formatting_func=format,\n",
    "        neftune_noise_alpha=5,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.push_to_hub()\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "# model = AutoModelForCausalLM.from_pretrained(basemodel_name, torch_dtype=torch.bfloat16,\n",
    "#                                                 low_cpu_mem_usage=True, attn_implementation=\"eager\",\n",
    "#                                                 device_map='balanced')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(basemodel_name)\n",
    "\n",
    "# Mistral 7B is missing a padding token by default, so we need to assign\n",
    "# another token to the padding job during training.\n",
    "# Unfortunately we cannot use the </s> token, because we need the model to\n",
    "# learn to output </s> at the end of its turn, so that we can stop generating\n",
    "# when it emits it. If we were to also use it as the padding token,\n",
    "# any loss computed on </s> would then be discarded, nothing would be learned\n",
    "# and the model would never stop generating.\n",
    "# Trust me, I learned this the hard way ;).\n",
    "# Therefore, we take the least bad alternative action and assign\n",
    "# the rarely used <UNK> token to the padding role.\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'right'\n",
    "model.config.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "no_robots_nl = load_dataset('Rijgersberg/no_robots_nl')\n",
    "no_robots_nl[\"train_sft\"]=no_robots_nl[\"train_sft\"].select(range(2))\n",
    "no_robots_nl[\"test_sft\"]=no_robots_nl[\"test_sft\"].select(range(2))\n",
    "\n",
    "ultrachat_nl = load_dataset('Rijgersberg/ultrachat_10k_nl')\n",
    "ultrachat_nl[\"train_sft\"]=ultrachat_nl[\"train_sft\"].select(range(2))\n",
    "ultrachat_nl[\"test_sft\"]=ultrachat_nl[\"test_sft\"].select(range(2))\n",
    "\n",
    "chat_dataset = DatasetDict({\n",
    "    'train_sft': concatenate_datasets([no_robots_nl['train_sft'],\n",
    "                                        ultrachat_nl['train_sft']]).shuffle(seed=42),\n",
    "    'test_sft': concatenate_datasets([no_robots_nl['test_sft'],\n",
    "                                        ultrachat_nl['test_sft']]).shuffle(seed=42),\n",
    "})\n",
    "\n",
    "chat_dataset = chat_dataset.filter(lambda row: all(turn['content'] != '<TRANSLATION FAILED>'\n",
    "                                                    for turn in row['messages_nl']))\n",
    "\n",
    "trained_model = train(model, tokenizer, chat_dataset,\n",
    "        new_model_name='FemkeBakker/TryOutFinetuningGeitje')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## WORKING VERSION OF GEITJE-Chat using AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\n",
    "                                             low_cpu_mem_usage=True, attn_implementation=\"eager\",\n",
    "                                             device_map='balanced')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate(conversation, temperature=0.2, top_k=50, max_new_tokens=1_000):\n",
    "    tokenized = tokenizer.apply_chat_template(conversation, add_generation_prompt=True,\n",
    "                                              return_tensors='pt').to(device)\n",
    "    outputs = model.generate(tokenized, do_sample=True, temperature=temperature,\n",
    "                             top_k=top_k, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?'\n",
    "    }\n",
    "]\n",
    "print(generate(conversation))\n",
    "# <|user|>\n",
    "# Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"? \n",
    "# <|assistant|>\n",
    "# Het woord dat niet op zijn plaats staat is 'geit'. Een geit zou niet tussen een lijst van vervoersmiddelen moeten staan. Het past beter bij een boerderijthema of dierenlijst."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
