{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "# import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# setup environment GEITje-7B Finetuning\n",
    "# - pip install torch\n",
    "# - pip install datasets\n",
    "# - pip install transformers\n",
    "# - pip install trl\n",
    "# - pip install accelerate (restart after)\n",
    "# - switch device_map='auto' to avaoid memory error\n",
    "\n",
    "# - pip install sentencepiece\n",
    "# - pip install jupyter\n",
    "# - pip install protobuf \n",
    "# pip install bitsandbytes\n",
    "# pip install bnb\n",
    "# pip install wandb==0.13.3 --upgrade\n",
    "#pip install tensorboardX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8768a614fb4dac8e521f0cc8b053ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning GEITje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4270000967a7446dbfcaea868ad08041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef170fa080347baa73b55b4cbe0fbce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2b75d476184db8afecfad5e7cfbfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/307k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0736f4f25224050af142326746db00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/341k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a83d096d674554955e6f61685b6cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753814de28de4f25abcef8ffe509beb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7656141de374cb3b7d7c068bb7dc0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab117f21300f47f3a8a466cee8d76adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split:   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24c01e42f7048d58cb604bed4eb086d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating discard split:   0%|          | 0/8718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load GEITje formatted data\n",
    "from datasets import load_dataset\n",
    "chat_dataset = load_dataset('FemkeBakker/AmsterdamBalancedFirst200Tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Classificeer het document in één van de categoriën. Categoriën: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']. Categoriseer dit document: X Gemeente Amsterdam W B\\n% Raadscommissie voor Bouwen, Wonen, Wijkaanpak en Dierenwelzijn\\n% Agenda, woensdag 19 april 2017\\nHierbij wordt u uitgenodigd voor de openbare vergadering van de Raadscommissie\\nvoor Bouwen, Wonen, Wijkaanpak en Dierenwelzijn\\nTijd 09.00 uur tot 12.30 uur\\nLocatie De Rooszaal 0239, Stadhuis\\nAlgemeen\\n1 Opening procedureel gedeelte\\n2 Mededelingen\\n3 Vaststellen agenda\\n4 Conceptverslag van de openbare vergadering van de Raadscommissie WB d.d.\\n29 maart 2017\\ne Tekstuele \\nGeef de output in de vorm van een JSON file: {'categorie': categorie van het document}. \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_dataset['val']['message'][0][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_dataset[\"train\"]=chat_dataset[\"train\"].select(range(2))\n",
    "# chat_dataset[\"test\"]=chat_dataset[\"test\"].select(range(2))\n",
    "# chat_dataset[\"val\"]=chat_dataset[\"val\"].select(range(2))\n",
    "# chat_dataset[\"dev\"]=chat_dataset[\"dev\"].select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc85a3172a4429bbd719f72568917eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181630b8edd2480bbf1ecf660daac551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "basemodel_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
    "# basemodel_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "# basemodel_name = \"stabilityai/stablelm-2-1_6b\"\n",
    "# basemodel_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel_name, torch_dtype=torch.bfloat16,\n",
    "                                                low_cpu_mem_usage=True, attn_implementation=\"sdpa\",\n",
    "                                                device_map='cpu')\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(basemodel_name)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "model.config.pad_token_id = tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Classificeer het document in één van de categoriën. Categoriën: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']. Categoriseer dit document: > Gemeente\n",
      "Amsterdam\n",
      "Motie\n",
      "Datum raadsvergadering 19 juli 2023\n",
      "Ingekomen onder nummer 456\n",
      "Status Aangenomen\n",
      "Onderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\n",
      "Kabamba en Krom inzake een grote schoonmaak voor iedereen op de\n",
      "wachtlijst Hulp bij Huishouden\n",
      "Onderwerp\n",
      "Een grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\n",
      "Aan de gemeenteraad\n",
      "Ondergetekenden hebben de eer voor te stellen:\n",
      "De Raad,\n",
      "Gehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\n",
      "Constaterende dat:\n",
      "e \n",
      "Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}. </s>\n",
      "<|assistant|>\n",
      "{'categorie': Motie}</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# geitje base\n",
    "print(tokenizer.apply_chat_template(chat_dataset['train']['message'][0], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Classificeer het document in één van de categoriën. Categoriën: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']. Categoriseer dit document: > Gemeente\n",
      "Amsterdam\n",
      "Motie\n",
      "Datum raadsvergadering 19 juli 2023\n",
      "Ingekomen onder nummer 456\n",
      "Status Aangenomen\n",
      "Onderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\n",
      "Kabamba en Krom inzake een grote schoonmaak voor iedereen op de\n",
      "wachtlijst Hulp bij Huishouden\n",
      "Onderwerp\n",
      "Een grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\n",
      "Aan de gemeenteraad\n",
      "Ondergetekenden hebben de eer voor te stellen:\n",
      "De Raad,\n",
      "Gehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\n",
      "Constaterende dat:\n",
      "e \n",
      "Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}. [/INST] {'categorie': Motie} </s>\n"
     ]
    }
   ],
   "source": [
    "# llama\n",
    "print(tokenizer.apply_chat_template(chat_dataset['train']['message'][0], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Classificeer het document in één van de categoriën. Categoriën: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']. Categoriseer dit document: > Gemeente\n",
      "Amsterdam\n",
      "Motie\n",
      "Datum raadsvergadering 19 juli 2023\n",
      "Ingekomen onder nummer 456\n",
      "Status Aangenomen\n",
      "Onderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\n",
      "Kabamba en Krom inzake een grote schoonmaak voor iedereen op de\n",
      "wachtlijst Hulp bij Huishouden\n",
      "Onderwerp\n",
      "Een grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\n",
      "Aan de gemeenteraad\n",
      "Ondergetekenden hebben de eer voor te stellen:\n",
      "De Raad,\n",
      "Gehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\n",
      "Constaterende dat:\n",
      "e \n",
      "Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}. </s>\n",
      "<|assistant|>\n",
      "{'categorie': Motie}</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# geitje chat\n",
    "print(tokenizer.apply_chat_template(chat_dataset['train']['message'][0], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def combine_and_save_df(model_df, save_to_path):\n",
    "    \n",
    "    # combine with earlier runs if exists\n",
    "    if os.path.exists(save_to_path):\n",
    "        original = pd.read_pickle(save_to_path)\n",
    "        model_df = pd.concat([original, model_df])\n",
    "\n",
    "    model_df.to_pickle(save_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import prediction_helperfunctions as ph\n",
    "\n",
    "def train(model, model_name, tokenizer, chat_dataset, chat_dataset_name, new_model_name, train_set, test_set, run_id='No_id', save_to_hub=True, resume=False):\n",
    "    start_time = time.time()\n",
    "    def format(examples):\n",
    "        return [tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "                for conversation in examples['message']]\n",
    "\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    steps_per_epoch = len(chat_dataset[train_set])\\\n",
    "                // (torch.cuda.device_count() * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = steps_per_epoch // 5\n",
    "\n",
    "    output_directory = f'{cf.output_path}/finetuning_output/AmsterdamDocClassificationGEITje200T'\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        optim='adamw_bnb_8bit',\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy='epoch',\n",
    "        bf16=False, #bf16=True require CUDA 11 -> original code bf16=True\n",
    "        output_dir=output_directory,\n",
    "        report_to=[\"tensorboard\", 'wandb'],\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        hub_model_id=new_model_name,\n",
    "        push_to_hub=True,\n",
    "        hub_private_repo=True,\n",
    "        hub_strategy='all_checkpoints',\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=8192,\n",
    "        train_dataset=chat_dataset[train_set],\n",
    "        eval_dataset=chat_dataset[test_set],\n",
    "        formatting_func=format,\n",
    "        neftune_noise_alpha=5,\n",
    "    )\n",
    "\n",
    "  \n",
    "    dict_info = {\n",
    "        'model':new_model_name,\n",
    "        'base_model':model_name,\n",
    "        'chat_dataset':chat_dataset_name,\n",
    "        'train_set':train_set,\n",
    "        'test_set': test_set,\n",
    "        'training_args': training_args,\n",
    "        'resume_from_checkpoint':resume,\n",
    "        'date':ph.get_datetime(),\n",
    "        'runtime': False,\n",
    "        'Error': False,\n",
    "        'run_id':run_id,\n",
    "        'save_to_hub':save_to_hub,\n",
    "        'output_dir': output_directory\n",
    "        }\n",
    "\n",
    "    data = pd.DataFrame(columns=dict_info.keys())\n",
    "\n",
    "    # trainer.train(resume_from_checkpoint=resume)\n",
    "    # if save_to_hub == True:\n",
    "    #     trainer.push_to_hub()\n",
    "    # return trainer\n",
    "\n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=resume)\n",
    "        if save_to_hub == True:\n",
    "            trainer.push_to_hub()\n",
    "            \n",
    "        dict_info['runtime'] = time.time()-start_time\n",
    "\n",
    "        data.loc[len(data)] = dict_info\n",
    "        combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "        print(\"Finished without error!\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        dict_info['Error'] = 'KeyboardInterrupt'        \n",
    "        dict_info['runtime'] = time.time()-start_time\n",
    "\n",
    "        data.loc[len(data)] = dict_info\n",
    "        combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "    except Exception  as e:\n",
    "        print(e)\n",
    "        dict_info['Error'] = e\n",
    "        dict_info['runtime'] = time.time()-start_time\n",
    "\n",
    "        data.loc[len(data)] = dict_info\n",
    "        combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "\n",
    "        model_df = pd.DataFrame(dict_info)\n",
    "        combine_and_save_df(model_df, f'{cf.output_path}/overview_models.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "resume_from_checkpoitn, gives error if last epoch was not fully run, because then files are missing but the folder exists, thus it throws an error. Removing the last checkpoint folder solves this, does mean the epoch need to restart completely. If this is a viable solution  depends on how long an epoch takes to train. Might be quite long. However if all files are in folder then it is fine. How long does it take before all files are saved in folder? Does this need the epoch to be completed?\n",
    "\n",
    "\n",
    "MAKE SURE: no previous checkpints of runs unrelated to current run are in output folder!!\n",
    "\n",
    "MAKE SURE: run_id is unique, for each seperate run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:257: UserWarning: You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override the one in the `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aac3f19c15748c9b62641e33231f545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f8f0be75054346b79e93246537e150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "train(model, basemodel_name, tokenizer, chat_dataset, 'FemkeBakker/AmsterdamBalancedFirst200Tokens',\n",
    "          'FemkeBakker/AmsterdamDocClassificationGEITje200T', 'train', 'val',  run_id=15, save_to_hub=True, resume=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "260 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>base_model</th>\n",
       "      <th>chat_dataset</th>\n",
       "      <th>train_set</th>\n",
       "      <th>test_set</th>\n",
       "      <th>training_args</th>\n",
       "      <th>resume_from_checkpoint</th>\n",
       "      <th>date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>Error</th>\n",
       "      <th>run_id</th>\n",
       "      <th>save_to_hub</th>\n",
       "      <th>output_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-25 13:21:46.201696+02:00</td>\n",
       "      <td>0.876791</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-25 13:24:59.884223+02:00</td>\n",
       "      <td>0.274652</td>\n",
       "      <td>Error(s) in loading state_dict for MistralForC...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-25 13:25:57.527833+02:00</td>\n",
       "      <td>0.362344</td>\n",
       "      <td>No valid checkpoint found in output directory ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-25 13:26:24.369492+02:00</td>\n",
       "      <td>0.319212</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 112.00 M...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 08:47:21.945291+02:00</td>\n",
       "      <td>0.392476</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 08:47:57.151755+02:00</td>\n",
       "      <td>0.600505</td>\n",
       "      <td>string longer than 2147483647 bytes</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryoutGeitje2</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-26 08:56:52.283214+02:00</td>\n",
       "      <td>0.449522</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryoutGeitje2</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 09:26:12.329799+02:00</td>\n",
       "      <td>3.251497</td>\n",
       "      <td>[Errno 2] No such file or directory: '/home/az...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryoutGeitje2</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 09:29:06.636546+02:00</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 09:46:03.908296+02:00</td>\n",
       "      <td>0.364137</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 09:48:18.644406+02:00</td>\n",
       "      <td>0.791530</td>\n",
       "      <td>Expected input batch_size (2) to match target ...</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 11:28:02.756504+02:00</td>\n",
       "      <td>1.342644</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 11:48:12.624523+02:00</td>\n",
       "      <td>4.499735</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 11:53:35.436951+02:00</td>\n",
       "      <td>0.519014</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistral-community/Mistral-7B-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-30 13:32:55.123342+02:00</td>\n",
       "      <td>1.238507</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 22.00 Mi...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistral-community/Mistral-7B-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 10:14:42.103986+02:00</td>\n",
       "      <td>4.744335</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/LlamaSmallData200Tokens</td>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 10:22:35.180386+02:00</td>\n",
       "      <td>0.644924</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistral-community/Mistral-7B-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 11:30:37.119100+02:00</td>\n",
       "      <td>1.784342</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 112.00 M...</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 13:29:55.638421+02:00</td>\n",
       "      <td>4.780315</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 14:02:55.215348+02:00</td>\n",
       "      <td>0.597784</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/MistralSmallData200Tokens</td>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 14:06:39.657276+02:00</td>\n",
       "      <td>1467.518128</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B-chat-v2</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 14:33:40.318320+02:00</td>\n",
       "      <td>1367.711304</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/LlamaSmallData200Tokens</td>\n",
       "      <td>meta-llama/Llama-2-7b-chat-hf</td>\n",
       "      <td>FemkeBakker/AmsterdamFormat200LlamaTokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-02 15:01:21.038330+02:00</td>\n",
       "      <td>1040.267759</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-28 09:33:15.666775+02:00</td>\n",
       "      <td>1274.191825</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-05-28 09:54:46.992883+02:00</td>\n",
       "      <td>2.428063</td>\n",
       "      <td>No valid checkpoint found in output directory ...</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-05-28 09:57:30.580525+02:00</td>\n",
       "      <td>0.332646</td>\n",
       "      <td>No valid checkpoint found in output directory ...</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamBalancedFirst200Tokens</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-05-28 09:57:47.191583+02:00</td>\n",
       "      <td>6866.744590</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   model                          base_model  \\\n",
       "0     FemkeBakker/TryOutFinetuningGeitje               Rijgersberg/GEITje-7B   \n",
       "0     FemkeBakker/TryOutFinetuningGeitje               Rijgersberg/GEITje-7B   \n",
       "0     FemkeBakker/TryOutFinetuningGeitje               Rijgersberg/GEITje-7B   \n",
       "0     FemkeBakker/TryOutFinetuningGeitje               Rijgersberg/GEITje-7B   \n",
       "0     FemkeBakker/TryOutFinetuningGeitje               Rijgersberg/GEITje-7B   \n",
       "0     FemkeBakker/TryOutFinetuningGeitje               Rijgersberg/GEITje-7B   \n",
       "0              FemkeBakker/TryoutGeitje2               Rijgersberg/GEITje-7B   \n",
       "0              FemkeBakker/TryoutGeitje2               Rijgersberg/GEITje-7B   \n",
       "0              FemkeBakker/TryoutGeitje2               Rijgersberg/GEITje-7B   \n",
       "0             FemkeBakker/tryoutstablelm         stabilityai/stablelm-2-1_6b   \n",
       "0             FemkeBakker/tryoutstablelm         stabilityai/stablelm-2-1_6b   \n",
       "0   FemkeBakker/GEITjeSmallData200Tokens               Rijgersberg/GEITje-7B   \n",
       "0   FemkeBakker/GEITjeSmallData200Tokens               Rijgersberg/GEITje-7B   \n",
       "0   FemkeBakker/GEITjeSmallData200Tokens               Rijgersberg/GEITje-7B   \n",
       "0  FemkeBakker/MistralSmallData200Tokens   mistral-community/Mistral-7B-v0.2   \n",
       "0  FemkeBakker/MistralSmallData200Tokens   mistral-community/Mistral-7B-v0.2   \n",
       "0    FemkeBakker/LlamaSmallData200Tokens            meta-llama/Llama-2-7b-hf   \n",
       "0  FemkeBakker/MistralSmallData200Tokens   mistral-community/Mistral-7B-v0.2   \n",
       "0  FemkeBakker/MistralSmallData200Tokens  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "0  FemkeBakker/MistralSmallData200Tokens  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "0  FemkeBakker/MistralSmallData200Tokens  mistralai/Mistral-7B-Instruct-v0.2   \n",
       "0   FemkeBakker/GEITjeSmallData200Tokens       Rijgersberg/GEITje-7B-chat-v2   \n",
       "0    FemkeBakker/LlamaSmallData200Tokens       meta-llama/Llama-2-7b-chat-hf   \n",
       "0             FemkeBakker/tryoutstablelm         stabilityai/stablelm-2-1_6b   \n",
       "0             FemkeBakker/tryoutstablelm         stabilityai/stablelm-2-1_6b   \n",
       "0             FemkeBakker/tryoutstablelm         stabilityai/stablelm-2-1_6b   \n",
       "0             FemkeBakker/tryoutstablelm         stabilityai/stablelm-2-1_6b   \n",
       "\n",
       "                                  chat_dataset train_set test_set  \\\n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0   FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0    FemkeBakker/AmsterdamFormat200LlamaTokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train     test   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train     test   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train     test   \n",
       "0  FemkeBakker/AmsterdamBalancedFirst200Tokens     train     test   \n",
       "\n",
       "                                       training_args  resume_from_checkpoint  \\\n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "\n",
       "                              date      runtime  \\\n",
       "0 2024-04-25 13:21:46.201696+02:00     0.876791   \n",
       "0 2024-04-25 13:24:59.884223+02:00     0.274652   \n",
       "0 2024-04-25 13:25:57.527833+02:00     0.362344   \n",
       "0 2024-04-25 13:26:24.369492+02:00     0.319212   \n",
       "0 2024-04-26 08:47:21.945291+02:00     0.392476   \n",
       "0 2024-04-26 08:47:57.151755+02:00     0.600505   \n",
       "0 2024-04-26 08:56:52.283214+02:00     0.449522   \n",
       "0 2024-04-26 09:26:12.329799+02:00     3.251497   \n",
       "0 2024-04-26 09:29:06.636546+02:00     0.543515   \n",
       "0 2024-04-29 09:46:03.908296+02:00     0.364137   \n",
       "0 2024-04-29 09:48:18.644406+02:00     0.791530   \n",
       "0 2024-04-29 11:28:02.756504+02:00     1.342644   \n",
       "0 2024-04-29 11:48:12.624523+02:00     4.499735   \n",
       "0 2024-04-29 11:53:35.436951+02:00     0.519014   \n",
       "0 2024-04-30 13:32:55.123342+02:00     1.238507   \n",
       "0 2024-05-02 10:14:42.103986+02:00     4.744335   \n",
       "0 2024-05-02 10:22:35.180386+02:00     0.644924   \n",
       "0 2024-05-02 11:30:37.119100+02:00     1.784342   \n",
       "0 2024-05-02 13:29:55.638421+02:00     4.780315   \n",
       "0 2024-05-02 14:02:55.215348+02:00     0.597784   \n",
       "0 2024-05-02 14:06:39.657276+02:00  1467.518128   \n",
       "0 2024-05-02 14:33:40.318320+02:00  1367.711304   \n",
       "0 2024-05-02 15:01:21.038330+02:00  1040.267759   \n",
       "0 2024-05-28 09:33:15.666775+02:00  1274.191825   \n",
       "0 2024-05-28 09:54:46.992883+02:00     2.428063   \n",
       "0 2024-05-28 09:57:30.580525+02:00     0.332646   \n",
       "0 2024-05-28 09:57:47.191583+02:00  6866.744590   \n",
       "\n",
       "                                               Error  run_id save_to_hub  \\\n",
       "0                                  KeyboardInterrupt       0         NaN   \n",
       "0  Error(s) in loading state_dict for MistralForC...       0         NaN   \n",
       "0  No valid checkpoint found in output directory ...       0         NaN   \n",
       "0  CUDA out of memory. Tried to allocate 112.00 M...       0         NaN   \n",
       "0                                  KeyboardInterrupt       0         NaN   \n",
       "0                string longer than 2147483647 bytes       0         NaN   \n",
       "0                                              False       1         NaN   \n",
       "0  [Errno 2] No such file or directory: '/home/az...       1         NaN   \n",
       "0                                              False       1         NaN   \n",
       "0                                  KeyboardInterrupt       2         NaN   \n",
       "0  Expected input batch_size (2) to match target ...       2       False   \n",
       "0                                  KeyboardInterrupt       3        True   \n",
       "0                                  KeyboardInterrupt       4        True   \n",
       "0                                              False       4        True   \n",
       "0  CUDA out of memory. Tried to allocate 22.00 Mi...       5        True   \n",
       "0                                  KeyboardInterrupt       6        True   \n",
       "0                                              False       7        True   \n",
       "0  CUDA out of memory. Tried to allocate 112.00 M...       8        True   \n",
       "0                                              False       9        True   \n",
       "0                                  KeyboardInterrupt      10        True   \n",
       "0                                              False      10        True   \n",
       "0                                              False      11        True   \n",
       "0                                              False      12        True   \n",
       "0                                  KeyboardInterrupt      13        True   \n",
       "0  No valid checkpoint found in output directory ...      13        True   \n",
       "0  No valid checkpoint found in output directory ...      13        True   \n",
       "0                                  KeyboardInterrupt      13        True   \n",
       "\n",
       "                                          output_dir  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yeet = pd.read_pickle(f'{cf.output_path}/overview_models.pkl')\n",
    "display(yeet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import prediction_helperfunctions as ph\n",
    "\n",
    "def train(model, model_name, tokenizer, dataset, chat_dataset_name, new_model_name, train_set, test_set, run_id='No_id', save_to_hub=True, resume=False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    # tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_data = dataset[train_set].shuffle(seed=42).select(range(100))\n",
    "    train_data = train_data.map(tokenize_function, batched=True)\n",
    "    test_data = dataset[test_set].shuffle(seed=42).select(range(100))    \n",
    "    test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    steps_per_epoch = len(train_data)\\\n",
    "                // (torch.cuda.device_count() * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = steps_per_epoch // 5\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        optim='adamw_bnb_8bit',\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy='epoch',\n",
    "        bf16=False, #bf16=True require CUDA 11 -> original code bf16=True\n",
    "        output_dir=f'{cf.output_path}/finetuning_output/stablelm_finetuning_output',\n",
    "        report_to=[\"tensorboard\", 'wandb'],\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        hub_model_id=new_model_name,\n",
    "        push_to_hub=True,\n",
    "        hub_private_repo=True,\n",
    "        hub_strategy='all_checkpoints',\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=8192,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        packing=True\n",
    "        # formatting_func=format,\n",
    "        # neftune_noise_alpha=5,\n",
    "    )\n",
    "\n",
    "  \n",
    "    dict_info = {\n",
    "        'model':new_model_name,\n",
    "        'base_model':model_name,\n",
    "        'chat_dataset':chat_dataset_name,\n",
    "        'train_set':train_set,\n",
    "        'test_set': test_set,\n",
    "        'training_args': training_args,\n",
    "        'resume_from_checkpoint':resume,\n",
    "        'date':ph.get_datetime(),\n",
    "        'runtime': time.time()-start_time,\n",
    "        'Error': False,\n",
    "        'run_id':run_id,\n",
    "        'save_to_hub':save_to_hub\n",
    "        }\n",
    "\n",
    "    data = pd.DataFrame(columns=dict_info.keys())\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume)\n",
    "    if save_to_hub == True:\n",
    "        trainer.push_to_hub()\n",
    "    return trainer\n",
    "\n",
    "    # try:\n",
    "    #     trainer.train(resume_from_checkpoint=resume)\n",
    "    #     if save_to_hub == True:\n",
    "    #         trainer.push_to_hub()\n",
    "    #     data.loc[len(data)] = dict_info\n",
    "    #     combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "    # except KeyboardInterrupt:\n",
    "    #     dict_info['Error'] = 'KeyboardInterrupt'\n",
    "    #     data.loc[len(data)] = dict_info\n",
    "    #     combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "    # except Exception  as e:\n",
    "    #     print(e)\n",
    "    #     dict_info['Error'] = e\n",
    "    #     data.loc[len(data)] = dict_info\n",
    "    #     combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "\n",
    "    #     model_df = pd.DataFrame(dict_info)\n",
    "    #     combine_and_save_df(model_df, f'{cf.output_path}/overview_models.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "basemodel_name = \"stabilityai/stablelm-2-1_6b\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(basemodel_name,num_labels=5, torch_dtype=torch.bfloat16,\n",
    "                                                low_cpu_mem_usage=True, attn_implementation=\"sdpa\",\n",
    "                                                device_map='cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(basemodel_name)\n",
    "# tokenizer.pad_token = tokenizer.unk_token\n",
    "# tokenizer.padding_side = 'right'\n",
    "\n",
    "# model.config.pad_token_id = tokenizer.unk_token_id\n",
    "model.config.problem_type = \"multi_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_class = train(model, basemodel_name, tokenizer, dataset, 'FemkeBakker/AmsterdamGEITjeFormat200Tokens',\n",
    "          'FemkeBakker/tryoutstablelm', 'train', 'test',  run_id=2, save_to_hub=False, resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"FemkeBakker/TryoutGeitje2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize input text\n",
    "input_text = \"This is a sample sentence.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Model inference\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=1)\n",
    "\n",
    "# Post-processing (e.g., convert predictions to labels)\n",
    "predicted_label = predictions.item()\n",
    "print(\"Predicted label:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tryout finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## WORKING VERSION OF GEITJE-Chat using AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\n",
    "                                             low_cpu_mem_usage=True, attn_implementation=\"eager\",\n",
    "                                             device_map='balanced')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate(conversation, temperature=0.2, top_k=50, max_new_tokens=1_000):\n",
    "    tokenized = tokenizer.apply_chat_template(conversation, add_generation_prompt=True,\n",
    "                                              return_tensors='pt').to(device)\n",
    "    outputs = model.generate(tokenized, do_sample=True, temperature=temperature,\n",
    "                             top_k=top_k, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Xx Gemeente Amsterdam F l N\\n% Raadscommissie voor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies,\\nAanpak Belastingen, Waterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\n% Agenda, woensdag 3 juni 2015\\nHierbij wordt u uitgenodigd voor de openbare vergadering van de Raadscommissie\\nvoor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies, Aanpak Belastingen,\\nWaterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\nTijd 09.00 tot 12.30 uur en van 13.30 tot 17',\n",
    "    }\n",
    "]\n",
    "print(generate(conversation))\n",
    "# <|user|>\n",
    "# Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"? \n",
    "# <|assistant|>\n",
    "# Het woord dat niet op zijn plaats staat is 'geit'. Een geit zou niet tussen een lijst van vervoersmiddelen moeten staan. Het past beter bij een boerderijthema of dierenlijst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation = [\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': f\"\"\"\n",
    "         Classificeer het document in één van de categoriën.\n",
    "        Houd het kort, geef enkel de naam van de categorie als response.\n",
    "    \n",
    "    Categoriën:   ['Voordracht', 'Besluit', 'Schriftelijke Vragen', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheets']\n",
    "    \n",
    "    Document: \n",
    "\n",
    "        'Xx Gemeente Amsterdam F l N\\n% Raadscommissie voor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies,\\nAanpak Belastingen, Waterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\n% Agenda, woensdag 3 juni 2015\\nHierbij wordt u uitgenodigd voor de openbare vergadering van de Raadscommissie\\nvoor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies, Aanpak Belastingen,\\nWaterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\nTijd 09.00 tot 12.30 uur en van 13.30 tot 17'\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "print(generate(conversation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dataset['val'][0]['message'][0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code works for GEITje example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel_name, torch_dtype=torch.bfloat16,\n",
    "                                                low_cpu_mem_usage=True, attn_implementation=\"sdpa\",\n",
    "                                                device_map='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "def train(model, tokenizer, chat_dataset, new_model_name):\n",
    "\n",
    "    def format(examples):\n",
    "        return [tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "                for conversation in examples['messages_nl']]\n",
    "\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    steps_per_epoch = len(chat_dataset['train_sft'])\\\n",
    "                 // (torch.cuda.device_count() * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = steps_per_epoch // 5\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        optim='adamw_bnb_8bit',\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        output_dir=f'{cf.output_path}/geitje_finetuning_output',\n",
    "        report_to=[\"tensorboard\", 'wandb'],\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        hub_model_id=new_model_name,\n",
    "        push_to_hub=True,\n",
    "        hub_private_repo=True,\n",
    "        hub_strategy='all_checkpoints',\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=8192,\n",
    "        train_dataset=chat_dataset['train_sft'],\n",
    "        eval_dataset=chat_dataset['test_sft'],\n",
    "        formatting_func=format,\n",
    "        neftune_noise_alpha=5,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.push_to_hub()\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "# model = AutoModelForCausalLM.from_pretrained(basemodel_name, torch_dtype=torch.bfloat16,\n",
    "#                                                 low_cpu_mem_usage=True, attn_implementation=\"eager\",\n",
    "#                                                 device_map='balanced')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(basemodel_name)\n",
    "\n",
    "# Mistral 7B is missing a padding token by default, so we need to assign\n",
    "# another token to the padding job during training.\n",
    "# Unfortunately we cannot use the </s> token, because we need the model to\n",
    "# learn to output </s> at the end of its turn, so that we can stop generating\n",
    "# when it emits it. If we were to also use it as the padding token,\n",
    "# any loss computed on </s> would then be discarded, nothing would be learned\n",
    "# and the model would never stop generating.\n",
    "# Trust me, I learned this the hard way ;).\n",
    "# Therefore, we take the least bad alternative action and assign\n",
    "# the rarely used <UNK> token to the padding role.\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'right'\n",
    "model.config.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "no_robots_nl = load_dataset('Rijgersberg/no_robots_nl')\n",
    "no_robots_nl[\"train_sft\"]=no_robots_nl[\"train_sft\"].select(range(2))\n",
    "no_robots_nl[\"test_sft\"]=no_robots_nl[\"test_sft\"].select(range(2))\n",
    "\n",
    "ultrachat_nl = load_dataset('Rijgersberg/ultrachat_10k_nl')\n",
    "ultrachat_nl[\"train_sft\"]=ultrachat_nl[\"train_sft\"].select(range(2))\n",
    "ultrachat_nl[\"test_sft\"]=ultrachat_nl[\"test_sft\"].select(range(2))\n",
    "\n",
    "chat_dataset = DatasetDict({\n",
    "    'train_sft': concatenate_datasets([no_robots_nl['train_sft'],\n",
    "                                        ultrachat_nl['train_sft']]).shuffle(seed=42),\n",
    "    'test_sft': concatenate_datasets([no_robots_nl['test_sft'],\n",
    "                                        ultrachat_nl['test_sft']]).shuffle(seed=42),\n",
    "})\n",
    "\n",
    "chat_dataset = chat_dataset.filter(lambda row: all(turn['content'] != '<TRANSLATION FAILED>'\n",
    "                                                    for turn in row['messages_nl']))\n",
    "\n",
    "trained_model = train(model, tokenizer, chat_dataset,\n",
    "        new_model_name='FemkeBakker/TryOutFinetuningGeitje')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## WORKING VERSION OF GEITJE-Chat using AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\n",
    "                                             low_cpu_mem_usage=True, attn_implementation=\"eager\",\n",
    "                                             device_map='balanced')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate(conversation, temperature=0.2, top_k=50, max_new_tokens=1_000):\n",
    "    tokenized = tokenizer.apply_chat_template(conversation, add_generation_prompt=True,\n",
    "                                              return_tensors='pt').to(device)\n",
    "    outputs = model.generate(tokenized, do_sample=True, temperature=temperature,\n",
    "                             top_k=top_k, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?'\n",
    "    }\n",
    "]\n",
    "print(generate(conversation))\n",
    "# <|user|>\n",
    "# Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"? \n",
    "# <|assistant|>\n",
    "# Het woord dat niet op zijn plaats staat is 'geit'. Een geit zou niet tussen een lijst van vervoersmiddelen moeten staan. Het past beter bij een boerderijthema of dierenlijst."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2AmsterdamLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
