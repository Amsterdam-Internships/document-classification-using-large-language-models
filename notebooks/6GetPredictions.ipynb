{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1712584227159
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Select where to run notebook: \"azure\" or \"local\"\n",
        "my_run = \"local\"\n",
        "\n",
        "# import my_secrets as sc\n",
        "# import settings as st\n",
        "\n",
        "if my_run == \"azure\":\n",
        "    import config_azure as cf\n",
        "elif my_run == \"local\":\n",
        "    import config as cf\n",
        "\n",
        "\n",
        "import os\n",
        "if my_run == \"azure\":\n",
        "    if not os.path.exists(cf.HUGGING_CACHE):\n",
        "        os.mkdir(cf.HUGGING_CACHE)\n",
        "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook overview\n",
        "Goal: run all experiments\n",
        "\n",
        "Data:\n",
        "- using txtfiles.pkl\n",
        "- data is already split\n",
        "- text is already converted to tokens using model tokenizer \n",
        "\n",
        "*Previous notebook: finetuning*\n",
        "\n",
        "*Next notebook: baseline*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5eedcf4604444f24ba90d25cb0ca664d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "sys.path.append('../src/') \n",
        "import prompt_template as pt\n",
        "import prediction_helperfunctions as ph\n",
        "import truncation as tf\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Trial run Models \n",
        "Code to run the models with a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='meta-llama/Llama-2-7b-chat-hf',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "## EXAMPLE PROMPT\n",
        "# print(chatbot(\n",
        "    # Conversation('Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?')\n",
        "# ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment functions\n",
        "Prompt model for each document and save the prediction, return response, response time and the prompt version.\n",
        "After running, all predictions are save in one file and a row in overview.pkl is added with information about the run, such scores and runtime.\n",
        "\n",
        "Code structure:\n",
        "- predictions_incontextlearning -> given a df with docs that need to be predicted, prompt the model\n",
        "- run the experiment -> built in failsaves (df run in parts, with saves in between)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from bm25 import BM25\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# chatbot = loaded model, either the base_model or the finetuned version\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template \n",
        "# train_df = dataframe with docs, which can be used as examples/training data/context data\n",
        "# num_examples = number of examples in the prompt\n",
        "\n",
        "def predictions_incontextlearning(chatbot, docs_df, text_column, prompt_function, train_df, num_examples):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date', 'prompt'])\n",
        "    \n",
        "    if prompt_function == pt.fewshot_prompt_with_template or prompt_function == pt.fewshot_prompt_no_template:\n",
        "        BM25_model = BM25()\n",
        "        BM25_model.fit(train_df[text_column])\n",
        "\n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "\n",
        "        # each prompt function takes different arguments\n",
        "        # zeroshot prompt for geitje\n",
        "        if prompt_function == pt.zeroshot_prompt_geitje:\n",
        "            prompt = prompt_function(txt)\n",
        "\n",
        "        # zeroshot function for mistral and llama\n",
        "        elif prompt_function == pt.zeroshot_prompt_mistral_llama:\n",
        "            prompt = prompt_function(txt)\n",
        "        \n",
        "        # fewshot function without template, used for geitje\n",
        "        elif prompt_function == pt.fewshot_prompt_no_template:\n",
        "            prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "\n",
        "        # fewshot function with template, used for llama and mistral\n",
        "        elif prompt_function == pt.fewshot_prompt_with_template:\n",
        "            prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Prompt function not recognised. Check if prompt function is in prompt_template.py and included in the options above.\")\n",
        "\n",
        "        # prompt and get the response\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "        print(\"label: \", row['label'].lower())\n",
        "        print(\"response: \", response)\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = ph.get_prediction_from_response(response)\n",
        "        print(\"prediction:\", prediction)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : docs_df.iloc[0]['trunc_col'],\n",
        "            'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': ph.get_datetime(),\n",
        "            'prompt':prompt\n",
        "        }\n",
        "    return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\"\"\"\n",
        "Function to run GEITje In-Context Learning experiment. \n",
        "The function allows to resume experiment, if run_id matches.\n",
        "\"\"\"\n",
        "# chatbot = loaded model, either the base_model or the finetuned version\n",
        "# df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# run_id = unqiue for each experiment. \n",
        "# prompt_function = which prompt from prompt_template.py to use\n",
        "# text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# label_col = column with the true label\n",
        "# prediction_path = path to file where predictions need to be saved.\n",
        "# overview_path = path to file where results of each run need to be saved.\n",
        "# model_name = name of the model. string.\n",
        "# num_exmples = number of exaples given to prompt. zero in case of zeroshot. \n",
        "\n",
        "def run_experiment(chatbot, df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "    test_df = df.loc[df[split_col]==subset_test]\n",
        "    train_df = df.loc[df[split_col]==subset_train]\n",
        "    \n",
        "    # get rows of df that still need to be predicted for the specific run_id\n",
        "    to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "    # devide to_predict into subsection of 50 predictions at a time. \n",
        "    # Allows to rerun without problem. And save subsections of 50 predictions.\n",
        "    step_range = list(range(0, len(to_predict), 10))\n",
        "\n",
        "    for i in range(len(step_range)):\n",
        "        try:\n",
        "            sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "            print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "        except Exception as e:\n",
        "            sub_to_predict = to_predict[step_range[i]:]\n",
        "            print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "        # prompt geitje\n",
        "        predictions = predictions_incontextlearning(chatbot, sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
        "\n",
        "        # save info\n",
        "        predictions['run_id'] = run_id\n",
        "        predictions['shots'] = num_examples\n",
        "\n",
        "        # save new combinations in file\n",
        "        print(\"Dont interrupt, saving predictions...\")\n",
        "        ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "        # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "        try:\n",
        "            predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "            previous_predictions = predictions\n",
        "        except Exception as e:\n",
        "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "            previous_predictions = predictions\n",
        "\n",
        "        # save results in overview file\n",
        "        date = ph.get_datetime()\n",
        "        y_test = predictions['label']\n",
        "        y_pred = predictions['prediction']\n",
        "\n",
        "        report = classification_report(y_test, y_pred)\n",
        "\n",
        "        overview = pd.DataFrame(\n",
        "            [{\n",
        "                'model':model_name,\n",
        "                'run_id':run_id,\n",
        "                'date': date,\n",
        "                'train_set': subset_train,\n",
        "                'test_set': subset_test,\n",
        "                'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "                'test_set_support':len(predictions),\n",
        "                'split_col':split_col,\n",
        "                'text_col':df.iloc[0]['trunc_col'],\n",
        "                'runtime':sum(predictions['runtime']),\n",
        "                'accuracy': accuracy_score(y_test, y_pred),\n",
        "                'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "                'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "                'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "                'weighted_avg_precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "                'weighted_avg_recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "                'weighted_avg_f1': f1_score(y_test, y_pred, average='weighted'),\n",
        "                'classification_report':report\n",
        "            }   ]\n",
        "        )\n",
        "        # remove previous results of run_id, replace with new/updated results\n",
        "        ph.replace_and_save_df(overview, overview_path, run_id)\n",
        "        print(\"Saving done! Interrupting is allowed.\")\n",
        "        print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up variables that are the same for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'train' # must be dev or train\n",
        "TEST_SET = 'test' # must be val or test\n",
        "SPLIT_COLUMN = 'balanced_split' # column containing to which subset the doc belongs, either val, test, train or dev, depending on the split. \n",
        "LABEL_COLUMN = 'label'\n",
        "TEXT_COLUMN = 'trunc_txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GEITje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "SHORT_MODEL_NAME = 'GEITje'\n",
        "PROMPT = pt.fewshot_prompt_no_template # Change to prompt function you want to use. Needs to be in src/prompt_template.py\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT==pt.zeroshot_prompt_geitje:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_no_template:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - In-context learning\n",
        "Note - ONLY load one model: either in-context or fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49516b7d728c42689f91d896eb9c6b7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9bf069f74f349d29e243b9b67c0299b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ba5eb6298fb4dc19fa3f9b4f83688d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cf2bef1c3fa4aeda18cd9e489af9294",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5c27475eb6146dfb3d57d9988bee4d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0870ac7f6c3c40bcb613f65a2e693ccc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1aa01cb9f4a4d179937b65d1cfa660c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ba413e321ed44548f0c2d12a35602c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce63ce28c0e24e608523fcebbfd5c68b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d237c8db54f74ddaab7c80ec87fa4350",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e34ed387fde475e9fe441dfbf5392b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2ed30c0eccd48d2af1b1e542e4b7fe4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                    device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc77af8a7f6a473e80ee877f3cb340ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/667 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d163fbdf09c4be89d736fd8242a8d53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85a246ea54ec4447aedeffc6147f8325",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ed586cbd79d47e6a75d2f2bd40a212c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f52af9bcbe1b4326b36384719da7cea5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7462ff9d6d3406a8f57e3f518989552",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "686e0dea9fec4566a477250ab137a182",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bf649bc3d4f4aa48274bbf6f71fc48d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46dd8fdf76c2407ebaf61bbc41866266",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8aeea88af97143e08242ccff33346c4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4015cd6c0fc4b489a10bba7e1c762f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1325d4f521fe49a59ec476f348d959c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='FemkeBakker/AmsterdamDocClassificationGEITje200T1Epochs',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'AmsterdamDocClassificationGEITje200T1Epochs'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'\n",
        "EPOCHS = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set-up paths to save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../local_data/predictionsFinal/finetuning/1epochs/overview.pkl\n",
            "../local_data/predictionsFinal/finetuning/1epochs/GEITjeFirst200Last0Predictions.pkl\n",
            "\n",
            " FT_AmsterdamDocClassificationGEITje200T1Epochsfewshot_prompt_no_templateLlamaTokens200_0traintest_numEx2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 4split is only used for the validation experiments\n",
        "if SPLIT_COLUMN == '4split' or SPLIT_COLUMN == '2split':\n",
        "    OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/overview.pkl\"\n",
        "    PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/predictions.pkl\"\n",
        "    \n",
        "# balancedsplit is used for the final experiments\n",
        "elif SPLIT_COLUMN == 'balanced_split':\n",
        "    if SUBFOLDER == 'finetuning':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{EPOCHS}epochs/overview.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{EPOCHS}epochs/{SHORT_MODEL_NAME}First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "    elif SUBFOLDER == 'in_context':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/overview.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "print(OVERVIEW_PATH)\n",
        "print(PREDICTION_PATH)\n",
        "\n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(OVERVIEW_PATH))):\n",
        "    raise ValueError(\"Folder to OVERVIEW_PATH does not exist\") \n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(PREDICTION_PATH))):\n",
        "    raise ValueError(\"Folder to PREDICTION_PATH does not exist\") \n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "print ('\\n', run_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = txt.head(5)\n",
        "txt['balanced_split'] = ['test', 'train', 'train', 'train', 'train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting...last 1 docs\n",
            "label:  raadsadres\n",
            "response:  {'categorie': Raadsadres}\n",
            "prediction: raadsadres\n",
            "Dont interrupt, saving predictions...\n",
            "Saving done! Interrupting is allowed.\n",
            "Accuracy:  1.0\n"
          ]
        }
      ],
      "source": [
        "# ----- EXPERIMENT --------\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_geitje, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "display(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Llama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "SHORT_MODEL_NAME = 'Llama'\n",
        "PROMPT = pt.fewshot_prompt_with_template # Change to prompt function you want to use. Needs to be in src/prompt_template.py\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT==pt.zeroshot_prompt_mistral_llama:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_with_template:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - In-context learning\n",
        "Note - ONLY load one model: either in-context or fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12e14a37201c4e95a31fca3f3905332b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "957cd64068834a4d902edd6830b18ecc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75e996c5339e4f059efd21cdcdee3405",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e92418c9effd446293b215242439a2fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "846170e271dc48cbae945f6cd68aff2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bea16a32eca5436c91231b8be6f13e94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3067d3c1131401fa3a97ad4a37723c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae94af5fb80a48339cfc4b47e5612e20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3b34acfc75043868839526d46b02107",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "775a30e2653541f1b828dc7708f57dcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5ebda17e36a445bbba67ddf0b31b583",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='meta-llama/Llama-2-7b-chat-hf',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "# load llama using cpu, else will give cuda out of memory error when running fewshot bm25 prompt.\n",
        "\n",
        "MODEL_NAME = 'Llama-2-7b-chat-hf'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='FemkeBakker/AmsterdamDocClassificationLlama200T3Epochs',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'AmsterdamDocClassificationLlama200T1Epochs'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'\n",
        "EPOCHS = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set-up paths to save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../local_data/predictionsFinal/in_context/Llama/overview.pkl\n",
            "../local_data/predictionsFinal/in_context/Llama/fewshot_prompt_with_template/First200Last0Predictions.pkl\n",
            "\n",
            " IC_Llama-2-7b-chat-hffewshot_prompt_with_templateLlamaTokens200_0traintest_numEx2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 4split is only used for the validation experiments\n",
        "if SPLIT_COLUMN == '4split' or SPLIT_COLUMN == '2split':\n",
        "    OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/overview.pkl\"\n",
        "    PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/predictions.pkl\"\n",
        "    \n",
        "# balancedsplit is used for the final experiments\n",
        "elif SPLIT_COLUMN == 'balanced_split':\n",
        "    if SUBFOLDER == 'finetuning':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{EPOCHS}epochs/overview.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{EPOCHS}epochs/{SHORT_MODEL_NAME}First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "    elif SUBFOLDER == 'in_context':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/overview.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "print(OVERVIEW_PATH)\n",
        "print(PREDICTION_PATH)\n",
        "\n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(OVERVIEW_PATH))):\n",
        "    raise ValueError(\"Folder to OVERVIEW_PATH does not exist\") \n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(PREDICTION_PATH))):\n",
        "    raise ValueError(\"Folder to PREDICTION_PATH does not exist\") \n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "print ('\\n', run_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting...last 1 docs\n",
            "label:  raadsadres\n",
            "response:   Sure, I can help you classify the documents! Based on the information provided, here are my suggestions for each document:\n",
            "\n",
            "Document 1: AGENDA (concept)\n",
            "Categorie: Agenda\n",
            "\n",
            "Output: {'categorie': 'agenda'}\n",
            "\n",
            "Document 2: Gemeente Amsterdam, Gemeenteraad, 14 januari 2016\n",
            "Categorie: Motie\n",
            "\n",
            "Output: {'categorie':'motie'}\n",
            "\n",
            "Now, please provide the actual document text for the third document, and I will categorize it accordingly.\n",
            "prediction: MultiplePredictionErrorInFormatting\n",
            "Dont interrupt, saving predictions...\n",
            "Saving done! Interrupting is allowed.\n",
            "Accuracy:  0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_llama, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>run_id</th>\n",
              "      <th>date</th>\n",
              "      <th>train_set</th>\n",
              "      <th>test_set</th>\n",
              "      <th>train_set_support</th>\n",
              "      <th>test_set_support</th>\n",
              "      <th>split_col</th>\n",
              "      <th>text_col</th>\n",
              "      <th>runtime</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_avg_precision</th>\n",
              "      <th>macro_avg_recall</th>\n",
              "      <th>macro_avg_f1</th>\n",
              "      <th>weighted_avg_precision</th>\n",
              "      <th>weighted_avg_recall</th>\n",
              "      <th>weighted_avg_f1</th>\n",
              "      <th>classification_report</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>IC_Llama-2-7b-chat-hffewshot_prompt_with_templ...</td>\n",
              "      <td>2024-07-09 14:07:57.212857+02:00</td>\n",
              "      <td>train</td>\n",
              "      <td>test</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>balanced_split</td>\n",
              "      <td>TruncationLlamaTokensFront200Back0</td>\n",
              "      <td>185.056847</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>precision...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model                                             run_id  \\\n",
              "0  Llama-2-7b-chat-hf  IC_Llama-2-7b-chat-hffewshot_prompt_with_templ...   \n",
              "\n",
              "                              date train_set test_set  train_set_support  \\\n",
              "0 2024-07-09 14:07:57.212857+02:00     train     test                  4   \n",
              "\n",
              "   test_set_support       split_col                            text_col  \\\n",
              "0                 1  balanced_split  TruncationLlamaTokensFront200Back0   \n",
              "\n",
              "      runtime  accuracy  macro_avg_precision  macro_avg_recall  macro_avg_f1  \\\n",
              "0  185.056847       0.0                  0.0               0.0           0.0   \n",
              "\n",
              "   weighted_avg_precision  weighted_avg_recall  weighted_avg_f1  \\\n",
              "0                     0.0                  0.0              0.0   \n",
              "\n",
              "                               classification_report  \n",
              "0                                       precision...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "display(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SHORT_MODEL_NAME = 'Mistral'\n",
        "PROMPT = pt.zeroshot_prompt_mistral_llama # Change to prompt function you want to use. Needs to be in src/prompt_template.py\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT==pt.zeroshot_prompt_mistral_llama:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_with_template:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - In-context learning\n",
        "Note - ONLY load one model: either in-context or fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'Mistral-7B-Instruct-v0.2'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='FemkeBakker/AmsterdamDocClassificationMistral200T2Epochs',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'AmsterdamDocClassificationMistral200T2Epochs'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'\n",
        "EPOCHS = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set-up paths to save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 4split is only used for the validation experiments\n",
        "if SPLIT_COLUMN == '4split' or SPLIT_COLUMN == '2split':\n",
        "    OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/overview.pkl\"\n",
        "    PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/predictions.pkl\"\n",
        "    \n",
        "# balancedsplit is used for the final experiments\n",
        "elif SPLIT_COLUMN == 'balanced_split':\n",
        "    if SUBFOLDER == 'finetuning':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{EPOCHS}epochs/overview.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{EPOCHS}epochs/{SHORT_MODEL_NAME}First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "    elif SUBFOLDER == 'in_context':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/overview.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "print(OVERVIEW_PATH)\n",
        "print(PREDICTION_PATH)\n",
        "\n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(OVERVIEW_PATH))):\n",
        "    raise ValueError(\"Folder to OVERVIEW_PATH does not exist\") \n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(PREDICTION_PATH))):\n",
        "    raise ValueError(\"Folder to PREDICTION_PATH does not exist\") \n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "print ('\\n', run_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run experiment\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_mistral, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "display(pred)\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "amsterdamincontextlearning"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "nl"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
