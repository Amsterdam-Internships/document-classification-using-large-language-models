{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "# import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# setup environment GEITje-7B Finetuning\n",
    "# - pip install torch\n",
    "# - pip install datasets\n",
    "# - pip install transformers\n",
    "# - pip install trl\n",
    "# - pip install accelerate (restart after)\n",
    "# - switch device_map='auto' to avaoid memory error\n",
    "\n",
    "# - pip install sentencepiece\n",
    "# - pip install jupyter\n",
    "# - pip install protobuf \n",
    "# pip install bitsandbytes\n",
    "# pip install bnb\n",
    "# pip install wandb==0.13.3 --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "Goal: get predictions of the finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import prompt_template as pt\n",
    "import prediction_helperfunctions as ph\n",
    "import truncation as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-04-29 11:27:58.375087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-29 11:28:02.042065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8f56957c71411b817bae35e38cd50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccc773e51b246dd83a0c7faacb82d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "from transformers import pipeline, Conversation\n",
    "\n",
    "chatbot = pipeline(task='conversational', model='FemkeBakker/GEITjeSmallData200Tokens',\n",
    "                   device_map='auto', model_kwargs={'offload_buffers':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from bm25 import BM25\n",
    "\n",
    "\n",
    "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
    "# docs_df = dataframe with the documents that need to be predicted\n",
    "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
    "# prompt_function = prompt template \n",
    "# train_df = dataframe with docs, which can be used as examples/training data/context data\n",
    "# num_examples = number of examples in the prompt\n",
    "\n",
    "def predictions_incontextlearning(chatbot, docs_df, text_column, prompt_function, train_df, num_examples):\n",
    "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date', 'prompt'])\n",
    "\n",
    "\n",
    "    if prompt_function == pt.fewshot_prompt_bm25:\n",
    "        BM25_model = BM25()\n",
    "        BM25_model.fit(train_df[text_column])\n",
    "\n",
    "    \n",
    "    # prompt each document\n",
    "    for index, row in docs_df.iterrows():\n",
    "        if (index + 1) % 200 == 0:\n",
    "            print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # get the prompt, with the doc filled in\n",
    "        txt = row[text_column]\n",
    "\n",
    "        # each prompt function takes different arguments\n",
    "        # simple function is zeroshot+simple instruction\n",
    "        if prompt_function == pt.simple_prompt:\n",
    "            prompt = prompt_function(txt)\n",
    "      \n",
    "        # select fewshot examples using bm25\n",
    "        elif prompt_function == pt.fewshot_prompt_bm25:\n",
    "            prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Prompt function not recognised. Check if prompt function is in prompt_template.py and included in the options above.\")\n",
    "\n",
    "        # prompt and get the response\n",
    "        converse = chatbot(Conversation(prompt))\n",
    "        response = converse[1]['content']\n",
    "        print(\"label: \", row['label'].lower())\n",
    "        print(\"response: \", response)\n",
    "\n",
    "        # extract prediction from response\n",
    "        prediction = ph.get_prediction_from_response(response)\n",
    "        print(\"prediction:\", prediction)\n",
    "\n",
    "        # save results in dataframe\n",
    "        results_df.loc[len(results_df)] = {\n",
    "            'id': row['id'],\n",
    "            'path' : row['path'],\n",
    "            'text_column' : docs_df.iloc[0]['trunc_col'],\n",
    "            'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
    "            'response':response,\n",
    "            'prediction':prediction,\n",
    "            'label':row['label'].lower(),\n",
    "            'runtime':time.time()-start_time,\n",
    "            'date': ph.get_datetime(),\n",
    "            'prompt':prompt\n",
    "        }\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\"\"\"\n",
    "Function to run GEITje In-Context Learning experiment. \n",
    "The function allows to resume experiment, if run_id matches.\n",
    "\"\"\"\n",
    "# df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
    "# run_id = unqiue for each experiment. \n",
    "# prompt_function = which prompt from prompt_template.py to use\n",
    "# text_col = colum in df where the text is. (Needs to be already truncated)\n",
    "# split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
    "# subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
    "# subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
    "# label_col = column with the true label\n",
    "# prediction_path = path to file where predictions need to be saved.\n",
    "# overview_path = path to file where results of each run need to be saved.\n",
    "# model_name = name of the model. string.\n",
    "# num_exmples = number of exaples given to prompt. zero in case of zeroshot. \n",
    "\n",
    "def run_experiment(chatbot, df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
    "    print(num_examples)\n",
    "    start_time = time.time()\n",
    "    test_df = df.loc[df[split_col]==subset_test]\n",
    "    train_df = df.loc[df[split_col]==subset_train]\n",
    "    \n",
    "    # get rows of df that still need to be predicted for the specific run_id\n",
    "    to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
    "\n",
    "    # devide to_predict into subsection of 50 predictions at a time. \n",
    "    # Allows to rerun without problem. And save subsections of 50 predictions.\n",
    "    step_range = list(range(0, len(to_predict), 25))\n",
    "\n",
    "    for i in range(len(step_range)):\n",
    "        try:\n",
    "            sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
    "            print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
    "        except Exception as e:\n",
    "            sub_to_predict = to_predict[step_range[i]:]\n",
    "            print(f'Starting...last {len(sub_to_predict)} docs')\n",
    "\n",
    "        # prompt geitje\n",
    "        predictions = predictions_incontextlearning(chatbot, sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
    "\n",
    "        # save info\n",
    "        predictions['run_id'] = run_id\n",
    "        predictions['train_set'] = subset_train\n",
    "        predictions['test_set'] = subset_test\n",
    "        predictions['shots'] = num_examples\n",
    "\n",
    "        # save new combinations in file\n",
    "        ph.combine_and_save_df(predictions, prediction_path)\n",
    "\n",
    "        # if previous predictions, combine previous with new predictions, to get update classification report\n",
    "        try:\n",
    "            predictions = pd.concat([predictions, previous_predictions])\n",
    "\n",
    "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
    "            previous_predictions = predictions\n",
    "        except Exception as e:\n",
    "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
    "            previous_predictions = predictions\n",
    "\n",
    "        # save results in overview file\n",
    "        date = ph.get_datetime()\n",
    "        y_test = predictions['label']\n",
    "        y_pred = predictions['prediction']\n",
    "        report = classification_report(y_test, y_pred)\n",
    "\n",
    "        overview = pd.DataFrame(\n",
    "            [{\n",
    "                'model':model_name,\n",
    "                'run_id':run_id,\n",
    "                'date': date,\n",
    "                'train_set': subset_train,\n",
    "                'test_set': subset_test,\n",
    "                'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
    "                'test_set_support':len(predictions),\n",
    "                'split_col':split_col,\n",
    "                'text_col':df.iloc[0]['trunc_col'],\n",
    "                'runtime':sum(predictions['runtime']),\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
    "                'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
    "                'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
    "                'classification_report':report\n",
    "            }   ]\n",
    "        )\n",
    "        # remove previous results of run_id, replace with new/updated results\n",
    "        ph.replace_and_save_df(overview, overview_path, run_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set  variables, same for each model\n",
    "TRAIN_SET = 'dev' # must be dev or train\n",
    "TEST_SET = 'val' # must be val or test\n",
    "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
    "LABEL_COLUMN = 'label'\n",
    "TEXT_COLUMN = 'trunc_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PREDICTION_PATH = f\"{cf.output_path}/predictions/GeitjeFinetuningPredictions.pkl\"\n",
    "OVERVIEW_PATH = f\"{cf.output_path}/overview/GeitjeFinetuningPredictions.pkl\"\n",
    "\n",
    "\n",
    "MODEL_NAME = 'FemkeBakker/GEITjeSmallData200Tokens'\n",
    "PROMPT = pt.simple_prompt\n",
    "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
    "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
    "FRONT_THRESHOLD = 200\n",
    "BACK_THRESHOLD = 0\n",
    "NUMBER_EXAMPLES = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Starting...0:25 out of 209\n",
      "label:  agenda\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n",
      "label:  brief\n",
      "response:  {'categorie': Brief}\n",
      "prediction: brief\n",
      "label:  agenda\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n",
      "label:  onderzoeksrapport\n",
      "response:  {'categorie': Onderzoeksrapport}\n",
      "prediction: onderzoeksrapport\n",
      "label:  factsheet\n",
      "response:  {'categorie': Factsheet}\n",
      "prediction: factsheet\n",
      "label:  voordracht\n",
      "response:  {'categorie': Voordracht}\n",
      "prediction: voordracht\n",
      "label:  motie\n",
      "response:  {'categorie': Raadsadres}\n",
      "prediction: raadsadres\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/logging/__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/logging/__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_7167/2423823012.py\", line 7, in <module>\n",
      "    run_experiment(chatbot, trunc_df, f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}', PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n",
      "  File \"/tmp/ipykernel_7167/607904531.py\", line 44, in run_experiment\n",
      "    predictions = predictions_incontextlearning(chatbot, sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
      "  File \"/tmp/ipykernel_7167/3287996474.py\", line 46, in predictions_incontextlearning\n",
      "    converse = chatbot(Conversation(prompt))\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/pipelines/conversational.py\", line 287, in __call__\n",
      "    outputs = super().__call__(conversations, num_workers=num_workers, **kwargs)\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  raadsadres\n",
      "response:  {'categorie': Raadsadres}\n",
      "prediction: raadsadres\n",
      "label:  raadsadres\n",
      "response:  {'categorie': Raadsadres}\n",
      "prediction: raadsadres\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  brief\n",
      "response:  {'categorie': Brief}\n",
      "prediction: brief\n",
      "label:  agenda\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n",
      "label:  agenda\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  voordracht\n",
      "response:  {'categorie': Voordracht}\n",
      "prediction: voordracht\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  agenda\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  actualiteit\n",
      "response:  {'categorie': Actualiteit}\n",
      "prediction: actualiteit\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  voordracht\n",
      "response:  {'categorie': Voordracht}\n",
      "prediction: voordracht\n",
      "Starting...25:50 out of 209\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  agenda\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Memo}\n",
      "prediction: NoPredictionInOutput\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  voordracht\n",
      "response:  {'categorie': Voordracht}\n",
      "prediction: voordracht\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  voordracht\n",
      "response:  {'categorie': Voordracht}\n",
      "prediction: voordracht\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  raadsadres\n",
      "response:  {'categorie': Raadsadres}\n",
      "prediction: raadsadres\n",
      "label:  onderzoeksrapport\n",
      "response:  {'categorie': Plan}\n",
      "prediction: NoPredictionInOutput\n",
      "label:  onderzoeksrapport\n",
      "response:  {'categorie': Onderzoeksrapport}\n",
      "prediction: onderzoeksrapport\n",
      "label:  onderzoeksrapport\n",
      "response:  {'categorie': Onderzoeksrapport}\n",
      "prediction: onderzoeksrapport\n",
      "label:  brief\n",
      "response:  {'categorie': Brief}\n",
      "prediction: brief\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  agenda\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  actualiteit\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...50:75 out of 209\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  raadsadres\n",
      "response:  {'categorie': Aanbiedingsformulier}\n",
      "prediction: NoPredictionInOutput\n",
      "label:  agenda\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n",
      "label:  agenda\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  agenda\n",
      "response:  {'categorie': Agenda}\n",
      "prediction: agenda\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  brief\n",
      "response:  {'categorie': Brief}\n",
      "prediction: brief\n",
      "label:  brief\n",
      "response:  {'categorie': Brief}\n",
      "prediction: brief\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  raadsadres\n",
      "response:  {'categorie': Raadsadres}\n",
      "prediction: raadsadres\n",
      "label:  onderzoeksrapport\n",
      "response:  {'categorie': Onderzoeksrapport}\n",
      "prediction: onderzoeksrapport\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  onderzoeksrapport\n",
      "response:  {'categorie': Onderzoeksrapport}\n",
      "prediction: onderzoeksrapport\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  brief\n",
      "response:  {'categorie': Brief}\n",
      "prediction: brief\n",
      "label:  voordracht\n",
      "response:  {'categorie': Voordracht}\n",
      "prediction: voordracht\n",
      "label:  voordracht\n",
      "response:  {'categorie': Voordracht}\n",
      "prediction: voordracht\n",
      "label:  schriftelijke vraag\n",
      "response:  {'categorie': Schriftelijke Vraag}\n",
      "prediction: schriftelijke vraag\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  voordracht\n",
      "response:  {'categorie': Voordracht}\n",
      "prediction: voordracht\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...75:100 out of 209\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  motie\n",
      "response:  {'categorie': Motie}\n",
      "prediction: motie\n",
      "label:  voordracht\n",
      "response:  {'categorie': Voordracht}\n",
      "prediction: voordracht\n",
      "label:  onderzoeksrapport\n",
      "response:  {'categorie': Onderzoeksrapport}\n",
      "prediction: onderzoeksrapport\n",
      "label:  voordracht\n",
      "response:  {'categorie': Voordracht}\n",
      "prediction: voordracht\n"
     ]
    }
   ],
   "source": [
    "# ----- EXPERIMENT --------\n",
    "\n",
    "# add new column with truncated text -> new dataframe with column + new column name\n",
    "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
    "\n",
    "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
    "run_experiment(chatbot, trunc_df, f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}', PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AmsterdamInContextLearning",
   "language": "python",
   "name": "amsterdamincontextlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
