{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "- Get insight into tokenizer, tokens and doc lengths.\n",
    "- Test different text truncation thresholds on the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text truncation -- overview in tokenizer/doc lengths\n",
    "- check how many docs exceed max\n",
    "\n",
    "- tokenize text using tokenizer of mistral, geitje and Llama.\n",
    "- Check if mistral and geitje indeed have the same tokenizer.\n",
    "- After getting the tokens, check distribution and how many exceed max_threshold.\n",
    "\n",
    "Results are saved in txtfiles_tokenizer.pkl, so that txtfiles.pkl is a back-up file, in case anything gets messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_tokens(model_name, df, save_to_path, text_col, new_col_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    all_texts = list(df[text_col].values)\n",
    "\n",
    "    all_tokens = []\n",
    "    all_tokens_len = []\n",
    "    for txt in all_texts:\n",
    "        tokens = tokenizer.tokenize(txt)\n",
    "        all_tokens.append(tokens)\n",
    "        all_tokens_len.append(len(tokens))\n",
    "\n",
    "    df[new_col_name] = all_tokens\n",
    "    df[f\"count_{new_col_name}\"] = all_tokens_len\n",
    "    df.to_pickle(save_to_path)\n",
    "    return df\n",
    "\n",
    "# subdf = df.iloc[0:2]\n",
    "# # display(subdf)\n",
    "# get_token_length('Rijgersberg/GEITje-7B-chat-v2', subdf, f\"{cf.output_path}/try_out_token_count.pkl\", 'text', 'token_count_geitje')\n",
    "\n",
    "def fraction_token(df, max_token, token_len_col):\n",
    "    for col in token_len_col:\n",
    "        print(f\"{len(df.loc[df[col]>max_token])} out of {len(df)} ({round(len(df.loc[df[col]>max_token])/len(df)*100, 2)}%) docs exceed a token length of {max_token}\")\n",
    "\n",
    "    for col in token_len_col:\n",
    "        print(df[col].describe())\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get token lengths of model tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"GEITje\"\"\" ## not necesarry -> since tokenizer is the same as mistral\n",
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# get_tokens('Rijgersberg/GEITje-7B-chat-v2', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'GEITjeTokens')\n",
    "\n",
    "\"\"\"Mistral\"\"\"\n",
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# get_tokens('mistralai/Mistral-7B-v0.1', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'MistralTokens')\n",
    "\n",
    "\"\"\"Llama\"\"\"\n",
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# get_tokens('meta-llama/Llama-2-7b-hf', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'LlamaTokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse token length of model tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tok = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# fraction_token(tok, 4096, ['count_MistralTokens', 'count_LlamaTokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test text truncation on baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_truncation_column(df,text_col, token_col_name, front_token_threshold, back_token_threshold=0):\n",
    "    input = []\n",
    "    for index, row in df.iterrows():\n",
    "        # select text according to the token threshold -> first FRONT\n",
    "        # select first n (= token_theshold) tokens using the model tokenizer\n",
    "        tokens = row[token_col_name][0:front_token_threshold]\n",
    "\n",
    "        # combine tokens into txt\n",
    "        tokens_txt = ''.join(tokens)\n",
    "\n",
    "        # \\n is converted by tokenizer to <0x0A>, we reverse this to get original length\n",
    "        len_char = len(tokens_txt.replace(\"<0x0A>\", \"\\n\")) # get character length\n",
    "\n",
    "        # select the same amount of characters as the tokens\n",
    "        front_txt = row[text_col][0:len_char]\n",
    "\n",
    "        # Check if back of document also given as input\n",
    "        if back_token_threshold != 0:\n",
    "            # select LAST n (= token_theshold) tokens using the model tokenizer\n",
    "            tokens = row[token_col_name][-back_token_threshold:]\n",
    "\n",
    "            # combine tokens into txt\n",
    "            tokens_txt = ''.join(tokens)\n",
    "\n",
    "            # \\n is converted by tokenizer to <0x0A>, we reverse this to get original length\n",
    "            len_char = len(tokens_txt.replace(\"<0x0A>\", \"\\n\")) # get character length\n",
    "\n",
    "            # select the same amount of characters as the tokens\n",
    "            back_txt = row[text_col][-len_char:]\n",
    "\n",
    "            # combine front and back text\n",
    "            input_txt = front_txt + ' ' + back_txt\n",
    "\n",
    "        else:\n",
    "            input_txt = front_txt\n",
    "\n",
    "        input.append(input_txt)\n",
    "\n",
    "    df['trunc_txt'] = input\n",
    "    df['trunc_col'] = f\"Truncation{token_col_name}Front{front_token_threshold}Back{back_token_threshold}\"\n",
    "    return df\n",
    "\n",
    "# trunc = add_truncation_column(tok, 'text', 'GEITjeTokens', 50,50)\n",
    "# display(trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "front_thresholds = [0,100, 200, 500, 1000, 2000]\n",
    "back_thresholds = [0,100,200,500,1000,2000]\n",
    "all_combinations = list(itertools.product(front_thresholds, back_thresholds))\n",
    "\n",
    "# remove combinations which have more than 2000 tokens.\n",
    "all_combinations = [comb for comb in all_combinations if sum(comb) <= 2000 and sum(comb)>0]\n",
    "\n",
    "# load file with baseline function\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import baseline as bf\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "# variables for text truncation\n",
    "DATAFRAME = tok\n",
    "TEXT_COL = 'text'\n",
    "TOKENS_COL = 'LlamaTokens'\n",
    "\n",
    "# variables for baseline\n",
    "BASELINE_FUNCTION = LinearSVC()\n",
    "MODEL_NAME = 'LinearSVC'\n",
    "TRAIN_SET = 'train' # must be dev or train\n",
    "TEST_SET = 'test' # must be val or test\n",
    "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
    "LABEL_COLUMN = 'label'\n",
    "PREDICTION_PATH = f\"{cf.output_path}/predictions/baselineTruncationPredictions.pkl\"\n",
    "OVERVIEW_PATH = f\"{cf.output_path}/overview/baselineTruncationOverview.pkl\"\n",
    "TRUNC_COLUMN = 'trunc_txt'\n",
    "threshold_combinations =[(100,0), (200,0), (500,0), (1000,0), (2000,0), (100,100),(200,200), (500,500), (1000,1000), (0,100), (0,200), (0,500), (0,1000), (0,2000)]\n",
    "threshold_combinations = threshold_combinations[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "         Actualiteit       0.97      0.78      0.86       183\n",
      "              Agenda       0.99      0.99      0.99       935\n",
      "             Besluit       0.99      0.99      0.99       145\n",
      "               Brief       0.93      0.87      0.90       396\n",
      "          Factsheets       0.61      0.36      0.45        47\n",
      "               Motie       0.95      0.98      0.96      1644\n",
      "   Onderzoeksrapport       0.82      0.95      0.88       263\n",
      "          Raadsadres       0.90      0.94      0.91       385\n",
      "        Raadsnotulen       1.00      0.98      0.99        55\n",
      "Schriftelijke Vragen       1.00      0.95      0.97       591\n",
      "          Voordracht       0.99      1.00      1.00       696\n",
      "\n",
      "            accuracy                           0.96      5340\n",
      "           macro avg       0.92      0.89      0.90      5340\n",
      "        weighted avg       0.96      0.96      0.95      5340\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loop through all thresholds and save predictions\n",
    "for thresholds in threshold_combinations:\n",
    "    front_threshold = thresholds[0]\n",
    "    back_threshold = thresholds[1]\n",
    "    trunc = add_truncation_column(DATAFRAME, TEXT_COL, TOKENS_COL, front_threshold,back_threshold)\n",
    "    bf.run_baseline(BASELINE_FUNCTION,MODEL_NAME, trunc, SPLIT_COLUMN, TRAIN_SET, TEST_SET, TRUNC_COLUMN, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>date</th>\n",
       "      <th>train_set</th>\n",
       "      <th>test_set</th>\n",
       "      <th>train_set_support</th>\n",
       "      <th>test_set_support</th>\n",
       "      <th>split_col</th>\n",
       "      <th>text_col</th>\n",
       "      <th>runtime</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_avg_precision</th>\n",
       "      <th>macro_avg_recall</th>\n",
       "      <th>macro_avg_f1</th>\n",
       "      <th>classification_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>2024-04-23 17:16:25.669297+02:00</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "      <td>20028</td>\n",
       "      <td>5340</td>\n",
       "      <td>4split</td>\n",
       "      <td>X Gemeente Amsterdam J C\\n% Raadscommissie voo...</td>\n",
       "      <td>35.418173</td>\n",
       "      <td>0.955618</td>\n",
       "      <td>0.9217</td>\n",
       "      <td>0.889681</td>\n",
       "      <td>0.90133</td>\n",
       "      <td>precision    recall  f1-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>2024-04-23 17:18:38.664940+02:00</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "      <td>20028</td>\n",
       "      <td>5340</td>\n",
       "      <td>4split</td>\n",
       "      <td>TruncationLlamaTokensFront100Back0</td>\n",
       "      <td>57.089323</td>\n",
       "      <td>0.955618</td>\n",
       "      <td>0.9217</td>\n",
       "      <td>0.889681</td>\n",
       "      <td>0.90133</td>\n",
       "      <td>precision    recall  f1-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model                             date train_set test_set  \\\n",
       "0  LinearSVC 2024-04-23 17:16:25.669297+02:00     train     test   \n",
       "0  LinearSVC 2024-04-23 17:18:38.664940+02:00     train     test   \n",
       "\n",
       "   train_set_support  test_set_support split_col  \\\n",
       "0              20028              5340    4split   \n",
       "0              20028              5340    4split   \n",
       "\n",
       "                                            text_col    runtime  accuracy  \\\n",
       "0  X Gemeente Amsterdam J C\\n% Raadscommissie voo...  35.418173  0.955618   \n",
       "0                 TruncationLlamaTokensFront100Back0  57.089323  0.955618   \n",
       "\n",
       "   macro_avg_precision  macro_avg_recall  macro_avg_f1  \\\n",
       "0               0.9217          0.889681       0.90133   \n",
       "0               0.9217          0.889681       0.90133   \n",
       "\n",
       "                               classification_report  \n",
       "0                        precision    recall  f1-...  \n",
       "0                        precision    recall  f1-...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "yeet = pd.read_pickle(OVERVIEW_PATH)\n",
    "# yeet = yeet.sort_values(by=['accuracy','macro_avg_f1'], ascending=False)\n",
    "display(yeet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AmsterdamInContextLearning",
   "language": "python",
   "name": "amsterdamincontextlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
