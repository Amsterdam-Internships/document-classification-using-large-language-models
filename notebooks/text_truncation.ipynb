{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text truncation\n",
    "- check how many docs exceed max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1964 out of 26704 (7.35%) docs exceed max window length of mistral (4096) \n",
      "7831 out of 26704 (29.33%) docs are longer than 1000 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(df.loc[df['token_count']>4096])} out of {len(df)} ({round(len(df.loc[df['token_count']>4096])/len(df)*100, 2)}%) docs exceed max window length of mistral (4096) \")\n",
    "print(f\"{len(df.loc[df['token_count']>1000])} out of {len(df)} ({round(len(df.loc[df['token_count']>1000])/len(df)*100, 2)}%) docs are longer than 1000 tokens\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_count</th>\n",
       "      <th>clean_tokens_count</th>\n",
       "      <th>num_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7831.000000</td>\n",
       "      <td>7831.000000</td>\n",
       "      <td>7831.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5867.532499</td>\n",
       "      <td>2937.387435</td>\n",
       "      <td>13.957732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12054.449388</td>\n",
       "      <td>6257.314713</td>\n",
       "      <td>25.415472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1001.000000</td>\n",
       "      <td>439.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1305.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1905.000000</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4128.000000</td>\n",
       "      <td>2089.500000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>275597.000000</td>\n",
       "      <td>143782.000000</td>\n",
       "      <td>502.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token_count  clean_tokens_count    num_pages\n",
       "count    7831.000000         7831.000000  7831.000000\n",
       "mean     5867.532499         2937.387435    13.957732\n",
       "std     12054.449388         6257.314713    25.415472\n",
       "min      1001.000000          439.000000     1.000000\n",
       "25%      1305.000000          660.000000     4.000000\n",
       "50%      1905.000000          954.000000     5.000000\n",
       "75%      4128.000000         2089.500000    11.000000\n",
       "max    275597.000000       143782.000000   502.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.loc[df['token_count']>1000].describe().drop(columns=['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeet = pd.read_pickle(f\"{cf.output_path}/txtfiles_notcleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2452\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['token_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_token_length(model_name, df, save_to_path, text_col, new_col_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    all_texts = list(df[text_col].values)\n",
    "\n",
    "    all_len_tokens = []\n",
    "    for txt in all_texts:\n",
    "        len_tokens = len(tokenizer.tokenize(txt))\n",
    "        all_len_tokens.append(len_tokens)\n",
    "\n",
    "    df[new_col_name] = all_len_tokens\n",
    "    df.to_pickle(save_to_path)\n",
    "    return df\n",
    "\n",
    "# subdf = df.iloc[0:2]\n",
    "# # display(subdf)\n",
    "# get_token_length('Rijgersberg/GEITje-7B-chat-v2', subdf, f\"{cf.output_path}/try_out_token_count.pkl\", 'text', 'token_count_geitje')\n",
    "\n",
    "def fraction_token(df, max_token, token_len_col):\n",
    "    for col in token_len_col:\n",
    "        print(f\"{len(df.loc[df[col]>max_token])} out of {len(df)} ({round(len(df.loc[df[col]>max_token])/len(df)*100, 2)}%) docs exceed a token length of {max_token}\")\n",
    "\n",
    "    for col in token_len_col:\n",
    "        print(df[col].describe())\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get token lengths of model tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"GEITje\"\"\"\n",
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n",
    "# get_token_length('Rijgersberg/GEITje-7B-chat-v2', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'token_count_geitje')\n",
    "\n",
    "\"\"\"Mistral\"\"\"\n",
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# get_token_length('mistralai/Mistral-7B-v0.1', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'token_count_mistral')\n",
    "\n",
    "\"\"\"Llama\"\"\"\n",
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# get_token_length('meta-llama/Llama-2-7b-hf', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'token_count_llama2_7b_hf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse token length of model tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4005 out of 26704 (15.0%) docs exceed a token length of 4096\n",
      "4005 out of 26704 (15.0%) docs exceed a token length of 4096\n",
      "3839 out of 26704 (14.38%) docs exceed a token length of 4096\n",
      "count     26704.000000\n",
      "mean       4310.098749\n",
      "std       14728.324592\n",
      "min          75.000000\n",
      "25%         680.000000\n",
      "50%        1213.000000\n",
      "75%        2609.000000\n",
      "max      621995.000000\n",
      "Name: token_count_geitje, dtype: float64\n",
      "count     26704.000000\n",
      "mean       4310.098749\n",
      "std       14728.324592\n",
      "min          75.000000\n",
      "25%         680.000000\n",
      "50%        1213.000000\n",
      "75%        2609.000000\n",
      "max      621995.000000\n",
      "Name: token_count_mistral, dtype: float64\n",
      "count     26704.000000\n",
      "mean       4167.322124\n",
      "std       14243.284787\n",
      "min          74.000000\n",
      "25%         662.000000\n",
      "50%        1184.000000\n",
      "75%        2530.000000\n",
      "max      618067.000000\n",
      "Name: token_count_llama2_7b_hf, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_token_len = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "fraction_token(df_token_len, 4096, ['token_count_geitje', 'token_count_mistral', 'token_count_llama2_7b_hf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# get_token_length('mistralai/Mistral-7B-v0.1', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'token_count_mistral')\n",
    "\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Tokenize some input.\n",
    "# tokens = tokenizer.tokenize(txt)\n",
    "# print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_token(df_token_len, 1000, 'token_count_mistral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# get_token_length('meta-llama/Llama-2-7b-hf', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'token_count_llama2_7b_hf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4924\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize some input.\n",
    "tokens = tokenizer.tokenize(txt)\n",
    "print(len(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AmsterdamInContextLearning",
   "language": "python",
   "name": "amsterdamincontextlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
