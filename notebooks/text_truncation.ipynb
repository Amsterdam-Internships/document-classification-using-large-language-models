{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "- Get insight into tokenizer, tokens and doc lengths.\n",
    "- Test different text truncation thresholds on the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text truncation -- overview in tokenizer/doc lengths\n",
    "- tokenize text using tokenizer of mistral, geitje and Llama.\n",
    "- Check if mistral and geitje indeed have the same tokenizer.\n",
    "- After getting the tokens, check distribution.\n",
    "- Truncate text and test multiple thresholds on baseline\n",
    "\n",
    "Results are saved in txtfiles_tokenizer.pkl, so that txtfiles.pkl is a back-up file, in case anything gets messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_tokens(model_name, df, save_to_path, text_col, new_col_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    all_texts = list(df[text_col].values)\n",
    "\n",
    "    all_tokens = []\n",
    "    all_tokens_len = []\n",
    "    for txt in all_texts:\n",
    "        tokens = tokenizer.tokenize(txt)\n",
    "        all_tokens.append(tokens)\n",
    "        all_tokens_len.append(len(tokens))\n",
    "\n",
    "    df[new_col_name] = all_tokens\n",
    "    df[f\"count_{new_col_name}\"] = all_tokens_len\n",
    "    df.to_pickle(save_to_path)\n",
    "    return df\n",
    "\n",
    "# subdf = df.iloc[0:2]\n",
    "# # display(subdf)\n",
    "# get_token_length('Rijgersberg/GEITje-7B-chat-v2', subdf, f\"{cf.output_path}/try_out_token_count.pkl\", 'text', 'token_count_geitje')\n",
    "\n",
    "def fraction_token(df, max_token, token_len_col):\n",
    "    for col in token_len_col:\n",
    "        print(f\"{len(df.loc[df[col]>max_token])} out of {len(df)} ({round(len(df.loc[df[col]>max_token])/len(df)*100, 2)}%) docs exceed a token length of {max_token}\")\n",
    "\n",
    "    for col in token_len_col:\n",
    "        print(df[col].describe())\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GEITje\"\"\" ## not necesarry -> since tokenizer is the same as mistral\n",
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# get_tokens('Rijgersberg/GEITje-7B-chat-v2', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'GEITjeTokens')\n",
    "\n",
    "\"\"\"Mistral\"\"\"\n",
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# get_tokens('mistralai/Mistral-7B-v0.1', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'MistralTokens')\n",
    "\n",
    "\"\"\"Llama\"\"\"\n",
    "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# get_tokens('meta-llama/Llama-2-7b-hf', df, f\"{cf.output_path}/txtfiles_tokenizer.pkl\", 'text', 'LlamaTokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse token length of model tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tok = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
    "# fraction_token(tok, 4096, ['count_MistralTokens', 'count_LlamaTokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test text truncation on baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "# load file with baseline function\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import baseline as bf\n",
    "\n",
    "# load file with truncation function\n",
    "from truncation import add_truncation_column\n",
    "\n",
    "# variables for text truncation\n",
    "DATAFRAME = tok\n",
    "TEXT_COL = 'text'\n",
    "TOKENS_COL = 'LlamaTokens'\n",
    "\n",
    "# variables for baseline\n",
    "BASELINE_FUNCTION = MultinomialNB()\n",
    "MODEL_NAME = 'MultinomialNB'\n",
    "TRAIN_SET = 'train' # must be dev or train\n",
    "TEST_SET = 'test' # must be val or test\n",
    "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
    "LABEL_COLUMN = 'label'\n",
    "PREDICTION_PATH = f\"{cf.output_path}/predictions/baselineTruncationPredictions.pkl\"\n",
    "OVERVIEW_PATH = f\"{cf.output_path}/overview/baselineTruncationOverview.pkl\"\n",
    "# PREDICTION_PATH = f\"{cf.output_path}/predictions/tryoutBaselineTruncationPredictions.pkl\"\n",
    "# OVERVIEW_PATH = f\"{cf.output_path}/overview/tryoutBaselineTruncationOverview.pkl\"\n",
    "TRUNC_COLUMN = 'trunc_txt'\n",
    "threshold_combinations =[(100,0), (200,0), (500,0), (1000,0), (2000,0), (100,100),(200,200), (500,500), (1000,1000), (0,100), (0,200), (0,500), (0,1000), (0,2000)]\n",
    "threshold_combinations =[(200,200), (500,500), (1000,1000), (0,100), (0,200), (0,500), (0,1000), (0,2000)]\n",
    "\n",
    "# threshold_combinations = [(100,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all thresholds and save predictions\n",
    "for thresholds in threshold_combinations:\n",
    "    front_threshold = thresholds[0]\n",
    "    back_threshold = thresholds[1]\n",
    "    trunc = add_truncation_column(DATAFRAME, TEXT_COL, TOKENS_COL, front_threshold,back_threshold)\n",
    "    bf.run_baseline(BASELINE_FUNCTION,MODEL_NAME, trunc, SPLIT_COLUMN, TRAIN_SET, TEST_SET, TRUNC_COLUMN, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "PREDICTION_PATH = f\"{cf.output_path}/predictions/baselineTruncationPredictions.pkl\"\n",
    "OVERVIEW_PATH = f\"{cf.output_path}/overview/baselineTruncationOverview.pkl\"\n",
    "\n",
    "yeet = pd.read_pickle(OVERVIEW_PATH)\n",
    "yeet = yeet.sort_values(by=['macro_avg_f1', 'accuracy'], ascending=False)\n",
    "display(yeet)\n",
    "\n",
    "bl = pd.read_pickle(f\"{cf.output_path}/overview/baselineOverview.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ls = yeet.loc[yeet['model']=='LinearSVC']\n",
    "ls = pd.concat([bl.loc[bl['model']=='LinearSVC'], yeet.loc[yeet['model']=='LinearSVC']]).sort_values(by=['macro_avg_f1', 'accuracy'], ascending=False)\n",
    "ls = ls.loc[~ls['text_col'].isin(['TruncationLlamaTokensFront2000Back0', 'TruncationLlamaTokensFront1000Back0', 'TruncationLlamaTokensFront1000Back1000', 'TruncationLlamaTokensFront0Back2000',''])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = yeet.loc[yeet['model']=='MultinomialNB']\n",
    "# nb = nb.loc[~nb['text_col'].isin(['TruncationLlamaTokensFront2000Back0', 'TruncationLlamaTokensFront1000Back0', 'TruncationLlamaTokensFront1000Back1000', 'TruncationLlamaTokensFront0Back2000',''])]\n",
    "display(nb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best cut-off point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DummyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
