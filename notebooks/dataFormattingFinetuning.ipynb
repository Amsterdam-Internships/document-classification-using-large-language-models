{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "Goal: turn dataframe with txt files, into dataset format, to finetune GEITje.\n",
    "The dataset is save to huggingface.\n",
    "\n",
    "Funtion allows to truncate text. txtfiles_tokenizer, should already indlue columns MistralTokens and LlamaTokens, with the tokens split using the models tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load example data**\n",
    "\n",
    "Datasets used in Finetuning.py example of the GitHub of GEITje. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets,Dataset\t\n",
    "\n",
    "\n",
    "no_robots_nl = load_dataset('Rijgersberg/no_robots_nl')\n",
    "print(no_robots_nl)\n",
    "no_robots_nl[\"train_sft\"]=no_robots_nl[\"train_sft\"].select(range(2))\n",
    "no_robots_nl[\"test_sft\"]=no_robots_nl[\"test_sft\"].select(range(2))\n",
    "\n",
    "ultrachat_nl = load_dataset('Rijgersberg/ultrachat_10k_nl')\n",
    "print(ultrachat_nl)\n",
    "ultrachat_nl[\"train_sft\"]=ultrachat_nl[\"train_sft\"].select(range(2))\n",
    "ultrachat_nl[\"test_sft\"]=ultrachat_nl[\"test_sft\"].select(range(2))\n",
    "\n",
    "chat_dataset = DatasetDict({\n",
    "    'train_sft': concatenate_datasets([no_robots_nl['train_sft'],\n",
    "                                        ultrachat_nl['train_sft']]).shuffle(seed=42),\n",
    "    'test_sft': concatenate_datasets([no_robots_nl['test_sft'],\n",
    "                                        ultrachat_nl['test_sft']]).shuffle(seed=42),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspection**: we only need the columns prompt_id (equal to doc id) and messages_nl. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text using GEITje/Mistral tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load amsterdam data     \n",
    "df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets,Dataset\t\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import prompt_template as pt\n",
    "\n",
    "def format_message(input_txt, label):\n",
    "    message_user = {\n",
    "        \"content\":pt.zeroshot_prompt_mistral_llama(input_txt, remove_template=True),\n",
    "        'role':'user'\n",
    "    }\n",
    "\n",
    "    message_model = {\n",
    "        \"content\":f\"{{'categorie': {label}}}\",\n",
    "        'role':'assistant'\n",
    "    }\n",
    "\n",
    "    return [message_user, message_model]\n",
    "\n",
    "\n",
    "\n",
    "def format_data(df, text_col, model_token_col, label_col, split_col,  token_threshold='full_text'):\n",
    "    format_df = pd.DataFrame(columns=['prompt_id', 'message', split_col])\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        # select whole text\n",
    "        if token_threshold == 'full_text':\n",
    "            input_txt = row[text_col]\n",
    "\n",
    "        # else select text according to the token threshold\n",
    "        else:\n",
    "            first_tokens_threshold = token_threshold[0]\n",
    "            last_tokens_threshold = token_threshold[1]\n",
    "\n",
    "            # select first tokens\n",
    "            tokens = row[model_token_col][0:first_tokens_threshold]\n",
    "\n",
    "            # combine tokens into txt\n",
    "            tokens_txt = ''.join(tokens)\n",
    "\n",
    "            # \\n is converted by tokenizer to <0x0A>, we reverse this to get original length\n",
    "            len_char = len(tokens_txt.replace(\"<0x0A>\", \"\\n\")) # get character length\n",
    "\n",
    "            # select the same amount of characters as the tokens\n",
    "            front_txt = row[text_col][0:len_char]\n",
    "\n",
    "            # Check if back of document also given as input\n",
    "            if last_tokens_threshold != 0:\n",
    "                # select LAST n (= token_theshold) tokens using the model tokenizer\n",
    "                tokens = row[model_token_col][-last_tokens_threshold:]\n",
    "\n",
    "                # combine tokens into txt\n",
    "                tokens_txt = ''.join(tokens)\n",
    "\n",
    "                # \\n is converted by tokenizer to <0x0A>, we reverse this to get original length\n",
    "                len_char = len(tokens_txt.replace(\"<0x0A>\", \"\\n\")) # get character length\n",
    "\n",
    "                # select the same amount of characters as the tokens\n",
    "                back_txt = row[text_col][-len_char:]\n",
    "\n",
    "                # combine front and back text\n",
    "                input_txt = front_txt + ' ' + back_txt\n",
    "\n",
    "            else:\n",
    "                input_txt = front_txt\n",
    "        \n",
    "        # format message\n",
    "        label = row[label_col]\n",
    "        message = format_message(input_txt, label)\n",
    "\n",
    "        # save in dataframe\n",
    "        format_df.loc[len(format_df)] = {'prompt_id':row['id'], 'message':message, split_col:row[split_col]}\n",
    "\n",
    "    # split data\n",
    "    train_set = format_df.loc[format_df[split_col]=='train'].drop(columns=[split_col])\n",
    "    test_set = format_df.loc[format_df[split_col]=='test'].drop(columns=[split_col])\n",
    "\n",
    "\n",
    "    if split_col == '4split':\n",
    "        dev_set = format_df.loc[format_df[split_col]=='dev'].drop(columns=[split_col])\n",
    "        val_set = format_df.loc[format_df[split_col]=='val'].drop(columns=[split_col])\n",
    "\n",
    "        chat_dataset = DatasetDict({\n",
    "            'train': Dataset.from_pandas(train_set).remove_columns('__index_level_0__'),\n",
    "            'test': Dataset.from_pandas(test_set).remove_columns('__index_level_0__'),\n",
    "            'dev': Dataset.from_pandas(dev_set).remove_columns('__index_level_0__'),\n",
    "            'val': Dataset.from_pandas(val_set).remove_columns('__index_level_0__')\n",
    "        })\n",
    "\n",
    "    elif split_col == '2split':\n",
    "            chat_dataset = DatasetDict({\n",
    "            'train': Dataset.from_pandas(train_set).remove_columns('__index_level_0__'),\n",
    "            'test': Dataset.from_pandas(test_set).remove_columns('__index_level_0__'),\n",
    "        })\n",
    "\n",
    "    elif split_col == 'balanced_split':\n",
    "        val_set = format_df.loc[format_df[split_col]=='val'].drop(columns=[split_col])\n",
    "        discarded_set = format_df.loc[format_df[split_col]=='discard'].drop(columns=[split_col])\n",
    "\n",
    "        chat_dataset = DatasetDict({\n",
    "            'train': Dataset.from_pandas(train_set).remove_columns('__index_level_0__'),\n",
    "            'test': Dataset.from_pandas(test_set).remove_columns('__index_level_0__'),\n",
    "            'val': Dataset.from_pandas(val_set).remove_columns('__index_level_0__'),\n",
    "            'discard': Dataset.from_pandas(discarded_set).remove_columns('__index_level_0__')\n",
    "        })\n",
    "\n",
    "\n",
    "    return chat_dataset\n",
    "\n",
    "\n",
    "data = format_data(df, 'text', 'LlamaTokens', 'label', 'balanced_split', [100,100])\n",
    "data.push_to_hub(\"FemkeBakker/AmsterdamBalancedFirst100Last100Tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt_id', 'message'],\n",
       "        num_rows: 9900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt_id', 'message'],\n",
       "        num_rows: 1100\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['prompt_id', 'message'],\n",
       "        num_rows: 1100\n",
       "    })\n",
       "    discard: Dataset({\n",
       "        features: ['prompt_id', 'message'],\n",
       "        num_rows: 8718\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493421230fb1458c8b1f6b06575c1246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bead32470c98408cad88cdb97631ab17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a122773deadf4dd6a8ae87082bbd61b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/307k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363f36680a834bc7b4333777e06a8bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/341k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902c5f0550bd4d089ef0ec9156ef72f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd76dbd8f8d48a493122e62b6991fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c520bf63e47547ac9170b01f2e1c6652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d608949197e8415dbc04e49371ce493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split:   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df914d87c2b4b9c8cae62df90f2af4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating discard split:   0%|          | 0/8718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yeet = load_dataset('FemkeBakker/AmsterdamBalancedFirst200Tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"Classificeer het document in één van de categoriën. Categoriën: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']. Categoriseer dit document: x Gemeente Amsterdam R\\nGemeenteraad\\n\\n% Gemeenteblad\\n% Motie\\n\\nJaar 2017\\n\\nAfdeling 1\\n\\nNummer 531\\n\\nPublicatiedatum 28 juli 2017\\n\\nIngekomen op 15 juni 2017\\n\\nIngekomen in raadscommissie JC\\n\\nBehandeld op 20 juli 2017\\n\\nUitslag Aangenomen\\n\\nOnderwerp\\n\\nMotie van het lid Flentge inzake de Voorjaarsnota 2017 (verkenning stagecoaches).\\n\\nAan de gemeenteraad\\n\\nOndergetekende heeft de eer voor te stellen:\\n\\nDe raad,\\n\\nGehoord de discussie over de Voorjaarsnota 2017 (Gemeenteblad afd. 1, nr. 446).\\n\\nOverwe\\nGeef de output in de vorm van een JSON file: {'categorie': categorie van het document}. \",\n",
       "  'role': 'user'},\n",
       " {'content': \"{'categorie': Motie}\", 'role': 'assistant'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['message'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
