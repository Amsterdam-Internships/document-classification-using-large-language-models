{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1712584227159
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Select where to run notebook: \"azure\" or \"local\"\n",
        "my_run = \"azure\"\n",
        "\n",
        "import my_secrets as sc\n",
        "import settings as st\n",
        "\n",
        "if my_run == \"azure\":\n",
        "    import config_azure as cf\n",
        "elif my_run == \"local\":\n",
        "    import config as cf\n",
        "\n",
        "\n",
        "import os\n",
        "if my_run == \"azure\":\n",
        "    if not os.path.exists(cf.HUGGING_CACHE):\n",
        "        os.mkdir(cf.HUGGING_CACHE)\n",
        "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
        "\n",
        "# set-up environment - GEITje-7b-chat InContextLearning:\n",
        "# - install blobfuse -> sudo apt-get install blobfuse\n",
        "# - pip install transformers\n",
        "# - pip install torch\n",
        "# - pip install accelerate\n",
        "# - pip install jupyter\n",
        "# - pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook overview\n",
        "- Goal: Run experiment for InContext Learning GEITje\n",
        "- Trial run model -> prompt GEITje using, example prompt\n",
        "- Zeroshot prompts\n",
        "- Fewshot prompts\n",
        "\n",
        "Load data and functions:\n",
        "- data is already split\n",
        "- text is already converted to tokens using model tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "import sys\n",
        "sys.path.append('../scripts/') \n",
        "import prompt_template as pt\n",
        "import prediction_helperfunctions as ph\n",
        "import truncation as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Trial run Models \n",
        "Code to run the models with a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "2024-05-01 12:48:59.201322: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='meta-llama/Llama-2-7b-chat-hf',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "## EXAMPLE PROMPT\n",
        "# print(chatbot(\n",
        "    # Conversation('Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?')\n",
        "# ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Experiment functions\n",
        "Prompt GEITje for each document and save the prediction, return response, response time and the prompt version\n",
        "\n",
        "Code structure:\n",
        "- 2 functions/cells:\n",
        "- predictions_incontextlearning -> given a df with docs that need to be predicted, prompt the model\n",
        "- run the experiment -> built in failsaves (df run in parts, with saves in between)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from bm25 import BM25\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template \n",
        "# train_df = dataframe with docs, which can be used as examples/training data/context data\n",
        "# num_examples = number of examples in the prompt\n",
        "\n",
        "def predictions_incontextlearning(chatbot, docs_df, text_column, prompt_function, train_df, num_examples):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date', 'prompt'])\n",
        "\n",
        "\n",
        "    if prompt_function == pt.fewshot_prompt_bm25:\n",
        "        BM25_model = BM25()\n",
        "        BM25_model.fit(train_df[text_column])\n",
        "    \n",
        "    # elif prompt_function == fewshot_prompt_bm25:\n",
        "    #     BM25_model = BM25()\n",
        "    #     BM25_model.fit(train_df[text_column])\n",
        "\n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        if (index + 1) % 200 == 0:\n",
        "            print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "\n",
        "        # each prompt function takes different arguments\n",
        "        # simple function is zeroshot+simple instruction\n",
        "        if prompt_function == pt.simple_prompt:\n",
        "            prompt = prompt_function(txt)\n",
        "\n",
        "        # elif prompt_function == simple_prompt:\n",
        "        #     prompt = prompt_function(txt)\n",
        "      \n",
        "        # select fewshot examples using bm25\n",
        "        elif prompt_function == pt.fewshot_prompt_bm25:\n",
        "            prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "\n",
        "        # elif prompt_function == fewshot_prompt_bm25:\n",
        "        #     prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Prompt function not recognised. Check if prompt function is in prompt_template.py and included in the options above.\")\n",
        "\n",
        "        # prompt and get the response\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "        print(\"label: \", row['label'].lower())\n",
        "        print(\"response: \", response)\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = ph.get_prediction_from_response(response)\n",
        "        print(\"prediction:\", prediction)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : docs_df.iloc[0]['trunc_col'],\n",
        "            'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': ph.get_datetime(),\n",
        "            'prompt':prompt\n",
        "        }\n",
        "    return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\"\"\"\n",
        "Function to run GEITje In-Context Learning experiment. \n",
        "The function allows to resume experiment, if run_id matches.\n",
        "\"\"\"\n",
        "# df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# run_id = unqiue for each experiment. \n",
        "# prompt_function = which prompt from prompt_template.py to use\n",
        "# text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# label_col = column with the true label\n",
        "# prediction_path = path to file where predictions need to be saved.\n",
        "# overview_path = path to file where results of each run need to be saved.\n",
        "# model_name = name of the model. string.\n",
        "# num_exmples = number of exaples given to prompt. zero in case of zeroshot. \n",
        "\n",
        "def run_experiment(chatbot, df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "    start_time = time.time()\n",
        "    test_df = df.loc[df[split_col]==subset_test]\n",
        "    train_df = df.loc[df[split_col]==subset_train]\n",
        "    \n",
        "    # get rows of df that still need to be predicted for the specific run_id\n",
        "    to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "    # devide to_predict into subsection of 50 predictions at a time. \n",
        "    # Allows to rerun without problem. And save subsections of 50 predictions.\n",
        "    step_range = list(range(0, len(to_predict), 10))\n",
        "\n",
        "    for i in range(len(step_range)):\n",
        "        try:\n",
        "            sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "            print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "        except Exception as e:\n",
        "            sub_to_predict = to_predict[step_range[i]:]\n",
        "            print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "        # prompt geitje\n",
        "        predictions = predictions_incontextlearning(chatbot, sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
        "\n",
        "        # save info\n",
        "        predictions['run_id'] = run_id\n",
        "        predictions['train_set'] = subset_train\n",
        "        predictions['test_set'] = subset_test\n",
        "        predictions['shots'] = num_examples\n",
        "\n",
        "        # save new combinations in file\n",
        "        print(\"Dont interrupt, saving predictions...\")\n",
        "        ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "        # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "        try:\n",
        "            predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "            previous_predictions = predictions\n",
        "        except Exception as e:\n",
        "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "            previous_predictions = predictions\n",
        "\n",
        "        # save results in overview file\n",
        "        date = ph.get_datetime()\n",
        "        y_test = predictions['label']\n",
        "        y_pred = predictions['prediction']\n",
        "        report = classification_report(y_test, y_pred)\n",
        "\n",
        "        overview = pd.DataFrame(\n",
        "            [{\n",
        "                'model':model_name,\n",
        "                'run_id':run_id,\n",
        "                'date': date,\n",
        "                'train_set': subset_train,\n",
        "                'test_set': subset_test,\n",
        "                'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "                'test_set_support':len(predictions),\n",
        "                'split_col':split_col,\n",
        "                'text_col':df.iloc[0]['trunc_col'],\n",
        "                'runtime':sum(predictions['runtime']),\n",
        "                'accuracy': accuracy_score(y_test, y_pred),\n",
        "                'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "                'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "                'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "                'classification_report':report\n",
        "            }   ]\n",
        "        )\n",
        "        # remove previous results of run_id, replace with new/updated results\n",
        "        ph.replace_and_save_df(overview, overview_path, run_id)\n",
        "        print(\"Saving done! Interrupting is allowed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up variables that are the same for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "TEXT_COLUMN = 'trunc_txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GEITje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - In-context learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='FemkeBakker/GEITjeSmallData200Tokens',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'GEITjeSmallData200Tokens'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictions/trialfewShotGeitjepredictions.pkl\"\n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/overview/trialfewShotGeitjepredictions.pkl\"\n",
        "\n",
        "\n",
        "PROMPT = pt.fewshot_prompt_bm25\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT == pt.simple_prompt:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "    \n",
        "OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/GEITje/{PROMPT_NAME}/overview.pkl\"\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/GEITje/{PROMPT_NAME}/predictions.pkl\"\n",
        "\n",
        "# small = txt.iloc[16:22]\n",
        "# small['4split']=['val', 'dev', 'dev', 'dev', 'dev', 'dev']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- EXPERIMENT --------\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_geitje, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "# pred_run = pred.loc[pred['run_id']==f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}']\n",
        "display(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Llama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - In Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='meta-llama/Llama-2-7b-chat-hf',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "# load llama using cpu, else will give cuda out of memory error when running fewshot bm25 prompt.\n",
        "\n",
        "MODEL_NAME = 'Llama-2-7b-chat-hf'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='FemkeBakker/LlamaSmallData200Tokens',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'LlamaSmallData200Tokens'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "PROMPT = pt.fewshot_prompt_bm25\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT == pt.simple_prompt:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "    \n",
        "OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/Llama/{PROMPT_NAME}/overview.pkl\"\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/Llama/{PROMPT_NAME}/predictions.pkl\"\n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "\n",
        "# small = txt.iloc[16:22]\n",
        "# small['4split']=['val', 'dev', 'dev', 'dev', 'dev', 'dev']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_llama, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mistral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - in context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54429ea95eb042e4aad4656c706a34fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d04873ab145a49c195321eafe0426b46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'Mistral-7B-Instruct-v0.2'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='FemkeBakker/MistralSmallData200Tokens',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'MistralSmallData200Tokens'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "PROMPT = pt.fewshot_prompt_bm25\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT == pt.simple_prompt:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "    \n",
        "OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/Mistral/{PROMPT_NAME}/overview.pkl\"\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/Mistral/{PROMPT_NAME}/predictions.pkl\"\n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "\n",
        "# small = txt.iloc[16:22]\n",
        "# small['4split']=['val', 'dev', 'dev', 'dev', 'dev', 'dev']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run-id already known, resuming predictions...\n",
            "Starting...0:25 out of 109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  raadsnotulen\n",
            "response:  {\"categorie\": \"Raadsnotulen\"}\n",
            "prediction: raadsnotulen\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  raadsadres\n",
            "response:  Based on the provided context, the document \"Van: Natuurbescherming ZO [mailto:natuurbeschermingzo@xs4all.nl]\" with the subject \"...\" should be categorized as \"Actualiteit\" or \"Motie\" depending on the content of the document. Without access to the document's content, it is impossible to determine the exact category. Here's the JSON format for the potential categories:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Actualiteit\"}\n",
            "```\n",
            "or\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Motie\"}\n",
            "```\n",
            "\n",
            "Please provide the document content for accurate categorization.\n",
            "prediction: MultiplePredictionErrorInFormatting\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  besluit\n",
            "response:  Based on the provided context and examples, the document you have given should be categorized as follows:\n",
            "\n",
            "Input document:\n",
            "```vbnet\n",
            "Gemeente Bezoekadres\n",
            "Plein '40 '45 1\n",
            "Amsterdam 1064 Sw Amsterdam\n",
            "Nieuw-West Postbus 2003\n",
            "1000 CA Amsterdam\n",
            "Telefoon 14020\n",
            "> < Nieuwwest.amsterdam.nl\n",
            "Vergadering Bestuurscommissie\n",
            "Datum 7 december 2016\n",
            "Decos nummer 2016\\int\\1449\\1\n",
            "Onderwerp OLO2024613 ter hoogte van Maroastraat 39, Meet Inn\n",
            "Het algemeen bestuur van de bestuurscommissie van stadsdeel Nieuw-West\n",
            "Gezien de voordracht van het dagelijks bestuur van 22 november 2016\n",
            "Besluit\n",
            "\n",
            "1.\n",
            "```\n",
            "\n",
            "Output in JSON format:\n",
            "```json\n",
            "{\"categorie\": \"Bes\n",
            "prediction: NoPredictionFormat\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  Based on the provided information, the document to be categorised is:\n",
            "\n",
            "x Gemeente Amsterdam R\n",
            "Gemeenteblad\n",
            "% Motie\n",
            "Jaar 2018\n",
            "Afdeling 1\n",
            "Nummer 160\n",
            "Publicatiedatum 21 februari 2018\n",
            "Ingekomen onder\n",
            "Ingekomen op woensdag 14 februari 2018\n",
            "Behandeld op woensdag 14 februari 2018\n",
            "Status Verworpen\n",
            "Onderwerp\n",
            "Motie van het lid Van Lammeren inzake het geluidbeleid Evenementen en\n",
            "de locatieprofielen (gevelnorm 70 dB(A)).\n",
            "Aan de gemeentraad\n",
            "Ondergetekende heeft de eer voor te stellen:\n",
            "De raad,\n",
            "Gehoord de discussie over de beleidsregel “Geluid bij evenementen in Amsterdam”\n",
            "\n",
            "The document is a 'Motie'.\n",
            "\n",
            "Output in JSON format: {'categorie': 'Motie'}\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  onderzoeksrapport\n",
            "response:  Based on the provided list of categories and the document text, the document can be categorized as an \"Onderzoeksrapport\" (Research report). Here is the JSON output:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Onderzoeksrapport\"}\n",
            "```\n",
            "prediction: onderzoeksrapport\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  Based on the provided document, the output in JSON format would be:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Amendement\"}\n",
            "```\n",
            "\n",
            "Therefore, the complete output for the document would be:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Amendement\"}\n",
            "```\n",
            "prediction: MultiplePredictionErrorInFormatting\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  raadsnotulen\n",
            "response:  {\"categorie\": \"Raadsnotulen\"}\n",
            "prediction: raadsnotulen\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  onderzoeksrapport\n",
            "response:  Based on the provided context and example documents, the document \"WPI Kwartaal Factsheet December 2022\" should be categorized as follows:\n",
            "\n",
            "Output: {'categorie': 'Factsheet'}\n",
            "prediction: factsheet\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  {\"categorie\": \"Motie\"}\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  brief\n",
            "response:  Based on the provided context and the document you have given, the output in the form of a JSON file would be:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Raadsnotulen\"}\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "The document you provided is a Gemeente Raadsinformatiebrief, which is a type of document commonly used in Dutch municipalities to inform the city council about various matters. However, the specific category of this document can be determined based on its content. In this case, the document contains information about an evaluation of a program or project, which is typically the type of content found in Raadsnotulen (Council minutes or reports). Therefore, the appropriate category for this document would be \"Raadsnotulen\".\n",
            "prediction: raadsnotulen\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  {\"categorie\": \"Motie\"}\n",
            "\n",
            "For the given document, the category would be \"Motie\" since it is a motion presented to the city council.\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  besluit\n",
            "response:  Based on the provided information, the document can be categorized as 'Besluit'. Here is the JSON output:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Besluit\"}\n",
            "```\n",
            "prediction: besluit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  agenda\n",
            "response:  Based on the provided document, the output in JSON format would be:\n",
            "\n",
            "{'categorie': 'Agenda'}\n",
            "prediction: agenda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  schriftelijke vraag\n",
            "response:  {\"categorie\": \"Schriftelijke Vraag\"}\n",
            "prediction: schriftelijke vraag\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  besluit\n",
            "response:  Based on the provided context, the document should be categorized as \"Besluit\". Here's the JSON output:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Besluit\"}\n",
            "```\n",
            "prediction: besluit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  voordracht\n",
            "response:  Based on the provided context and the given document, the document can be categorized as 'Agenda'. Therefore, the output in JSON format would be: {'categorie': 'Agenda'}\n",
            "prediction: agenda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  Based on the provided information, the document with the following details should be categorized as a \"Motie\":\n",
            "\n",
            "{'categorie': 'Motie'}\n",
            "\n",
            "Gemeente Amsterdam R\n",
            "Gemeenteraad\n",
            "% Gemeenteblad\n",
            "% Motie\n",
            "Jaar 2017\n",
            "Afdeling 1\n",
            "Nummer 1641\n",
            "Publicatiedatum 29 december 2017\n",
            "Ingekomen onder J\n",
            "Ingekomen op woensdag 20 december 2017\n",
            "Behandeld op woensdag 20 december 2017\n",
            "Status Verworpen\n",
            "Onderwerp\n",
            "Motie van de leden Groen en Van den Berg inzake subsidieregeling aardgasloos II.\n",
            "Aan de gemeenteraad\n",
            "Ondergetekenden hebben de eer voor te stellen:\n",
            "De raad,\n",
            "Gehoord de discussie over de voordracht van het college van burgemeester en wethouders inzake kennisnemen van het besluit...\n",
            "\n",
            "This document is a 'Motie' because it is\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  {\"categorie\": \"Motie\"}\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  raadsadres\n",
            "response:  {\"categorie\": \"Brief\"}\n",
            "prediction: brief\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  {\"categorie\": \"Motie\"}\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  {\"categorie\": \"Motie\"}\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  voordracht\n",
            "response:  Based on the provided context, the document \"VN2023-022301 X Gemeente Raadscommissie voor Publieke Gezondheid en Preventie, Zorg en OZA, Werk, Participatie Maatschappelijke Ontwikkeling, Jeugd(zorg), Onderwijs en Armoede en en Inkomen % Amsterdam | Schuldhulpverlening, Voordracht voor de Commissie OZA van o1 november 2023, Ter kennisneming, Portefeuille Armoedebestrijding en Schuldhulpverlening, Agendapunt 2, Datum besluit 10 oktober 2023, College B&W, Onderwerp: Kennisnemen van de raadsinformatiebrief 'Energietoeslag 2023! De commissie'\" can be categorized as follows:\n",
            "\n",
            "Output: {'categorie': 'Voordracht'}\n",
            "prediction: voordracht\n"
          ]
        }
      ],
      "source": [
        "# run experiment\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_mistral, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>run_id</th>\n",
              "      <th>date</th>\n",
              "      <th>train_set</th>\n",
              "      <th>test_set</th>\n",
              "      <th>train_set_support</th>\n",
              "      <th>test_set_support</th>\n",
              "      <th>split_col</th>\n",
              "      <th>text_col</th>\n",
              "      <th>runtime</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_avg_precision</th>\n",
              "      <th>macro_avg_recall</th>\n",
              "      <th>macro_avg_f1</th>\n",
              "      <th>classification_report</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mistral-7B-Instruct-v0.2</td>\n",
              "      <td>IC_Mistral-7B-Instruct-v0.2fewshot_prompt_bm25...</td>\n",
              "      <td>2024-05-01 21:06:56.220884+02:00</td>\n",
              "      <td>dev</td>\n",
              "      <td>val</td>\n",
              "      <td>832</td>\n",
              "      <td>75</td>\n",
              "      <td>4split</td>\n",
              "      <td>TruncationLlamaTokensFront200Back0</td>\n",
              "      <td>9123.21593</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.30768</td>\n",
              "      <td>0.325973</td>\n",
              "      <td>precision...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      model  \\\n",
              "0  Mistral-7B-Instruct-v0.2   \n",
              "\n",
              "                                              run_id  \\\n",
              "0  IC_Mistral-7B-Instruct-v0.2fewshot_prompt_bm25...   \n",
              "\n",
              "                              date train_set test_set  train_set_support  \\\n",
              "0 2024-05-01 21:06:56.220884+02:00       dev      val                832   \n",
              "\n",
              "   test_set_support split_col                            text_col     runtime  \\\n",
              "0                75    4split  TruncationLlamaTokensFront200Back0  9123.21593   \n",
              "\n",
              "   accuracy  macro_avg_precision  macro_avg_recall  macro_avg_f1  \\\n",
              "0      0.64                 0.37           0.30768      0.325973   \n",
              "\n",
              "                               classification_report  \n",
              "0                                       precision...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "display(pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gibberish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fewshot Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictions/fewShotGeitjepredictions.pkl\"\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/overview/fewShotGeitjepredictions.pkl\"\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictions/trialfewShotGeitjepredictions.pkl\"\n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/overview/trialfewShotGeitjepredictions.pkl\"\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "TEXT_COLUMN = 'trunc_txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictions/fewShotLlamapredictions.pkl\"\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/overview/fewShotLlamapredictions.pkl\"\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictions/trialfewShotLlamapredictions.pkl\"\n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/overview/trialfewShotLlamapredictions.pkl\"\n",
        "MODEL_NAME = 'Llama-2-7b-chat-hf'\n",
        "TEXT_COLUMN = 'trunc_txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- EXPERIMENT: ?? --------\n",
        "\n",
        "# run experiment\n",
        "PROMPT = pt.fewshot_prompt_bm25\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens'\n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "NUMBER_EXAMPLES = 2\n",
        "# small = txt.iloc[0:5]\n",
        "# small['4split']='val'\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(trunc_df, f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}', PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "# pred_run = pred.loc[pred['run_id']==f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}']\n",
        "# print(sum(pred_run['runtime']))\n",
        "# pred['runtime'] = sum(pred_run['runtime'])\n",
        "display(pred.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(PREDICTION_PATH)\n",
        "pred_run = pred.loc[pred['run_id']==f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}']\n",
        "print(sum(pred_run['runtime']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## End notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_class_list():\n",
        "#     return ['Voordracht', 'Besluit', 'Schriftelijke Vragen', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheets']\n",
        "\n",
        "# def fewshot_prompt_examples(doc, train_df, num_examples, text_column):\n",
        "#     examples = train_df.sample(n=num_examples)\n",
        "\n",
        "#     prompt = f\"\"\"\n",
        "#     Het is jouw taak om een document te categoriseren in één van de categoriën.\n",
        "#     Eerst krijg je een lijst met mogelijke categoriën, daarna {num_examples} voorbeelden van documenten en tot slot het document dat gecategoriseerd moet worden. \n",
        "    \n",
        "#     Categoriën: {get_class_list()}\n",
        "#     \"\"\"\n",
        "\n",
        "#     for index, row in examples.iterrows():\n",
        "#         mini_prompt = f\"\"\"\n",
        "#     Dit is een voorbeeld document de categorie {row['label']}:\n",
        "#         {row[text_column]}\n",
        "#         \"\"\"\n",
        "\n",
        "#         prompt += mini_prompt\n",
        "\n",
        "#     doc_prompt = f\"\"\"\n",
        "#     Categoriseer dit document:\n",
        "#         {doc}\n",
        "#     \"\"\"\n",
        "\n",
        "#     prompt += doc_prompt\n",
        "#     return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def simple_prompt(doc,train_df, num_examples, text_column):\n",
        "#     prompt = f\"\"\"\n",
        "#     Classificeer het document in één van de categoriën.\n",
        "#     Houd het kort, geef enkel de naam van de categorie als response.\n",
        "    \n",
        "#     Categoriën: {get_class_list()}\n",
        "    \n",
        "#     Document: \n",
        "#     {doc}\n",
        "    \n",
        "#     \"\"\"\n",
        "#     return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import time\n",
        "# import os\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# \"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# # docs_df = dataframe with the documents that need to be predicted\n",
        "# # text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# # prompt_function = prompt template \n",
        "\n",
        "# def predictions_incontextlearning(docs_df, text_column, prompt_function, train_df, num_examples):\n",
        "#     results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date', 'prompt'])\n",
        "    \n",
        "#     # prompt each document\n",
        "#     for index, row in docs_df.iterrows():\n",
        "#         if (index + 1) % 200 == 0:\n",
        "#             print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         # get the prompt, with the doc filled in\n",
        "#         txt = row[text_column]\n",
        "\n",
        "#         # always give these as input, however not every template uses all of them\n",
        "#         prompt = prompt_function(txt, train_df, num_examples, text_column)\n",
        "\n",
        "#         # prompt and get the response\n",
        "#         converse = chatbot(Conversation(prompt))\n",
        "#         response = converse[1]['content']\n",
        "\n",
        "#         # extract prediction from response\n",
        "#         prediction = ph.get_prediction_from_response(response)\n",
        "\n",
        "#         # save results in dataframe\n",
        "#         results_df.loc[len(results_df)] = {\n",
        "#             'id': row['id'],\n",
        "#             'path' : row['path'],\n",
        "#             'text_column' : text_column,\n",
        "#             'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "#             'response':response,\n",
        "#             'prediction':prediction,\n",
        "#             'label':row['label'].lower(),\n",
        "#             'runtime':time.time()-start_time,\n",
        "#             'date': ph.get_datetime(),\n",
        "#             'prompt':prompt\n",
        "#         }\n",
        "#     return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import time\n",
        "# from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# \"\"\"\n",
        "# Function to run GEITje In-Context Learning experiment. \n",
        "# The function allows to resume experiment, if run_id matches.\n",
        "# \"\"\"\n",
        "# # df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# # run_id = unqiue for each experiment. \n",
        "# # prompt_function = which prompt from prompt_template.py to use\n",
        "# # text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# # split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# # subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# # subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# # label_col = column with the true label\n",
        "# # prediction_path = path to file where predictions need to be saved.\n",
        "# # overview_path = path to file where results of each run need to be saved.\n",
        "# # model_name = name of the model. string.\n",
        "# # num_exmples = number of exaples given to prompt. zero in case of zeroshot. \n",
        "\n",
        "# def run_experiment(df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "#     print(num_examples)\n",
        "#     start_time = time.time()\n",
        "#     test_df = df.loc[df[split_col]==subset_test]\n",
        "#     train_df = df.loc[df[split_col]==subset_train]\n",
        "    \n",
        "#     # get rows of df that still need to be predicted for the specific run_id\n",
        "#     to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "#     # devide to_predict into subsection of 50 predictions at a time. \n",
        "#     # Allows to rerun without problem. \n",
        "#     step_range = list(range(0, len(to_predict), 3))\n",
        "\n",
        "#     for i in range(len(step_range)):\n",
        "#         try:\n",
        "#             sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "#             print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "#         except Exception as e:\n",
        "#             sub_to_predict = to_predict[step_range[i]:]\n",
        "#             print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "#         # prompt geitje\n",
        "#         predictions = predictions_incontextlearning(sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
        "\n",
        "#         # save info\n",
        "#         predictions['run_id'] = run_id\n",
        "#         predictions['train_set'] = subset_train\n",
        "#         predictions['test_set'] = subset_test\n",
        "#         predictions['shots'] = num_examples\n",
        "\n",
        "#         # save new combinations in file\n",
        "#         ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "#         # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "#         try:\n",
        "#             predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "#         except Exception as e:\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "\n",
        "#         # save results in overview file\n",
        "#         date = ph.get_datetime()\n",
        "#         y_test = predictions['label']\n",
        "#         y_pred = predictions['prediction']\n",
        "#         report = classification_report(y_test, y_pred)\n",
        "\n",
        "#         overview = pd.DataFrame(\n",
        "#             [{\n",
        "#                 'model':model_name,\n",
        "#                 'run_id':run_id,\n",
        "#                 'date': date,\n",
        "#                 'train_set': subset_train,\n",
        "#                 'test_set': subset_test,\n",
        "#                 'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "#                 'test_set_support':len(predictions),\n",
        "#                 'split_col':split_col,\n",
        "#                 'text_col':text_col,\n",
        "#                 'runtime':time.time()-start_time,\n",
        "#                 'accuracy': accuracy_score(y_test, y_pred),\n",
        "#                 'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "#                 'classification_report':report\n",
        "#             }   ]\n",
        "#         )\n",
        "#         # remove previous results of run_id, replace with new/updated results\n",
        "#         ph.replace_and_save_df(overview, overview_path, run_id)\n",
        "\n",
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictions/tryoutGeitjepredictions.pkl\"\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/overview/tryoutGeitjepredictions.pkl\"\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "TEXT_COLUMN = 'trunc_txt'\n",
        "\n",
        "p_path = f\"{cf.output_path}/predictions/tryoutGeitjepredictions.pkl\"\n",
        "o_path = f\"{cf.output_path}/overview/tryoutGeitjeoverview.pkl\"\n",
        "run_experiment(small, 'tryout_zeroshot', pt.simple_prompt, 'trunc_txt', '4split', 'dev', 'val', 'label', p_path, o_path, 'GEITje-7B-chat-v2', 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yeet = pd.read_pickle(p_path)\n",
        "yeet  = yeet.loc[yeet['run_id']=='tryout_zeroshot']\n",
        "display(yeet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(yeet.iloc[0]['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "small = txt.iloc[0:5]\n",
        "small['4split']=['val', 'dev', 'val', 'dev', 'dev']\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(small,'text', 'LlamaTokens', 200, 200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GIbberish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import time\n",
        "# import os\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# \"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# # docs_df = dataframe with the documents that need to be predicted\n",
        "# # text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# # prompt_function = prompt template -> ONLY prompt templates that take doc as input (ZERO SHOT)\n",
        "\n",
        "# def zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function):\n",
        "#     results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date'])\n",
        "    \n",
        "#     # prompt each document\n",
        "#     for index, row in docs_df.iterrows():\n",
        "#         if (index + 1) % 200 == 0:\n",
        "#             print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         # get the prompt, with the doc filled in\n",
        "#         txt = row[text_column]\n",
        "#         prompt = prompt_function(txt)\n",
        "\n",
        "#         # prompt and get the response\n",
        "#         converse = chatbot(Conversation(prompt))\n",
        "#         response = converse[1]['content']\n",
        "\n",
        "#         # extract prediction from response\n",
        "#         prediction = ph.get_prediction_from_response(response)\n",
        "\n",
        "#         # save results in dataframe\n",
        "#         results_df.loc[len(results_df)] = {\n",
        "#             'id': row['id'],\n",
        "#             'path' : row['path'],\n",
        "#             'text_column' : text_column,\n",
        "#             'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "#             'response':response,\n",
        "#             'prediction':prediction,\n",
        "#             'label':row['label'].lower(),\n",
        "#             'runtime':time.time()-start_time,\n",
        "#             'date': ph.get_datetime()\n",
        "#         }\n",
        "#     return results_df\n",
        "\n",
        "\n",
        "#  import os\n",
        "# import time\n",
        "# from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# \"\"\"\n",
        "# Function to run GEITje ZEROSHOT experiment. \n",
        "# The function allows to resume experiment, if run_id matches.\n",
        "# \"\"\"\n",
        "# # df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# # run_id = unqiue for each experiment. \n",
        "# # prompt_function = which prompt from prompt_template.py to use\n",
        "# # text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# # split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# # subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# # subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# # label_col = column with the true label\n",
        "# # prediction_path = path to file where predictions need to be saved.\n",
        "# # overview_path = path to file where results of each run need to be saved.\n",
        "# # model_name = name of the model. string.\n",
        "# # num_exmples = number of exaples given to prompt. zero inn case of zeroshot. \n",
        "\n",
        "# def run_experiment(df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "#     start_time = time.time()\n",
        "#     test_df = df.loc[df[split_col]==subset_test]\n",
        "    \n",
        "#     # get rows of df that still need to be predicted for the specific run_id\n",
        "#     to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "#     # devide to_predict into subsection of 50 predictions at a time. \n",
        "#     # Allows to rerun without problem. \n",
        "#     step_range = list(range(0, len(to_predict), 3))\n",
        "\n",
        "#     for i in range(len(step_range)):\n",
        "#         try:\n",
        "#             sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "#             print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "#         except Exception as e:\n",
        "#             sub_to_predict = to_predict[step_range[i]:]\n",
        "#             print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "#         # prompt geitje\n",
        "#         predictions = zero_shot_predictions_incontextlearning(sub_to_predict, text_col, prompt_function)\n",
        "\n",
        "#         # save info\n",
        "#         predictions['run_id'] = run_id\n",
        "#         predictions['train_set'] = subset_train\n",
        "#         predictions['test_set'] = subset_test\n",
        "#         predictions['shots'] = num_examples\n",
        "\n",
        "#         # save new combinations in file\n",
        "#         ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "#         # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "#         try:\n",
        "#             predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "#         except Exception as e:\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "\n",
        "#         # save results in overview file\n",
        "#         date = ph.get_datetime()\n",
        "#         y_test = predictions['label']\n",
        "#         y_pred = predictions['prediction']\n",
        "#         report = classification_report(y_test, y_pred)\n",
        "\n",
        "#         overview = pd.DataFrame(\n",
        "#             [{\n",
        "#                 'model':model_name,\n",
        "#                 'run_id':run_id,\n",
        "#                 'date': date,\n",
        "#                 'train_set': subset_train,\n",
        "#                 'test_set': subset_test,\n",
        "#                 'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "#                 'test_set_support':len(predictions),\n",
        "#                 'split_col':split_col,\n",
        "#                 'text_col':text_col,\n",
        "#                 'runtime':time.time()-start_time,\n",
        "#                 'accuracy': accuracy_score(y_test, y_pred),\n",
        "#                 'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "#                 'classification_report':report\n",
        "#             }   ]\n",
        "#         )\n",
        "#         # remove previous results of run_id, replace with new/updated results\n",
        "#         ph.replace_and_save_df(overview, overview_path, run_id)\n",
        " \n",
        "# # p_path = f\"{cf.output_path}/predictions/tryoutGeitjepredictions.pkl\"\n",
        "# # o_path = f\"{cf.output_path}/overview/tryoutGeitjeoverview.pkl\"\n",
        "# # run_experiment(txt.iloc[25:30], 'tryout', pt.simple_prompt, trunc_col, '4split', 'dev', 'val', 'label', p_path, o_path, 'GEITje-7B-chat-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n",
        "df = df.loc[df['set']=='val']\n",
        "df['text_trunc_100'] = df['tokens'].apply(text_truncation,100)\n",
        "df['text_trunc_1000'] = df['tokens'].apply(text_truncation,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\"\n",
        "resume_predictions(df, path, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dummy code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_in_subsections(df, path, set_run_id):\n",
        "\n",
        "    iterations = list(range(0, len(df)+50, 50))\n",
        "    for i in range(len(iterations)):\n",
        "        try:\n",
        "            subdf = df.iloc[iterations[i]:iterations[i+1]]\n",
        "\n",
        "        except IndexError:\n",
        "            subdf = df.iloc[iterations[i]:]\n",
        "\n",
        "        # if set_run_id == 'new' and iterations[i]==0:\n",
        "        #     run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'new', path, 'val')\n",
        "        # else:\n",
        "        #     run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'previous', path, 'val')\n",
        "\n",
        "\n",
        "\n",
        "path = f\"{cf.output_path}/predictions/ICgeitje_predictions_tryout.pkl\"\n",
        "run_in_subsections(df, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_in_subsections(df, path):\n",
        "    subdf = df.iloc[0:50]\n",
        "    run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'new', path, 'val')\n",
        "\n",
        "    iterations = list(range(50, len(df)+50, 50))\n",
        "    for i in range(len(iterations)):\n",
        "        if i < len(iterations)-2:\n",
        "            subdf = df.iloc[iterations[i]:iterations[i+1]]\n",
        "            print(\"\\n\", \"iterations\", iterations[i], iterations[i+1], \"\\n\")\n",
        "            run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'previous', path, 'val')\n",
        "\n",
        "        elif i < len(iterations)-1:\n",
        "            subdf = df.iloc[iterations[i]:]\n",
        "            print(\"\\n\", \"iterations\", iterations[i], '\\n')\n",
        "            run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'previous', path, 'val')\n",
        "\n",
        "path = f\"{cf.output_path}/predictions/ICgeitje_predictions_tryout.pkl\"\n",
        "run_in_subsections(df, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yeet = pd.read_pickle(f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\")\n",
        "display(yeet)\n",
        "\n",
        "yeet = pd.read_pickle(f\"{cf.output_path}/overview_results.pkl\")\n",
        "display(yeet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import sys\n",
        "sys.path.append('../scripts/') \n",
        "import prompt_template as pt\n",
        "import prediction_helperfunctions as ph\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template -> ONLY prompt templates that take doc as input (ZERO SHOT)\n",
        "\n",
        "def zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date'])\n",
        "    \n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        if (index + 1) % 200 == 0:\n",
        "            print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "        prompt = prompt_function(txt)\n",
        "\n",
        "        # prompt and get the response\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = ph.get_prediction_from_response(response)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : text_column,\n",
        "            'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': ph.get_datetime()\n",
        "        }\n",
        "    return results_df\n",
        "\n",
        "# \"\"\" Run a prediction function -> can be ZeroShot or FewShot \"\"\"\n",
        "# def run_prediction(docs_df, text_column, prompt_function, subset=None, learning='ZeroShot'):\n",
        "#     if learning == 'ZeroShot':\n",
        "#         # get the predictions\n",
        "#         res = zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function)\n",
        "\n",
        "#         # INSERT ELSE STATEMENT HERE FOR FEWSHOT\n",
        "\n",
        "#         # get run_id\n",
        "#         path = f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\"\n",
        "#         res['run_id'], predictions_df = ph.get_runid(path)\n",
        "\n",
        "#         # combine earlier predictions with new ones\n",
        "#         all_predictions = pd.concat([predictions_df, res])\n",
        "\n",
        "#         # save predictions\n",
        "#         all_predictions.to_pickle(path)\n",
        "\n",
        "#         # save the evaluation metrics for each run\n",
        "#         ph.update_overview_results(res, 'Rijgersberg/GEITje-7B-chat-v2')\n",
        "#         return res\n",
        "# gestart om 10.15/\n",
        "# res = run_prediction(df, 'text_trunc_100', pt.simple_prompt, 'val')\n",
        "# display(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yeet = pd.read_pickle(f\"{cf.output_path}/overview_results.pkl\")\n",
        "display(yeet)\n",
        "\n",
        "yeet = pd.read_pickle(f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\")\n",
        "display(yeet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tryout GEITje\n",
        "Load chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='auto')\n",
        "\n",
        "## simple query\n",
        "print(chatbot(\n",
        "    Conversation(\"Hallo, ik ben Bram. Ik wil vanavond graag een film kijken. Heb je enkele suggesties?\")\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "# load_in_8bit: lower precision but saves a lot of GPU memory\n",
        "# device_map=auto: loads the model across multiple GPUs\n",
        "# chatbot = pipeline(\"conversational\", model=\"BramVanroy/GEITje-7B-ultra\",  model_kwargs={\"load_in_8bit\": True}, device_map=\"auto\")\n",
        "chatbot = pipeline(\"conversational\", model=\"BramVanroy/GEITje-7B-ultra\",  device_map=\"auto\")\n",
        "\n",
        "# start_messages = [\n",
        "#     # {\"role\": \"system\", \"content\": \"Je bent een grappige chatbot die Bert heet. Je maakt vaak mopjes.\"},\n",
        "#     {\"role\": \"user\", \"content\": \"Hallo, ik ben Bram. Ik wil vanavond graag een film kijken. Heb je enkele suggesties?\"}\n",
        "# ]\n",
        "# conversation = Conversation(start_messages)\n",
        "# conversation = chatbot(conversation)\n",
        "# response = conversation.messages[-1][\"content\"]\n",
        "# print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = df.iloc[0]['text']\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Classificeer de gegeven tekst in 1 van de categoriën.\n",
        "Geef als reactie enkel de naam van de categorie\n",
        "Categorieën: ['Voordracht', 'Besluit', 'Schriftelijke Vragen', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheets']\n",
        "Tekst: \n",
        "\n",
        "{txt}\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "start_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Jouw enige taak is om teksten te classificeren. Je geeft geen uitleg voor je keuzes.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chatbot(Conversation(start_messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = df.loc[df['clean_tokens_count'].idxmax()]['text']\n",
        "print(df.loc[df['clean_tokens_count'].idxmax()]['clean_tokens_count'])\n",
        "\n",
        "print(pt.simple_prompt(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(chatbot(\n",
        "    Conversation(pt.simple_prompt(text))\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16,\n",
        "                                             low_cpu_mem_usage=True, attn_implementation='eager',\n",
        "                                             device_map=device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate(conversation, temperature=0.2, top_k=50, max_new_tokens=1_000):\n",
        "    tokenized = tokenizer.apply_chat_template(conversation, add_generation_prompt=True,\n",
        "                                              return_tensors='pt').to(device)\n",
        "    outputs = model.generate(tokenized, do_sample=True, temperature=temperature,\n",
        "                             top_k=top_k, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?'\n",
        "    }\n",
        "]\n",
        "print(generate(conversation))\n",
        "# <|user|>\n",
        "# Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"? \n",
        "# <|assistant|>\n",
        "# Het woord dat niet op zijn plaats staat is 'geit'. Een geit zou niet tussen een lijst van vervoersmiddelen moeten staan. Het past beter bij een boerderijthema of dierenlijst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BACK-UP CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import pytz\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\"\"\" Given the string response, extract the prediction \"\"\"\n",
        "def get_prediction_from_response(response):\n",
        "    # get a list of the possible classes\n",
        "    classes_list = pt.get_class_list()\n",
        "\n",
        "    predictions = [True if category.lower() in response.lower() else False for category in classes_list]\n",
        "\n",
        "    # check if multiple classes were named, this is a prediction error\n",
        "    if Counter(predictions)[True] > 1:\n",
        "        return \"PredictionError\"\n",
        "\n",
        "    # check if exactly one class is named, this is the prediction\n",
        "    elif Counter(predictions)[True] == 1:\n",
        "        prediction = [category.lower() for category in classes_list if category.lower() in response.lower()]\n",
        "        return prediction[0]\n",
        "\n",
        "    # if no class is named, then this is a no prediction error\n",
        "    else:\n",
        "        return 'NoPrediction'\n",
        "\n",
        "\"\"\" Extract the promptfunction name \"\"\"\n",
        "def get_promptfunction_name(prompt_function):\n",
        "    string = f\"{prompt_function}\"\n",
        "    match = re.search(r'<function\\s+(\\w+)', string)\n",
        "    if match:\n",
        "        function_name = match.group(1)\n",
        "        return function_name\n",
        "    else:\n",
        "        return f\"{prompt_function}\"\n",
        "    \n",
        "\"\"\" Get the current time in the Netherlands \"\"\"\n",
        "def get_datetime():\n",
        "    current_datetime_utc = datetime.datetime.now(pytz.utc)\n",
        "\n",
        "    # Convert UTC time to Dutch time (CET)\n",
        "    dutch_timezone = pytz.timezone('Europe/Amsterdam')\n",
        "    current_datetime_dutch = current_datetime_utc.astimezone(dutch_timezone)\n",
        "    return current_datetime_dutch\n",
        "        \n",
        "\"\"\" Get the new runid \"\"\"\n",
        "def get_runid(path):\n",
        "\n",
        "    # if not first run, set runid to most recent run+1\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_pickle(path)\n",
        "        return max(df['run_id'])+1, df\n",
        "\n",
        "    # if first run, set runid to 0\n",
        "    else:\n",
        "        return 0, pd.DataFrame()\n",
        "    \n",
        "\"\"\" Save evaluation metrics of a run \"\"\"\n",
        "def update_overview_results(df, model_name, subset=None):\n",
        "    # df= dataframe with predictions for each do, one row per doc/prediction\n",
        "    # model_name = string with the name of the model\n",
        "    # subset = can be train, val, or test, or left open\n",
        " \n",
        "    # get evalaution scores\n",
        "    evaluation_dict = classification_report(df['label'], df['prediction'], output_dict=True)\n",
        "    evaluation = pd.DataFrame(evaluation_dict).transpose()\n",
        "    \n",
        "    new_row = {\n",
        "        # stuff about the run\n",
        "        'run_id':df.iloc[0]['run_id'],\n",
        "        'model':model_name,\n",
        "        'prompt_function':df.iloc[0]['prompt_function'],\n",
        "        'text_column':df.iloc[0]['text_column'],\n",
        "        'date': get_datetime(),\n",
        "        'runtime':sum(df['runtime']),\n",
        "        'set':subset,\n",
        "        'support':evaluation.iloc[-1]['support'],\n",
        "\n",
        "        # evaluation\n",
        "        'accuracy': evaluation_dict['accuracy'],\n",
        "\n",
        "        'recall_weighted_avg':evaluation.loc[evaluation.index=='weighted avg']['recall'].values[0],\n",
        "        'precision_weighted_avg': evaluation.loc[evaluation.index=='weighted avg']['precision'].values[0],\n",
        "        'f1_weighted_avg': evaluation.loc[evaluation.index=='weighted avg']['f1-score'].values[0],\n",
        "\n",
        "        'recall_macro_avg':evaluation.loc[evaluation.index=='macro avg']['recall'].values[0],\n",
        "        'precision_macro_avg': evaluation.loc[evaluation.index=='macro avg']['precision'].values[0],\n",
        "        'f1_macro_avg': evaluation.loc[evaluation.index=='macro avg']['f1-score'].values[0],\n",
        "\n",
        "\n",
        "        'recall_classes': dict(zip(evaluation.index[0:-3], evaluation['recall'][0:-3])),\n",
        "        'precision_classes': dict(zip(evaluation.index[0:-3], evaluation['precision'][0:-3])),\n",
        "        'f1_classes': dict(zip(evaluation.index[0:-3], evaluation['f1-score'][0:-3])),\n",
        "        'support_classes': dict(zip(evaluation.index[0:-3], evaluation['support'][0:-3])),\n",
        "\n",
        "        # docs that were predicted\n",
        "        'doc_paths':list(df['path'].values)\n",
        "        \n",
        "    }\n",
        "\n",
        "    # create a new dataframe with the evaluation, each run has one row\n",
        "    results = pd.DataFrame(columns=new_row.keys())\n",
        "    results.loc[len(results)] = new_row\n",
        "   \n",
        "    # if not the first run, get results from previous runs\n",
        "    path = f\"{cf.output_path}/overview_results.pkl\"\n",
        "    if os.path.exists(path):\n",
        "        earlier_results = pd.read_pickle(path)\n",
        "\n",
        "        # combine evaluation of previous runs with current run\n",
        "        results = pd.concat([earlier_results, results])\n",
        "\n",
        "    # save to overview_results.pkl\n",
        "    results.to_pickle(path)\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# update_overview_results(res, 'geitje')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# yeet = pd.read_pickle(f\"{cf.output_path}/overview_results.pkl\")\n",
        "# display(yeet)\n",
        "\n",
        "# yeet = pd.read_pickle(f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\")\n",
        "# display(yeet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template -> ONLY prompt templates that take doc as input (ZERO SHOT)\n",
        "\n",
        "def zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date'])\n",
        "    \n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "        prompt = prompt_function(txt)\n",
        "\n",
        "        # prompt and get the response\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = get_prediction_from_response(response)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : text_column,\n",
        "            'prompt_function': get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': get_datetime()\n",
        "        }\n",
        "    return results_df\n",
        "\n",
        "\"\"\" Run a prediction function -> can be ZeroShot or FewShot \"\"\"\n",
        "def run_prediction(docs_df, text_column, prompt_function, subset=None, learning='ZeroShot'):\n",
        "    if learning == 'ZeroShot':\n",
        "        # get the predictions\n",
        "        res = zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function)\n",
        "\n",
        "        # INSERT ELSE STATEMENT HERE FOR FEWSHOT\n",
        "\n",
        "        # get run_id\n",
        "        path = f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\"\n",
        "        res['run_id'], predictions_df = get_runid(path)\n",
        "\n",
        "        # combine earlier predictions with new ones\n",
        "        all_predictions = pd.concat([predictions_df, res])\n",
        "\n",
        "        # save predictions\n",
        "        all_predictions.to_pickle(path)\n",
        "\n",
        "        # save the evaluation metrics for each run\n",
        "        update_overview_results(res, 'Rijgersberg/GEITje-7B-chat-v2')\n",
        "        return res\n",
        "\n",
        "res = run_prediction(df, 'text_trunc', pt.simple_prompt)\n",
        "display(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "amsterdamincontextlearning"
    },
    "kernelspec": {
      "display_name": "AmsterdamInContextLearning",
      "language": "python",
      "name": "amsterdamincontextlearning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "nl"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
