{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1712584227159
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Select where to run notebook: \"azure\" or \"local\"\n",
        "my_run = \"azure\"\n",
        "\n",
        "# import my_secrets as sc\n",
        "# import settings as st\n",
        "\n",
        "if my_run == \"azure\":\n",
        "    import config_azure as cf\n",
        "elif my_run == \"local\":\n",
        "    import config as cf\n",
        "\n",
        "\n",
        "import os\n",
        "if my_run == \"azure\":\n",
        "    if not os.path.exists(cf.HUGGING_CACHE):\n",
        "        os.mkdir(cf.HUGGING_CACHE)\n",
        "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
        "\n",
        "# set-up environment - GEITje-7b-chat InContextLearning:\n",
        "# - install blobfuse -> sudo apt-get install blobfuse\n",
        "# - pip install transformers\n",
        "# - pip install torch\n",
        "# - pip install accelerate\n",
        "# - pip install jupyter\n",
        "# - pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook overview\n",
        "- Goal: Run experiment for InContext Learning GEITje\n",
        "- Trial run model -> prompt GEITje using, example prompt\n",
        "- Zeroshot prompts\n",
        "- Fewshot prompts\n",
        "\n",
        "Load data and functions:\n",
        "- data is already split\n",
        "- text is already converted to tokens using model tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "import sys\n",
        "sys.path.append('../scripts/') \n",
        "import prompt_template as pt\n",
        "import prediction_helperfunctions as ph\n",
        "import truncation as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Trial run Models \n",
        "Code to run the models with a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='meta-llama/Llama-2-7b-chat-hf',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "## EXAMPLE PROMPT\n",
        "# print(chatbot(\n",
        "    # Conversation('Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?')\n",
        "# ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Experiment functions\n",
        "Prompt GEITje for each document and save the prediction, return response, response time and the prompt version\n",
        "\n",
        "Code structure:\n",
        "- 2 functions/cells:\n",
        "- predictions_incontextlearning -> given a df with docs that need to be predicted, prompt the model\n",
        "- run the experiment -> built in failsaves (df run in parts, with saves in between)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from bm25 import BM25\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template \n",
        "# train_df = dataframe with docs, which can be used as examples/training data/context data\n",
        "# num_examples = number of examples in the prompt\n",
        "\n",
        "def predictions_incontextlearning(chatbot, docs_df, text_column, prompt_function, train_df, num_examples):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date', 'prompt'])\n",
        "\n",
        "\n",
        "    if prompt_function == pt.fewshot_prompt_with_template or prompt_function == pt.fewshot_prompt_no_template:\n",
        "        BM25_model = BM25()\n",
        "        BM25_model.fit(train_df[text_column])\n",
        "   \n",
        "\n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        # if (index + 1) % 200 == 0:\n",
        "        #     print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "\n",
        "        # each prompt function takes different arguments\n",
        "        # zeroshot prompt for geitje\n",
        "        if prompt_function == pt.zeroshot_prompt_geitje:\n",
        "            prompt = prompt_function(txt)\n",
        "\n",
        "        # zeroshot function for mistral and llama\n",
        "        elif prompt_function == pt.zeroshot_prompt_mistral_llama:\n",
        "            prompt = prompt_function(txt)\n",
        "\n",
        "        # select fewshot examples using bm25, fewshot is the same for all models\n",
        "        # elif prompt_function == pt.fewshot_prompt_bm25:\n",
        "        #     prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "        \n",
        "        elif prompt_function == pt.fewshot_prompt_no_template:\n",
        "            prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "\n",
        "        elif prompt_function == pt.fewshot_prompt_with_template:\n",
        "            prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Prompt function not recognised. Check if prompt function is in prompt_template.py and included in the options above.\")\n",
        "\n",
        "        # prompt and get the response\n",
        "        # print(prompt)\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "        print(\"label: \", row['label'].lower())\n",
        "        print(\"response: \", response)\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = ph.get_prediction_from_response(response)\n",
        "        print(\"prediction:\", prediction)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : docs_df.iloc[0]['trunc_col'],\n",
        "            'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': ph.get_datetime(),\n",
        "            'prompt':prompt\n",
        "        }\n",
        "    return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\"\"\"\n",
        "Function to run GEITje In-Context Learning experiment. \n",
        "The function allows to resume experiment, if run_id matches.\n",
        "\"\"\"\n",
        "# df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# run_id = unqiue for each experiment. \n",
        "# prompt_function = which prompt from prompt_template.py to use\n",
        "# text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# label_col = column with the true label\n",
        "# prediction_path = path to file where predictions need to be saved.\n",
        "# overview_path = path to file where results of each run need to be saved.\n",
        "# model_name = name of the model. string.\n",
        "# num_exmples = number of exaples given to prompt. zero in case of zeroshot. \n",
        "\n",
        "def run_experiment(chatbot, df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "    test_df = df.loc[df[split_col]==subset_test]\n",
        "    train_df = df.loc[df[split_col]==subset_train]\n",
        "    \n",
        "    # get rows of df that still need to be predicted for the specific run_id\n",
        "    to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "    # devide to_predict into subsection of 50 predictions at a time. \n",
        "    # Allows to rerun without problem. And save subsections of 50 predictions.\n",
        "    step_range = list(range(0, len(to_predict), 10))\n",
        "\n",
        "    for i in range(len(step_range)):\n",
        "        try:\n",
        "            sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "            print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "        except Exception as e:\n",
        "            sub_to_predict = to_predict[step_range[i]:]\n",
        "            print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "        # prompt geitje\n",
        "        predictions = predictions_incontextlearning(chatbot, sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
        "\n",
        "        # save info\n",
        "        predictions['run_id'] = run_id\n",
        "        predictions['train_set'] = subset_train\n",
        "        predictions['test_set'] = subset_test\n",
        "        predictions['shots'] = num_examples\n",
        "\n",
        "        # save new combinations in file\n",
        "        print(\"Dont interrupt, saving predictions...\")\n",
        "        ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "        # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "        try:\n",
        "            predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "            previous_predictions = predictions\n",
        "        except Exception as e:\n",
        "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "            previous_predictions = predictions\n",
        "\n",
        "        # save results in overview file\n",
        "        date = ph.get_datetime()\n",
        "        y_test = predictions['label']\n",
        "        y_pred = predictions['prediction']\n",
        "\n",
        "        # change error predictions to one error\n",
        "        # error_names = ['NoPredictionInOutput', 'MultiplePredictionErrorInFormatting','NoPredictionFormat', 'MultiplePredictionErrorInOutput']\n",
        "        # y_pred = ['OutputError' if x in error_names else x for x in y_pred]\n",
        "\n",
        "        report = classification_report(y_test, y_pred)\n",
        "\n",
        "        overview = pd.DataFrame(\n",
        "            [{\n",
        "                'model':model_name,\n",
        "                'run_id':run_id,\n",
        "                'date': date,\n",
        "                'train_set': subset_train,\n",
        "                'test_set': subset_test,\n",
        "                'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "                'test_set_support':len(predictions),\n",
        "                'split_col':split_col,\n",
        "                'text_col':df.iloc[0]['trunc_col'],\n",
        "                'runtime':sum(predictions['runtime']),\n",
        "                'accuracy': accuracy_score(y_test, y_pred),\n",
        "                'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "                'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "                'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "                'classification_report':report\n",
        "            }   ]\n",
        "        )\n",
        "        # remove previous results of run_id, replace with new/updated results\n",
        "        ph.replace_and_save_df(overview, overview_path, run_id)\n",
        "        print(\"Saving done! Interrupting is allowed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up variables that are the same for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "TEXT_COLUMN = 'trunc_txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# def fewshot_prompt(doc, train_df, num_examples, text_column, BM25_model):\n",
        "#     # select all texts in train_df, these are possible examples\n",
        "#     examples =list(train_df[text_column].values)\n",
        "\n",
        "#     # calculate BM25 scores for each example\n",
        "#     scores = np.argsort(BM25_model.transform(doc, [item for item in examples]))[::-1]\n",
        "\n",
        "#     # select top examples\n",
        "#     bm25_examples = [examples[score] for score in scores[:num_examples]]\n",
        "\n",
        "#     # start prompt with instructions\n",
        "#     instruction = (\"Classificeer het document in één van de categoriën. \"+\n",
        "#     f\"Eerst krijg je een lijst met mogelijke categoriën, daarna {num_examples} voorbeelden van documenten en tot slot het document dat gecategoriseerd moet worden. \" +\n",
        "#     f\"Categoriën: {pt.get_class_list()}. \"\n",
        "#     )\n",
        "\n",
        "#     # include examples in prompt\n",
        "#     for ex in range(len(bm25_examples)):\n",
        "#         example = bm25_examples[ex]\n",
        "#         label = train_df.loc[train_df[text_column]==example].iloc[0]['label']\n",
        "#         mini_prompt =(\n",
        "#             f\"Voorbeeld document {ex+1}: \" + \n",
        "#             f\"{example} \\n\" +\n",
        "#             f\"Output van voorbeeld document {ex+1}: {{'categorie': {label}}} \\n\")\n",
        "        \n",
        "#         instruction += mini_prompt\n",
        "\n",
        "#     doc_prompt = pt.get_doc_prompt(doc)\n",
        "#     # prompt = SYS_MES_CONTEXT + instruction + doc_prompt + \"[/INST]\"\n",
        "#     prompt = \"<s>[INST] \" + instruction + doc_prompt + \"[/INST]\"\n",
        "#     return prompt\n",
        "\n",
        "\n",
        "# # import numpy as np\n",
        "# # def fewshot_prompt(doc, train_df, num_examples, text_column, BM25_model):\n",
        "# #     # select all texts in train_df, these are possible examples\n",
        "# #     examples =list(train_df[text_column].values)\n",
        "\n",
        "# #     # calculate BM25 scores for each example\n",
        "# #     scores = np.argsort(BM25_model.transform(doc, [item for item in examples]))[::-1]\n",
        "\n",
        "# #     # select top examples\n",
        "# #     bm25_examples = [examples[score] for score in scores[:num_examples]]\n",
        "\n",
        "# #     # start prompt with instructions\n",
        "# #     instruction = (\"Classificeer het document in één van de categoriën. \"+\n",
        "# #     f\"Eerst krijg je een lijst met mogelijke categoriën, daarna {num_examples} voorbeelden van documenten en tot slot het document dat gecategoriseerd moet worden. \" +\n",
        "# #     f\"Categoriën: {pt.get_class_list()}. \"\n",
        "# #     )\n",
        "\n",
        "# #     # include examples in prompt\n",
        "# #     for ex in range(len(bm25_examples)):\n",
        "# #         example = bm25_examples[ex]\n",
        "# #         label = train_df.loc[train_df[text_column]==example].iloc[0]['label']\n",
        "# #         mini_prompt =(\n",
        "# #             f\"Voorbeeld document {ex+1}: \" + \n",
        "# #             f\"{example} \\n\" +\n",
        "# #             f\"Output van voorbeeld document {ex+1}: {{'categorie': {label}}} \\n\")\n",
        "        \n",
        "# #         instruction += mini_prompt\n",
        "\n",
        "# #     doc_prompt = pt.get_doc_prompt(doc)\n",
        "# #     # prompt = SYS_MES_CONTEXT + instruction + doc_prompt + \"[/INST]\"\n",
        "# #     prompt = instruction + doc_prompt\n",
        "# #     return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GEITje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "SHORT_MODEL_NAME = 'GEITje'\n",
        "PROMPT = pt.fewshot_prompt_no_template\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT==pt.zeroshot_prompt_geitje:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_no_template or PROMPT == pt.fewshot_prompt_with_template:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - In-context learning\n",
        "Note - ONLY load one model: either in-context or fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5880f5f199b74c45beb8bb2e111ed1a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d3337e97ecc47369cecb6e12d397f57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                    device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='FemkeBakker/GEITjeSmallData200Tokens',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'GEITjeSmallData200Tokens'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set-up paths to save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/azureuser/cloudfiles/code/blobfuse/raadsinformatie/processed_data/woo_document_classification/predictionsVal/in_context/GEITje/fewshot_prompt_no_template/overview.pkl\n",
            "/home/azureuser/cloudfiles/code/blobfuse/raadsinformatie/processed_data/woo_document_classification/predictionsVal/in_context/GEITje/fewshot_prompt_no_template/predictions.pkl\n",
            "\n",
            " IC_GEITje-7B-chat-v2fewshot_prompt_no_templateLlamaTokens200_0devval_numEx2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if SPLIT_COLUMN == '4split' or SPLIT_COLUMN == '2split':\n",
        "    OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/overview.pkl\"\n",
        "    PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/predictions.pkl\"\n",
        "    \n",
        "elif SPLIT_COLUMN == 'balanced_split':\n",
        "    if SUBFOLDER == 'finetuning':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/overview.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "    elif SUBFOLDER == 'in_context':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/overview2.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "print(OVERVIEW_PATH)\n",
        "print(PREDICTION_PATH)\n",
        "\n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(OVERVIEW_PATH))):\n",
        "    raise ValueError(\"Folder to OVERVIEW_PATH does not exist\") \n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(PREDICTION_PATH))):\n",
        "    raise ValueError(\"Folder to PREDICTION_PATH does not exist\") \n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "print ('\\n', run_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting...0:10 out of 209\n",
            "yeet\n"
          ]
        }
      ],
      "source": [
        "# ----- EXPERIMENT --------\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_geitje, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "# pred_run = pred.loc[pred['run_id']==f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}']\n",
        "display(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Llama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "SHORT_MODEL_NAME = 'Llama'\n",
        "PROMPT = fewshot_prompt\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 100\n",
        "BACK_THRESHOLD = 100\n",
        "\n",
        "if PROMPT==pt.zeroshot_prompt_mistral_llama:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25 or PROMPT == fewshot_prompt:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - In-context learning\n",
        "Note - ONLY load one model: either in-context or fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a24b41463064e6fa55d01d587b87218",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9db964beeb464b839727ac2f980a6237",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='meta-llama/Llama-2-7b-chat-hf',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "# load llama using cpu, else will give cuda out of memory error when running fewshot bm25 prompt.\n",
        "\n",
        "MODEL_NAME = 'Llama-2-7b-chat-hf'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "# chatbot_llama = pipeline(task='conversational', model='FemkeBakker/LlamaSmallData200Tokens',\n",
        "#                    device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'LlamaSmallData200Tokens'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set-up paths to save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/azureuser/cloudfiles/code/blobfuse/raadsinformatie/processed_data/woo_document_classification/predictionsFinal/in_context/Llama/overview2.pkl\n",
            "/home/azureuser/cloudfiles/code/blobfuse/raadsinformatie/processed_data/woo_document_classification/predictionsFinal/in_context/Llama/fewshot_prompt/First100Last100Predictions.pkl\n",
            "\n",
            " IC_Llama-2-7b-chat-hffewshot_promptLlamaTokens100_100traintest_numEx2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if SPLIT_COLUMN == '4split' or SPLIT_COLUMN == '2split':\n",
        "    OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/overview.pkl\"\n",
        "    PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/predictions.pkl\"\n",
        "    \n",
        "elif SPLIT_COLUMN == 'balanced_split':\n",
        "    if SUBFOLDER == 'finetuning':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/overview.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "    elif SUBFOLDER == 'in_context':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/overview2.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "print(OVERVIEW_PATH)\n",
        "print(PREDICTION_PATH)\n",
        "\n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(OVERVIEW_PATH))):\n",
        "    raise ValueError(\"Folder to OVERVIEW_PATH does not exist\") \n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(PREDICTION_PATH))):\n",
        "    raise ValueError(\"Folder to PREDICTION_PATH does not exist\") \n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "print ('\\n', run_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/batch/tasks/shared/LS_root/mounts/clusters/femke-2nd-gpu-110ram/code/Users/f.bakker/document-classification-using-large-language-models/notebooks/../scripts/truncation.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['trunc_txt'] = input\n",
            "/mnt/batch/tasks/shared/LS_root/mounts/clusters/femke-2nd-gpu-110ram/code/Users/f.bakker/document-classification-using-large-language-models/notebooks/../scripts/truncation.py:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['trunc_col'] = f\"Truncation{token_col_name}Front{front_token_threshold}Back{back_token_threshold}\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run-id already known, resuming predictions...\n",
            "Starting...last 8 docs\n",
            "yeet\n",
            "label:  besluit\n",
            "response:   Sure, I can help you with that! Based on the examples you provided, I would categorize the third document as follows:\n",
            "\n",
            " {'categorie': 'Besluit'}\n",
            "\n",
            "Here's why:\n",
            "\n",
            "The document appears to be a municipal decision (Besluit) taken by the General Meeting of the Amsterdam East (Oost) district council on June 2, 2015. The document includes information about the agenda, the decision made, and the voting results. It also includes the registrant number and the date of the meeting.\n",
            "\n",
            "Based on these features, I would categorize the document as a Besluit, which is a type of internal document that is used to record decisions made by a municipal council or committee.\n",
            "prediction: besluit\n",
            "label:  raadsadres\n",
            "response:   Sure, I can help you with that! Based on the provided examples and the text of the document you want to categorize, I would classify it under the category of 'Raadsadres'.\n",
            "\n",
            "Here's the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsadres\"\n",
            "}\n",
            "\n",
            "The reason for this classification is that the document appears to be a formal letter addressed to the city council, requesting action on a specific issue related to the renovation of a neighborhood. The document includes details such as the date, recipient, and registration number, which are all typical elements of a formal letter addressed to a government body.\n",
            "prediction: raadsadres\n",
            "label:  raadsadres\n",
            "response:   Sure, I can help you with that! Based on the two examples you provided, I would categorize the first document as \"Raadsadres\" and the second document as \"Bezoekadres\". Here are the outputs in JSON format:\n",
            "\n",
            "Document 1:\n",
            "{\n",
            "\"categorie\": \"Raadsadres\"\n",
            "}\n",
            "\n",
            "Document 2:\n",
            "{\n",
            "\"categorie\": \"Bezoekadres\"\n",
            "}\n",
            "\n",
            "I hope this helps! Let me know if you have any other questions.\n",
            "prediction: MultiplePredictionErrorInFormatting\n",
            "label:  schriftelijke vraag\n",
            "response:   Sure, I can help you with that! Based on the two examples you provided, I would categorize the third document as follows:\n",
            "\n",
            "{'categorie': 'Schriftelijke Vraag'}\n",
            "\n",
            "The document appears to be a response to a written question submitted to the Gemeente Amsterdam council, specifically regarding the beantwoording of written questions from council member Van Soest. The document provides information on the college's alertness regarding changes in the situation regarding the provision of housing and care, and mentions the council's intention to address this issue in the near future.\n",
            "\n",
            "Therefore, the category 'Schriftelijke Vraag' is the most appropriate for this document.\n",
            "prediction: schriftelijke vraag\n",
            "label:  voordracht\n",
            "response:   Sure, I can help you classify the document! Based on the examples provided and the categories you've given, I would categorize the document as:\n",
            "\n",
            "{'categorie': 'Voordracht'}\n",
            "\n",
            "Here's why:\n",
            "\n",
            "The document is a voordracht (a presentation or proposal) for the Gemeente Raadscommissie (City Council Committee) on housing, housing policy, and cultural heritage. The document is titled \"Voordracht voor de Commissie WV van 07 december 2022\" and is addressed to the committee members. It contains a detailed presentation of the portfolio and agenda points related to housing and cultural heritage, as well as recommendations and proposals for the committee's consideration.\n",
            "\n",
            "The document contains elements typical of a voordracht, such as an introduction, a detailed presentation of the portfolio and agenda points, and conclusions and recommendations. Therefore, based on the examples provided and the categories you've given, I would categorize the document as a voordracht.\n",
            "prediction: voordracht\n",
            "label:  voordracht\n",
            "response:   Sure, I can help you classify the document. Based on the provided examples and the categories you provided, I would categorize the document as:\n",
            "\n",
            "{'categorie': 'Voordracht'}\n",
            "\n",
            "The document appears to be a presentation or a speech given to the council committee, as it contains information on various topics related to the city's finances, airport, and harbor. The document includes a list of agenda items, a brief overview of each item, and some notes and recommendations. This is consistent with the characteristics of a voordracht, which is a formal presentation or speech given to a council or committee.\n",
            "prediction: voordracht\n",
            "label:  motie\n",
            "response:   Sure, I can help you with that! Based on the two examples you provided, I would categorize the third document as follows:\n",
            "\n",
            "{'categorie': 'Motie'}\n",
            "\n",
            "Here's why:\n",
            "\n",
            "The document you provided is a motion submitted to the Gemeente Amsterdam Rathouse in 2013. It proposes the introduction of a \"milieuzone\" for taxis in Amsterdam, with the goal of reducing emissions and improving air quality. The motion is written by a member of the Gemeenteraad (Council) and includes a request to the college of burgemeester en wethouders (Mayor and Aldermen) to take action on the proposal.\n",
            "\n",
            "Based on the content and structure of the document, it appears to be a motion, which is a type of document that is commonly used in municipal governance to propose new policies, laws, or actions to the Gemeenteraad for consideration. Therefore, the category for this document is \"Motie\".\n",
            "prediction: motie\n",
            "label:  brief\n",
            "response:   Sure, I can help you classify the document. Based on the provided examples and the categories you provided, I would categorize the document as:\n",
            "\n",
            "{ 'categorie': 'Brief' }\n",
            "\n",
            "The document appears to be a brief message from the mayor of Amsterdam to the members of the city council, informing them about an evaluation report regarding the municipal elections held on November 22, 2023. The document is written in a formal tone and includes the mayor's signature, which are characteristics of a brief document.\n",
            "prediction: brief\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n"
          ]
        }
      ],
      "source": [
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(small,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_llama, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>run_id</th>\n",
              "      <th>date</th>\n",
              "      <th>train_set</th>\n",
              "      <th>test_set</th>\n",
              "      <th>train_set_support</th>\n",
              "      <th>test_set_support</th>\n",
              "      <th>split_col</th>\n",
              "      <th>text_col</th>\n",
              "      <th>runtime</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_avg_precision</th>\n",
              "      <th>macro_avg_recall</th>\n",
              "      <th>macro_avg_f1</th>\n",
              "      <th>classification_report</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>IC_Llama-2-7b-chat-hfzeroshot_prompt_mistral_l...</td>\n",
              "      <td>2024-05-22 21:29:48.627017+02:00</td>\n",
              "      <td>train</td>\n",
              "      <td>test</td>\n",
              "      <td>9900</td>\n",
              "      <td>1100</td>\n",
              "      <td>balanced_split</td>\n",
              "      <td>TruncationLlamaTokensFront100Back0</td>\n",
              "      <td>140223.734147</td>\n",
              "      <td>0.422727</td>\n",
              "      <td>0.491578</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.287423</td>\n",
              "      <td>precision...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>IC_Llama-2-7b-chat-hfzeroshot_prompt_mistral_l...</td>\n",
              "      <td>2024-05-27 17:53:30.065278+02:00</td>\n",
              "      <td>train</td>\n",
              "      <td>test</td>\n",
              "      <td>9900</td>\n",
              "      <td>1100</td>\n",
              "      <td>balanced_split</td>\n",
              "      <td>TruncationLlamaTokensFront100Back100</td>\n",
              "      <td>164152.727557</td>\n",
              "      <td>0.425455</td>\n",
              "      <td>0.543238</td>\n",
              "      <td>0.334286</td>\n",
              "      <td>0.321521</td>\n",
              "      <td>precision    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>IC_Llama-2-7b-chat-hfzeroshot_prompt_mistral_l...</td>\n",
              "      <td>2024-05-26 21:20:27.966620+02:00</td>\n",
              "      <td>train</td>\n",
              "      <td>test</td>\n",
              "      <td>9900</td>\n",
              "      <td>1100</td>\n",
              "      <td>balanced_split</td>\n",
              "      <td>TruncationLlamaTokensFront200Back0</td>\n",
              "      <td>132040.014386</td>\n",
              "      <td>0.474545</td>\n",
              "      <td>0.537874</td>\n",
              "      <td>0.372857</td>\n",
              "      <td>0.349950</td>\n",
              "      <td>precision    ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model                                             run_id  \\\n",
              "0  Llama-2-7b-chat-hf  IC_Llama-2-7b-chat-hfzeroshot_prompt_mistral_l...   \n",
              "0  Llama-2-7b-chat-hf  IC_Llama-2-7b-chat-hfzeroshot_prompt_mistral_l...   \n",
              "0  Llama-2-7b-chat-hf  IC_Llama-2-7b-chat-hfzeroshot_prompt_mistral_l...   \n",
              "\n",
              "                              date train_set test_set  train_set_support  \\\n",
              "0 2024-05-22 21:29:48.627017+02:00     train     test               9900   \n",
              "0 2024-05-27 17:53:30.065278+02:00     train     test               9900   \n",
              "0 2024-05-26 21:20:27.966620+02:00     train     test               9900   \n",
              "\n",
              "   test_set_support       split_col                              text_col  \\\n",
              "0              1100  balanced_split    TruncationLlamaTokensFront100Back0   \n",
              "0              1100  balanced_split  TruncationLlamaTokensFront100Back100   \n",
              "0              1100  balanced_split    TruncationLlamaTokensFront200Back0   \n",
              "\n",
              "         runtime  accuracy  macro_avg_precision  macro_avg_recall  \\\n",
              "0  140223.734147  0.422727             0.491578          0.310000   \n",
              "0  164152.727557  0.425455             0.543238          0.334286   \n",
              "0  132040.014386  0.474545             0.537874          0.372857   \n",
              "\n",
              "   macro_avg_f1                              classification_report  \n",
              "0      0.287423                                       precision...  \n",
              "0      0.321521                                   precision    ...  \n",
              "0      0.349950                                   precision    ...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "display(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SHORT_MODEL_NAME = 'Mistral'\n",
        "PROMPT = pt.zeroshot_prompt_mistral_llama\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 100\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT==pt.zeroshot_prompt_mistral_llama:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - In-context learning\n",
        "Note - ONLY load one model: either in-context or fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'Mistral-7B-Instruct-v0.2'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='FemkeBakker/MistralSmallData200Tokens',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'MistralSmallData200Tokens'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set-up paths to save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if SPLIT_COLUMN == '4split' or SPLIT_COLUMN == '2split':\n",
        "    OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/overview.pkl\"\n",
        "    PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/predictions.pkl\"\n",
        "    \n",
        "elif SPLIT_COLUMN == 'balanced_split':\n",
        "    if SUBFOLDER == 'finetuning':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/overview.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "    elif SUBFOLDER == 'in_context':\n",
        "        OVERVIEW_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/overview.pkl\"\n",
        "        PREDICTION_PATH = f\"{cf.output_path}/predictionsFinal/{SUBFOLDER}/{SHORT_MODEL_NAME}/{PROMPT_NAME}/First{FRONT_THRESHOLD}Last{BACK_THRESHOLD}Predictions.pkl\"\n",
        "\n",
        "print(OVERVIEW_PATH)\n",
        "print(PREDICTION_PATH)\n",
        "\n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(OVERVIEW_PATH))):\n",
        "    raise ValueError(\"Folder to OVERVIEW_PATH does not exist\") \n",
        "if not os.path.isdir(os.path.dirname(os.path.abspath(PREDICTION_PATH))):\n",
        "    raise ValueError(\"Folder to PREDICTION_PATH does not exist\") \n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "print ('\\n', run_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run experiment\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_mistral, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "display(pred)\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "amsterdamincontextlearning"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "nl"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
