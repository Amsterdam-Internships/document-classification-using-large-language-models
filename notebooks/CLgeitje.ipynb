{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1712584227159
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Select where to run notebook: \"azure\" or \"local\"\n",
        "my_run = \"azure\"\n",
        "\n",
        "import my_secrets as sc\n",
        "import settings as st\n",
        "\n",
        "if my_run == \"azure\":\n",
        "    import config_azure as cf\n",
        "elif my_run == \"local\":\n",
        "    import config as cf\n",
        "\n",
        "\n",
        "import os\n",
        "if my_run == \"azure\":\n",
        "    if not os.path.exists(cf.HUGGING_CACHE):\n",
        "        os.mkdir(cf.HUGGING_CACHE)\n",
        "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
        "\n",
        "# set-up environment - GEITje-7b-chat InContextLearning:\n",
        "# - install blobfuse -> sudo apt-get install blobfuse\n",
        "# - pip install transformers\n",
        "# - pip install torch\n",
        "# - pip install accelerate\n",
        "# - pip install jupyter\n",
        "# - pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook overview\n",
        "- Goal: Run experiment for InContext Learning GEITje\n",
        "- Trial run model -> prompt GEITje using, example prompt\n",
        "- Zeroshot prompts\n",
        "- Fewshot prompts\n",
        "\n",
        "Load data and functions:\n",
        "- data is already split\n",
        "- text is already converted to tokens using model tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "import sys\n",
        "sys.path.append('../scripts/') \n",
        "import prompt_template as pt\n",
        "import prediction_helperfunctions as ph\n",
        "import truncation as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Trial run Models \n",
        "Code to run the models with a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "2024-05-01 12:48:59.201322: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='meta-llama/Llama-2-7b-chat-hf',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "## EXAMPLE PROMPT\n",
        "# print(chatbot(\n",
        "    # Conversation('Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?')\n",
        "# ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Experiment functions\n",
        "Prompt GEITje for each document and save the prediction, return response, response time and the prompt version\n",
        "\n",
        "Code structure:\n",
        "- 2 functions/cells:\n",
        "- predictions_incontextlearning -> given a df with docs that need to be predicted, prompt the model\n",
        "- run the experiment -> built in failsaves (df run in parts, with saves in between)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from bm25 import BM25\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template \n",
        "# train_df = dataframe with docs, which can be used as examples/training data/context data\n",
        "# num_examples = number of examples in the prompt\n",
        "\n",
        "def predictions_incontextlearning(chatbot, docs_df, text_column, prompt_function, train_df, num_examples):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date', 'prompt'])\n",
        "\n",
        "\n",
        "    if prompt_function == pt.fewshot_prompt_bm25:\n",
        "        BM25_model = BM25()\n",
        "        BM25_model.fit(train_df[text_column])\n",
        "    \n",
        "    # elif prompt_function == fewshot_prompt_bm25:\n",
        "    #     BM25_model = BM25()\n",
        "    #     BM25_model.fit(train_df[text_column])\n",
        "\n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        if (index + 1) % 200 == 0:\n",
        "            print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "\n",
        "        # each prompt function takes different arguments\n",
        "        # simple function is zeroshot+simple instruction\n",
        "        if prompt_function == pt.simple_prompt:\n",
        "            prompt = prompt_function(txt)\n",
        "\n",
        "        # elif prompt_function == simple_prompt:\n",
        "        #     prompt = prompt_function(txt)\n",
        "      \n",
        "        # select fewshot examples using bm25\n",
        "        elif prompt_function == pt.fewshot_prompt_bm25:\n",
        "            prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "\n",
        "        # elif prompt_function == fewshot_prompt_bm25:\n",
        "        #     prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Prompt function not recognised. Check if prompt function is in prompt_template.py and included in the options above.\")\n",
        "\n",
        "        # prompt and get the response\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "        print(\"label: \", row['label'].lower())\n",
        "        print(\"response: \", response)\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = ph.get_prediction_from_response(response)\n",
        "        print(\"prediction:\", prediction)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : docs_df.iloc[0]['trunc_col'],\n",
        "            'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': ph.get_datetime(),\n",
        "            'prompt':prompt\n",
        "        }\n",
        "    return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\"\"\"\n",
        "Function to run GEITje In-Context Learning experiment. \n",
        "The function allows to resume experiment, if run_id matches.\n",
        "\"\"\"\n",
        "# df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# run_id = unqiue for each experiment. \n",
        "# prompt_function = which prompt from prompt_template.py to use\n",
        "# text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# label_col = column with the true label\n",
        "# prediction_path = path to file where predictions need to be saved.\n",
        "# overview_path = path to file where results of each run need to be saved.\n",
        "# model_name = name of the model. string.\n",
        "# num_exmples = number of exaples given to prompt. zero in case of zeroshot. \n",
        "\n",
        "def run_experiment(chatbot, df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "    start_time = time.time()\n",
        "    test_df = df.loc[df[split_col]==subset_test]\n",
        "    train_df = df.loc[df[split_col]==subset_train]\n",
        "    \n",
        "    # get rows of df that still need to be predicted for the specific run_id\n",
        "    to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "    # devide to_predict into subsection of 50 predictions at a time. \n",
        "    # Allows to rerun without problem. And save subsections of 50 predictions.\n",
        "    step_range = list(range(0, len(to_predict), 25))\n",
        "\n",
        "    for i in range(len(step_range)):\n",
        "        try:\n",
        "            sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "            print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "        except Exception as e:\n",
        "            sub_to_predict = to_predict[step_range[i]:]\n",
        "            print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "        # prompt geitje\n",
        "        predictions = predictions_incontextlearning(chatbot, sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
        "\n",
        "        # save info\n",
        "        predictions['run_id'] = run_id\n",
        "        predictions['train_set'] = subset_train\n",
        "        predictions['test_set'] = subset_test\n",
        "        predictions['shots'] = num_examples\n",
        "\n",
        "        # save new combinations in file\n",
        "        print(\"Dont interrupt, saving predictions...\")\n",
        "        ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "        # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "        try:\n",
        "            predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "            previous_predictions = predictions\n",
        "        except Exception as e:\n",
        "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "            previous_predictions = predictions\n",
        "\n",
        "        # save results in overview file\n",
        "        date = ph.get_datetime()\n",
        "        y_test = predictions['label']\n",
        "        y_pred = predictions['prediction']\n",
        "        report = classification_report(y_test, y_pred)\n",
        "\n",
        "        overview = pd.DataFrame(\n",
        "            [{\n",
        "                'model':model_name,\n",
        "                'run_id':run_id,\n",
        "                'date': date,\n",
        "                'train_set': subset_train,\n",
        "                'test_set': subset_test,\n",
        "                'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "                'test_set_support':len(predictions),\n",
        "                'split_col':split_col,\n",
        "                'text_col':df.iloc[0]['trunc_col'],\n",
        "                'runtime':sum(predictions['runtime']),\n",
        "                'accuracy': accuracy_score(y_test, y_pred),\n",
        "                'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "                'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "                'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "                'classification_report':report\n",
        "            }   ]\n",
        "        )\n",
        "        # remove previous results of run_id, replace with new/updated results\n",
        "        ph.replace_and_save_df(overview, overview_path, run_id)\n",
        "        print(\"Saving done! Interrupting is allowed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up variables that are the same for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "TEXT_COLUMN = 'trunc_txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GEITje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictions/trialfewShotGeitjepredictions.pkl\"\n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/overview/trialfewShotGeitjepredictions.pkl\"\n",
        "\n",
        "\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "PROMPT = pt.fewshot_prompt_bm25\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT == pt.simple_prompt:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "    \n",
        "OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/in_context/GEITje/{PROMPT_NAME}/overview.pkl\"\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/in_context/GEITje/{PROMPT_NAME}/predictions.pkl\"\n",
        "\n",
        "small = txt.iloc[16:22]\n",
        "small['4split']=['val', 'dev', 'dev', 'dev', 'dev', 'dev']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- EXPERIMENT --------\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "run_id = f'IC_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_geitje, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "# pred_run = pred.loc[pred['run_id']==f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}']\n",
        "display(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Llama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='meta-llama/Llama-2-7b-chat-hf',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "# load llama using cpu, else will give cuda out of memory error when running fewshot bm25 prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictions/Llamapredictions.pkl\"\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/overview/Llamapredictions.pkl\"\n",
        "\n",
        "MODEL_NAME = 'Llama-2-7b-chat-hf'\n",
        "PROMPT = pt.simple_prompt\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens'\n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT == pt.simple_prompt:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "    \n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/in_context/Llama/{PROMPT_NAME}/overview.pkl\"\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/in_context/Llama/{PROMPT_NAME}/predictions.pkl\"\n",
        "\n",
        "run_id = f'IC_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "\n",
        "# small = txt.iloc[7:13]\n",
        "# small['4split']=['val', 'dev', 'dev', 'dev', 'dev', 'dev']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run experiment\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_llama, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "2024-05-02 09:42:45.618238: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-02 09:42:49.122814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd53c2aeb2e245f99649f54cad592e94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f002e1a252b4470bcaae61ce45ebdd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictions/mistralPredictions.pkl\"\n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/overview/mistralPredictions.pkl\"\n",
        "\n",
        "\n",
        "MODEL_NAME = 'Mistral-7B-Instruct-v0.2'\n",
        "PROMPT = pt.fewshot_prompt_bm25\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens'\n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT == pt.simple_prompt:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "\n",
        "\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/in_context/Mistral/{PROMPT_NAME}/overview.pkl\"\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/in_context/Mistral/{PROMPT_NAME}/predictions.pkl\"\n",
        "\n",
        "run_id = f'IC_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "\n",
        "# small = txt.iloc[7:13]\n",
        "# small['4split']=['val', 'dev', 'dev', 'dev', 'dev', 'dev']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run-id already known, resuming predictions...\n",
            "Starting...0:25 out of 134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  Based on the provided information, the document can be categorized as a \"Motie\". Therefore, the output in JSON format would be:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Motie\"}\n",
            "```\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  {\"categorie\": \"Motie\"}\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  voordracht\n",
            "response:  Based on the provided context, the document \"VN2022-027998 Raadscommissie voor Financiën, Lucht en Zeehaven, Bedrijfsvoering, Belastingen 96 Gemeente \"voort | moes FKD Amsterdam Personeel en Organisatie, Dienstverlening, Deelnemingen, Lokale Media, ICT en Digitale Stad, Kunst en Cultuur, Evenementen, Diversiteit en Antidiscriminatiebeleid Voordracht voor de Commissie FKD van 13 oktober 2022 Ter advisering aan de raad Portefeuille Financiën Agendapunt 8 Datum besluit 20 september 2022 Onderwerp Intrekken Verordening VMR op het land 2020\" can be categorized as a \"Besluit\".\n",
            "\n",
            "Output in JSON format: {'categorie': \"Besluit\"}\n",
            "prediction: besluit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  onderzoeksrapport\n",
            "response:  Based on the provided context, the document can be categorized as a \"Raadsnotulen\" (Council minutes) or \"Agenda\" (Agenda). However, the document seems to contain elements of both, such as a management summary and a progress report. Therefore, the most appropriate category would be \"Agenda\" with a sub-category or tag indicating that it includes a quarterly report or minutes from a specific committee or council meeting.\n",
            "\n",
            "Output: {'categorie': 'Agenda','sub-category': 'Quarterly Report/Council Minutes'}\n",
            "prediction: agenda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  voordracht\n",
            "response:  Based on the provided context, the document \"VN2021-034948 N% Gemeente Raadscommissie voor Algemene Zaken, Openbare Orde en Veiligheid, AZ onee Veoh Amsterdam Juridische Zaken, Communicatie, Raadsaangelegenheden, Preventie % en Toezicht Voordracht voor de Commissie AZ van 13 janvari 2022 Ter kennisneming Portefeuille Openbare Orde en Veiligheid Agendapunt 10 Datum besluit Onderwerp A. De halfjaarlijkse voortgangsbrief van het programma Radicalisering en Extremisme B. Vertrouwelijke rapporten\" should be categorized as an \"Onderzoeksrapport\". Therefore, the output in JSON format would be: {\"categorie\": \"Onderzoeksrapport\"}\n",
            "prediction: onderzoeksrapport\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  onderzoeksrapport\n",
            "response:  Based on the provided context and the examples given, the document \"ea | | | __ VERWEY ln ET eh A. 9 Ns, zn, | ie E | Î Kn | Procesevaluatie Subsidieregeling ‘Diversiteit en inclusiviteit voor allianties Amsterdam 2021-2023 Ervaringen met allianties and alliantiebeleid in Amsterdam Marian van der Klein, Mariam Badou en Micky Out mn an _ mn mn - Ervaringen met allianties mm: Procesevaluatie Subsidieregeling ‘Diversiteit en inclusiviteit voor allianties Amsterdam 2021-2023 Marian van der Klein Mariam Badou Micky Out mn Utrecht, november 2022\" should be categorized as an \"Agenda\" or a \"Raadsnotulen\" (Council minutes). However, without more context, it is difficult to determine the exact category. Since the document appears to include minutes of a meeting and mentions the names of people involved, it is more likely to be \"Raadsnot\n",
            "prediction: NoPredictionFormat\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  besluit\n",
            "response:  Based on the provided context, the document should be categorized as \"Besluit\". Therefore, the output in JSON format would be:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Besluit\"}\n",
            "```\n",
            "prediction: besluit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  raadsadres\n",
            "response:  Based on the provided context and the given document, the document can be categorized as 'Raadsadres'. Therefore, the output in JSON format would be:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Raadsadres\"}\n",
            "```\n",
            "prediction: raadsadres\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  schriftelijke vraag\n",
            "response:  Based on the provided context, the document can be categorized as follows:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Schriftelijke Vraag\"}\n",
            "```\n",
            "\n",
            "This document is a response to a written inquiry or question submitted to the city council of Amsterdam, as indicated by the \"Gemeenteblad\" header and the reference to \"schriftelijke vragen\" (written questions).\n",
            "prediction: schriftelijke vraag\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/logging/__init__.py\", line 663, in format\n",
            "    record.message = record.getMessage()\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
            "    msg = msg % self.args\n",
            "TypeError: not all arguments converted during string formatting\n",
            "Call stack:\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
            "    handle._run()\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
            "    await result\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipykernel_69572/3922135593.py\", line 7, in <module>\n",
            "    run_experiment(chatbot_mistral, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n",
            "  File \"/tmp/ipykernel_69572/894049037.py\", line 43, in run_experiment\n",
            "    predictions = predictions_incontextlearning(chatbot, sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
            "  File \"/tmp/ipykernel_69572/1435335569.py\", line 55, in predictions_incontextlearning\n",
            "    converse = chatbot(Conversation(prompt))\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/pipelines/conversational.py\", line 287, in __call__\n",
            "    outputs = super().__call__(conversations, num_workers=num_workers, **kwargs)\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
            "    logger.warning_once(\n",
            "  File \"/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
            "    self.warning(*args, **kwargs)\n",
            "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
            "Arguments: (<class 'UserWarning'>,)\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  Based on the provided context and the example documents, the document to be categorised can be identified as a 'Motie' since it follows the same structure as the example documents, which are all motions presented to the city council of Amsterdam regarding the budget for the year 2023. Therefore, the output in JSON format would be:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Motie\"}\n",
            "```\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  {\"categorie\": \"Motie\"}\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  agenda\n",
            "response:  Based on the provided information, the document can be categorized as an 'Agenda'. Here is the output in the requested JSON format:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Agenda\"}\n",
            "```\n",
            "prediction: agenda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  Based on the provided context and examples, the document to be categorised is:\n",
            "\n",
            "Gemeente Amsterdam\n",
            "Motie\n",
            "Datum raadsvergadering 27 mei 2021\n",
            "Ingekomen onder nummer 371\n",
            "Status Verworpen\n",
            "Onderwerp Motie van de leden Boomsma, Khan en Van Soest inzake de Regionale Energie Strategie Noord-Holland Zuid (Financiële participatie, lokaal eigendom)\n",
            "Aan de gemeenteraad\n",
            "Ondergetekenden hebben de eer voor te stellen:\n",
            "De Raad,\n",
            "Gehoord de discussie over het vaststellen van de Regionale Energiestrategie Noord-Holland Zuid,\n",
            "Overwegende dat:\n",
            "— Een eis is opgenomen van 50 procent lokaal eigendom, om de bewoners beter te betrekken en te stimuleren om deel te nemen in de strategie.\n",
            "\n",
            "The document is a 'Motie'.\n",
            "\n",
            "Output: {'categorie': 'Motie'}\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  Based on the provided information, the document \"x Gemeente Amsterdam R Gemeenteblad Motie Jaar 2016 Afdeling 1 Nummer 1756 Publicatiedatum 6 januari 2017 Ingekomen onder AU Ingekomen op woensdag 21 december 2016 Behandeld op donderdag 22 december 2016 Status Aangenomen Onderwerp Motie van de leden Flentge en Paternotte inzake de ruimtebehoefte voor internationaal geaccrediteerd onderwijs Amsterdam (gedeelde voorzieningen). Aan de gemeentraad Ondergetekenden hebben de eer voor te stellen: De raad, Gehoord de discussie over de ruimtebehoefte\" should be categorized as:\n",
            "\n",
            "{'categorie': 'Motie'}\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  brief\n",
            "response:  Based on the provided context, the document can be categorized as a 'Brief'. Here's the JSON output:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Brief\"}\n",
            "```\n",
            "prediction: brief\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  voordracht\n",
            "response:  Based on the provided context, the document \"iia0a1 034029 X Gemeente Raadscommissie voor Algemene Zaken, Openbare Orde en Veiligheid, AZ Directie Openbare Orde en Veiligheid % Amsterdam Juridische Zaken, Communicatie, Raadsaangelegenheden, Preventie Jeugderiminaliteit, Vluchtelingen en Ongedocumenteerden, Handhaving % Agendapunt 6 Datum besluit 25 november 2021 13 januari 2022 Onderwerp Afsluiting maatregelen ten behoeve van de veiligheid van de Raadssessies\" should be categorized as \"Raadsnotulen\".\n",
            "\n",
            "Output in JSON format: {\"categorie\": \"Raadsnotulen\"}\n",
            "prediction: raadsnotulen\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  brief\n",
            "response:  Based on the provided information, the document can be categorized as 'Raadsnotulen' (Council minutes or proceedings). This is because the document starts with the greeting \"Geachte leden van de gemeenteraad\" (Dear members of the municipal council), and it contains information about a municipal council meeting, specifically about the introduction of the Environmental Act on September 5, 2023.\n",
            "\n",
            "Output in JSON format: {'categorie': 'Raadsnotulen'}\n",
            "prediction: raadsnotulen\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  agenda\n",
            "response:  Based on the provided context, the document \"x Gemeente Amsterdam KSZ, Raadscommissie voor Kunst en Cultuur, Sport en Recreatie, Zorg en Welzijn, Monumenten en Lokale Media, Agenda, woensdag 6 maart 2013\" should be categorized as an \"Agenda\". The output in JSON format would be:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Agenda\"}\n",
            "```\n",
            "prediction: agenda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  schriftelijke vraag\n",
            "response:  Based on the provided context, the document to be categorized is a \"Schriftelijke Vraag\" (Dutch for \"Written Question\" or \"Written Inquiry\"). Here's the JSON output:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Schriftelijke Vraag\"}\n",
            "```\n",
            "prediction: schriftelijke vraag\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  Based on the provided context and examples, the document \"X Gemeente Amsterdam R, Gemeenteblad, Amendement, Jaar 2015, Afdeling 1, Nummer 1023, Publicatiedatum 9 oktober 2015, Ingekomen op 7 oktober 2015, Ingekomen in raadscommissie JC, Te behandelen op 4/5 november 2015, Onderwerp: Amendement van het lid Ernsting inzake de Begroting 2016 (Amsterdam blijft transformeren)\" should be categorized as:\n",
            "\n",
            "{'categorie': 'Motie'}\n",
            "\n",
            "This is based on the fact that all the examples and the document provided are amendments to the city council budget discussions, which is a common format for motions in a city council.\n",
            "prediction: motie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  raadsadres\n",
            "response:  Based on the provided context and the document you have given, the document can be categorized as \"Actualiteit\" due to its content discussing a current issue related to streetcars causing disturbance and overtaking the 1.5-meter distance rule. Therefore, the output in JSON format would be:\n",
            "\n",
            "```json\n",
            "{\"categorie\": \"Actualiteit\"}\n",
            "```\n",
            "prediction: actualiteit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  onderzoeksrapport\n",
            "response:  Based on the provided instruction and context, the task is to categorize the given document into one of the listed categories based on its content. First, you will be given a list of possible categories, followed by two example documents and finally the document that needs to be categorized.\n",
            "\n",
            "Categories: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']\n",
            "\n",
            "Example document 1: \"Jaarverslag 2019 Amsterdamse aanpak statushouders en inburgering\"\n",
            "\n",
            "Example document 1 output: {'categorie': 'Onderzoeksrapport'}\n",
            "\n",
            "Example document 2: \"OMster Nene end\\\" WNW MULT-CRANE con eh dam ne Re \\\" nn F0 hk % IN IR men ANN Te ie A P DE, : ek'h \\ Kk NN IR IN o Is ® ® © ® januari - juni 202\n",
            "prediction: onderzoeksrapport\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label:  motie\n",
            "response:  Based on the provided context and examples, the document \"X Gemeente Amsterdam R, Gemeenteblad, Motie, Jaar 2015, Afdeling 1, Nummer 454, Publicatiedatum 19 juni 2015, Ingekomen op 17 juni 2015, Ingekomen in raadscommissie WE, Te behandelen op 1/2 juli 2015\" should be categorized as follows:\n",
            "\n",
            "Output: {'categorie': 'Motie'}\n",
            "prediction: motie\n"
          ]
        }
      ],
      "source": [
        "# run experiment\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_mistral, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>run_id</th>\n",
              "      <th>date</th>\n",
              "      <th>train_set</th>\n",
              "      <th>test_set</th>\n",
              "      <th>train_set_support</th>\n",
              "      <th>test_set_support</th>\n",
              "      <th>split_col</th>\n",
              "      <th>text_col</th>\n",
              "      <th>runtime</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_avg_precision</th>\n",
              "      <th>macro_avg_recall</th>\n",
              "      <th>macro_avg_f1</th>\n",
              "      <th>classification_report</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mistral-7B-Instruct-v0.2</td>\n",
              "      <td>IC_Mistral-7B-Instruct-v0.2fewshot_prompt_bm25...</td>\n",
              "      <td>2024-05-01 21:06:56.220884+02:00</td>\n",
              "      <td>dev</td>\n",
              "      <td>val</td>\n",
              "      <td>832</td>\n",
              "      <td>75</td>\n",
              "      <td>4split</td>\n",
              "      <td>TruncationLlamaTokensFront200Back0</td>\n",
              "      <td>9123.21593</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.30768</td>\n",
              "      <td>0.325973</td>\n",
              "      <td>precision...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      model  \\\n",
              "0  Mistral-7B-Instruct-v0.2   \n",
              "\n",
              "                                              run_id  \\\n",
              "0  IC_Mistral-7B-Instruct-v0.2fewshot_prompt_bm25...   \n",
              "\n",
              "                              date train_set test_set  train_set_support  \\\n",
              "0 2024-05-01 21:06:56.220884+02:00       dev      val                832   \n",
              "\n",
              "   test_set_support split_col                            text_col     runtime  \\\n",
              "0                75    4split  TruncationLlamaTokensFront200Back0  9123.21593   \n",
              "\n",
              "   accuracy  macro_avg_precision  macro_avg_recall  macro_avg_f1  \\\n",
              "0      0.64                 0.37           0.30768      0.325973   \n",
              "\n",
              "                               classification_report  \n",
              "0                                       precision...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "display(pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gibberish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fewshot Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictions/fewShotGeitjepredictions.pkl\"\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/overview/fewShotGeitjepredictions.pkl\"\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictions/trialfewShotGeitjepredictions.pkl\"\n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/overview/trialfewShotGeitjepredictions.pkl\"\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "TEXT_COLUMN = 'trunc_txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictions/fewShotLlamapredictions.pkl\"\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/overview/fewShotLlamapredictions.pkl\"\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictions/trialfewShotLlamapredictions.pkl\"\n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/overview/trialfewShotLlamapredictions.pkl\"\n",
        "MODEL_NAME = 'Llama-2-7b-chat-hf'\n",
        "TEXT_COLUMN = 'trunc_txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- EXPERIMENT: ?? --------\n",
        "\n",
        "# run experiment\n",
        "PROMPT = pt.fewshot_prompt_bm25\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens'\n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "NUMBER_EXAMPLES = 2\n",
        "# small = txt.iloc[0:5]\n",
        "# small['4split']='val'\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(trunc_df, f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}', PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "# pred_run = pred.loc[pred['run_id']==f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}']\n",
        "# print(sum(pred_run['runtime']))\n",
        "# pred['runtime'] = sum(pred_run['runtime'])\n",
        "display(pred.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(PREDICTION_PATH)\n",
        "pred_run = pred.loc[pred['run_id']==f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}']\n",
        "print(sum(pred_run['runtime']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## End notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_class_list():\n",
        "#     return ['Voordracht', 'Besluit', 'Schriftelijke Vragen', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheets']\n",
        "\n",
        "# def fewshot_prompt_examples(doc, train_df, num_examples, text_column):\n",
        "#     examples = train_df.sample(n=num_examples)\n",
        "\n",
        "#     prompt = f\"\"\"\n",
        "#     Het is jouw taak om een document te categoriseren in één van de categoriën.\n",
        "#     Eerst krijg je een lijst met mogelijke categoriën, daarna {num_examples} voorbeelden van documenten en tot slot het document dat gecategoriseerd moet worden. \n",
        "    \n",
        "#     Categoriën: {get_class_list()}\n",
        "#     \"\"\"\n",
        "\n",
        "#     for index, row in examples.iterrows():\n",
        "#         mini_prompt = f\"\"\"\n",
        "#     Dit is een voorbeeld document de categorie {row['label']}:\n",
        "#         {row[text_column]}\n",
        "#         \"\"\"\n",
        "\n",
        "#         prompt += mini_prompt\n",
        "\n",
        "#     doc_prompt = f\"\"\"\n",
        "#     Categoriseer dit document:\n",
        "#         {doc}\n",
        "#     \"\"\"\n",
        "\n",
        "#     prompt += doc_prompt\n",
        "#     return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def simple_prompt(doc,train_df, num_examples, text_column):\n",
        "#     prompt = f\"\"\"\n",
        "#     Classificeer het document in één van de categoriën.\n",
        "#     Houd het kort, geef enkel de naam van de categorie als response.\n",
        "    \n",
        "#     Categoriën: {get_class_list()}\n",
        "    \n",
        "#     Document: \n",
        "#     {doc}\n",
        "    \n",
        "#     \"\"\"\n",
        "#     return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import time\n",
        "# import os\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# \"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# # docs_df = dataframe with the documents that need to be predicted\n",
        "# # text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# # prompt_function = prompt template \n",
        "\n",
        "# def predictions_incontextlearning(docs_df, text_column, prompt_function, train_df, num_examples):\n",
        "#     results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date', 'prompt'])\n",
        "    \n",
        "#     # prompt each document\n",
        "#     for index, row in docs_df.iterrows():\n",
        "#         if (index + 1) % 200 == 0:\n",
        "#             print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         # get the prompt, with the doc filled in\n",
        "#         txt = row[text_column]\n",
        "\n",
        "#         # always give these as input, however not every template uses all of them\n",
        "#         prompt = prompt_function(txt, train_df, num_examples, text_column)\n",
        "\n",
        "#         # prompt and get the response\n",
        "#         converse = chatbot(Conversation(prompt))\n",
        "#         response = converse[1]['content']\n",
        "\n",
        "#         # extract prediction from response\n",
        "#         prediction = ph.get_prediction_from_response(response)\n",
        "\n",
        "#         # save results in dataframe\n",
        "#         results_df.loc[len(results_df)] = {\n",
        "#             'id': row['id'],\n",
        "#             'path' : row['path'],\n",
        "#             'text_column' : text_column,\n",
        "#             'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "#             'response':response,\n",
        "#             'prediction':prediction,\n",
        "#             'label':row['label'].lower(),\n",
        "#             'runtime':time.time()-start_time,\n",
        "#             'date': ph.get_datetime(),\n",
        "#             'prompt':prompt\n",
        "#         }\n",
        "#     return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import time\n",
        "# from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# \"\"\"\n",
        "# Function to run GEITje In-Context Learning experiment. \n",
        "# The function allows to resume experiment, if run_id matches.\n",
        "# \"\"\"\n",
        "# # df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# # run_id = unqiue for each experiment. \n",
        "# # prompt_function = which prompt from prompt_template.py to use\n",
        "# # text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# # split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# # subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# # subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# # label_col = column with the true label\n",
        "# # prediction_path = path to file where predictions need to be saved.\n",
        "# # overview_path = path to file where results of each run need to be saved.\n",
        "# # model_name = name of the model. string.\n",
        "# # num_exmples = number of exaples given to prompt. zero in case of zeroshot. \n",
        "\n",
        "# def run_experiment(df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "#     print(num_examples)\n",
        "#     start_time = time.time()\n",
        "#     test_df = df.loc[df[split_col]==subset_test]\n",
        "#     train_df = df.loc[df[split_col]==subset_train]\n",
        "    \n",
        "#     # get rows of df that still need to be predicted for the specific run_id\n",
        "#     to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "#     # devide to_predict into subsection of 50 predictions at a time. \n",
        "#     # Allows to rerun without problem. \n",
        "#     step_range = list(range(0, len(to_predict), 3))\n",
        "\n",
        "#     for i in range(len(step_range)):\n",
        "#         try:\n",
        "#             sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "#             print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "#         except Exception as e:\n",
        "#             sub_to_predict = to_predict[step_range[i]:]\n",
        "#             print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "#         # prompt geitje\n",
        "#         predictions = predictions_incontextlearning(sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
        "\n",
        "#         # save info\n",
        "#         predictions['run_id'] = run_id\n",
        "#         predictions['train_set'] = subset_train\n",
        "#         predictions['test_set'] = subset_test\n",
        "#         predictions['shots'] = num_examples\n",
        "\n",
        "#         # save new combinations in file\n",
        "#         ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "#         # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "#         try:\n",
        "#             predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "#         except Exception as e:\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "\n",
        "#         # save results in overview file\n",
        "#         date = ph.get_datetime()\n",
        "#         y_test = predictions['label']\n",
        "#         y_pred = predictions['prediction']\n",
        "#         report = classification_report(y_test, y_pred)\n",
        "\n",
        "#         overview = pd.DataFrame(\n",
        "#             [{\n",
        "#                 'model':model_name,\n",
        "#                 'run_id':run_id,\n",
        "#                 'date': date,\n",
        "#                 'train_set': subset_train,\n",
        "#                 'test_set': subset_test,\n",
        "#                 'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "#                 'test_set_support':len(predictions),\n",
        "#                 'split_col':split_col,\n",
        "#                 'text_col':text_col,\n",
        "#                 'runtime':time.time()-start_time,\n",
        "#                 'accuracy': accuracy_score(y_test, y_pred),\n",
        "#                 'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "#                 'classification_report':report\n",
        "#             }   ]\n",
        "#         )\n",
        "#         # remove previous results of run_id, replace with new/updated results\n",
        "#         ph.replace_and_save_df(overview, overview_path, run_id)\n",
        "\n",
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictions/tryoutGeitjepredictions.pkl\"\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/overview/tryoutGeitjepredictions.pkl\"\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "TEXT_COLUMN = 'trunc_txt'\n",
        "\n",
        "p_path = f\"{cf.output_path}/predictions/tryoutGeitjepredictions.pkl\"\n",
        "o_path = f\"{cf.output_path}/overview/tryoutGeitjeoverview.pkl\"\n",
        "run_experiment(small, 'tryout_zeroshot', pt.simple_prompt, 'trunc_txt', '4split', 'dev', 'val', 'label', p_path, o_path, 'GEITje-7B-chat-v2', 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yeet = pd.read_pickle(p_path)\n",
        "yeet  = yeet.loc[yeet['run_id']=='tryout_zeroshot']\n",
        "display(yeet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(yeet.iloc[0]['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "small = txt.iloc[0:5]\n",
        "small['4split']=['val', 'dev', 'val', 'dev', 'dev']\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(small,'text', 'LlamaTokens', 200, 200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GIbberish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import time\n",
        "# import os\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# \"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# # docs_df = dataframe with the documents that need to be predicted\n",
        "# # text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# # prompt_function = prompt template -> ONLY prompt templates that take doc as input (ZERO SHOT)\n",
        "\n",
        "# def zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function):\n",
        "#     results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date'])\n",
        "    \n",
        "#     # prompt each document\n",
        "#     for index, row in docs_df.iterrows():\n",
        "#         if (index + 1) % 200 == 0:\n",
        "#             print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         # get the prompt, with the doc filled in\n",
        "#         txt = row[text_column]\n",
        "#         prompt = prompt_function(txt)\n",
        "\n",
        "#         # prompt and get the response\n",
        "#         converse = chatbot(Conversation(prompt))\n",
        "#         response = converse[1]['content']\n",
        "\n",
        "#         # extract prediction from response\n",
        "#         prediction = ph.get_prediction_from_response(response)\n",
        "\n",
        "#         # save results in dataframe\n",
        "#         results_df.loc[len(results_df)] = {\n",
        "#             'id': row['id'],\n",
        "#             'path' : row['path'],\n",
        "#             'text_column' : text_column,\n",
        "#             'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "#             'response':response,\n",
        "#             'prediction':prediction,\n",
        "#             'label':row['label'].lower(),\n",
        "#             'runtime':time.time()-start_time,\n",
        "#             'date': ph.get_datetime()\n",
        "#         }\n",
        "#     return results_df\n",
        "\n",
        "\n",
        "#  import os\n",
        "# import time\n",
        "# from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# \"\"\"\n",
        "# Function to run GEITje ZEROSHOT experiment. \n",
        "# The function allows to resume experiment, if run_id matches.\n",
        "# \"\"\"\n",
        "# # df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# # run_id = unqiue for each experiment. \n",
        "# # prompt_function = which prompt from prompt_template.py to use\n",
        "# # text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# # split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# # subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# # subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# # label_col = column with the true label\n",
        "# # prediction_path = path to file where predictions need to be saved.\n",
        "# # overview_path = path to file where results of each run need to be saved.\n",
        "# # model_name = name of the model. string.\n",
        "# # num_exmples = number of exaples given to prompt. zero inn case of zeroshot. \n",
        "\n",
        "# def run_experiment(df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "#     start_time = time.time()\n",
        "#     test_df = df.loc[df[split_col]==subset_test]\n",
        "    \n",
        "#     # get rows of df that still need to be predicted for the specific run_id\n",
        "#     to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "#     # devide to_predict into subsection of 50 predictions at a time. \n",
        "#     # Allows to rerun without problem. \n",
        "#     step_range = list(range(0, len(to_predict), 3))\n",
        "\n",
        "#     for i in range(len(step_range)):\n",
        "#         try:\n",
        "#             sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "#             print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "#         except Exception as e:\n",
        "#             sub_to_predict = to_predict[step_range[i]:]\n",
        "#             print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "#         # prompt geitje\n",
        "#         predictions = zero_shot_predictions_incontextlearning(sub_to_predict, text_col, prompt_function)\n",
        "\n",
        "#         # save info\n",
        "#         predictions['run_id'] = run_id\n",
        "#         predictions['train_set'] = subset_train\n",
        "#         predictions['test_set'] = subset_test\n",
        "#         predictions['shots'] = num_examples\n",
        "\n",
        "#         # save new combinations in file\n",
        "#         ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "#         # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "#         try:\n",
        "#             predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "#         except Exception as e:\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "\n",
        "#         # save results in overview file\n",
        "#         date = ph.get_datetime()\n",
        "#         y_test = predictions['label']\n",
        "#         y_pred = predictions['prediction']\n",
        "#         report = classification_report(y_test, y_pred)\n",
        "\n",
        "#         overview = pd.DataFrame(\n",
        "#             [{\n",
        "#                 'model':model_name,\n",
        "#                 'run_id':run_id,\n",
        "#                 'date': date,\n",
        "#                 'train_set': subset_train,\n",
        "#                 'test_set': subset_test,\n",
        "#                 'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "#                 'test_set_support':len(predictions),\n",
        "#                 'split_col':split_col,\n",
        "#                 'text_col':text_col,\n",
        "#                 'runtime':time.time()-start_time,\n",
        "#                 'accuracy': accuracy_score(y_test, y_pred),\n",
        "#                 'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "#                 'classification_report':report\n",
        "#             }   ]\n",
        "#         )\n",
        "#         # remove previous results of run_id, replace with new/updated results\n",
        "#         ph.replace_and_save_df(overview, overview_path, run_id)\n",
        " \n",
        "# # p_path = f\"{cf.output_path}/predictions/tryoutGeitjepredictions.pkl\"\n",
        "# # o_path = f\"{cf.output_path}/overview/tryoutGeitjeoverview.pkl\"\n",
        "# # run_experiment(txt.iloc[25:30], 'tryout', pt.simple_prompt, trunc_col, '4split', 'dev', 'val', 'label', p_path, o_path, 'GEITje-7B-chat-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n",
        "df = df.loc[df['set']=='val']\n",
        "df['text_trunc_100'] = df['tokens'].apply(text_truncation,100)\n",
        "df['text_trunc_1000'] = df['tokens'].apply(text_truncation,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\"\n",
        "resume_predictions(df, path, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dummy code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_in_subsections(df, path, set_run_id):\n",
        "\n",
        "    iterations = list(range(0, len(df)+50, 50))\n",
        "    for i in range(len(iterations)):\n",
        "        try:\n",
        "            subdf = df.iloc[iterations[i]:iterations[i+1]]\n",
        "\n",
        "        except IndexError:\n",
        "            subdf = df.iloc[iterations[i]:]\n",
        "\n",
        "        # if set_run_id == 'new' and iterations[i]==0:\n",
        "        #     run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'new', path, 'val')\n",
        "        # else:\n",
        "        #     run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'previous', path, 'val')\n",
        "\n",
        "\n",
        "\n",
        "path = f\"{cf.output_path}/predictions/ICgeitje_predictions_tryout.pkl\"\n",
        "run_in_subsections(df, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_in_subsections(df, path):\n",
        "    subdf = df.iloc[0:50]\n",
        "    run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'new', path, 'val')\n",
        "\n",
        "    iterations = list(range(50, len(df)+50, 50))\n",
        "    for i in range(len(iterations)):\n",
        "        if i < len(iterations)-2:\n",
        "            subdf = df.iloc[iterations[i]:iterations[i+1]]\n",
        "            print(\"\\n\", \"iterations\", iterations[i], iterations[i+1], \"\\n\")\n",
        "            run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'previous', path, 'val')\n",
        "\n",
        "        elif i < len(iterations)-1:\n",
        "            subdf = df.iloc[iterations[i]:]\n",
        "            print(\"\\n\", \"iterations\", iterations[i], '\\n')\n",
        "            run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'previous', path, 'val')\n",
        "\n",
        "path = f\"{cf.output_path}/predictions/ICgeitje_predictions_tryout.pkl\"\n",
        "run_in_subsections(df, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yeet = pd.read_pickle(f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\")\n",
        "display(yeet)\n",
        "\n",
        "yeet = pd.read_pickle(f\"{cf.output_path}/overview_results.pkl\")\n",
        "display(yeet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import sys\n",
        "sys.path.append('../scripts/') \n",
        "import prompt_template as pt\n",
        "import prediction_helperfunctions as ph\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template -> ONLY prompt templates that take doc as input (ZERO SHOT)\n",
        "\n",
        "def zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date'])\n",
        "    \n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        if (index + 1) % 200 == 0:\n",
        "            print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "        prompt = prompt_function(txt)\n",
        "\n",
        "        # prompt and get the response\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = ph.get_prediction_from_response(response)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : text_column,\n",
        "            'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': ph.get_datetime()\n",
        "        }\n",
        "    return results_df\n",
        "\n",
        "# \"\"\" Run a prediction function -> can be ZeroShot or FewShot \"\"\"\n",
        "# def run_prediction(docs_df, text_column, prompt_function, subset=None, learning='ZeroShot'):\n",
        "#     if learning == 'ZeroShot':\n",
        "#         # get the predictions\n",
        "#         res = zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function)\n",
        "\n",
        "#         # INSERT ELSE STATEMENT HERE FOR FEWSHOT\n",
        "\n",
        "#         # get run_id\n",
        "#         path = f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\"\n",
        "#         res['run_id'], predictions_df = ph.get_runid(path)\n",
        "\n",
        "#         # combine earlier predictions with new ones\n",
        "#         all_predictions = pd.concat([predictions_df, res])\n",
        "\n",
        "#         # save predictions\n",
        "#         all_predictions.to_pickle(path)\n",
        "\n",
        "#         # save the evaluation metrics for each run\n",
        "#         ph.update_overview_results(res, 'Rijgersberg/GEITje-7B-chat-v2')\n",
        "#         return res\n",
        "# gestart om 10.15/\n",
        "# res = run_prediction(df, 'text_trunc_100', pt.simple_prompt, 'val')\n",
        "# display(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yeet = pd.read_pickle(f\"{cf.output_path}/overview_results.pkl\")\n",
        "display(yeet)\n",
        "\n",
        "yeet = pd.read_pickle(f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\")\n",
        "display(yeet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tryout GEITje\n",
        "Load chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='auto')\n",
        "\n",
        "## simple query\n",
        "print(chatbot(\n",
        "    Conversation(\"Hallo, ik ben Bram. Ik wil vanavond graag een film kijken. Heb je enkele suggesties?\")\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "# load_in_8bit: lower precision but saves a lot of GPU memory\n",
        "# device_map=auto: loads the model across multiple GPUs\n",
        "# chatbot = pipeline(\"conversational\", model=\"BramVanroy/GEITje-7B-ultra\",  model_kwargs={\"load_in_8bit\": True}, device_map=\"auto\")\n",
        "chatbot = pipeline(\"conversational\", model=\"BramVanroy/GEITje-7B-ultra\",  device_map=\"auto\")\n",
        "\n",
        "# start_messages = [\n",
        "#     # {\"role\": \"system\", \"content\": \"Je bent een grappige chatbot die Bert heet. Je maakt vaak mopjes.\"},\n",
        "#     {\"role\": \"user\", \"content\": \"Hallo, ik ben Bram. Ik wil vanavond graag een film kijken. Heb je enkele suggesties?\"}\n",
        "# ]\n",
        "# conversation = Conversation(start_messages)\n",
        "# conversation = chatbot(conversation)\n",
        "# response = conversation.messages[-1][\"content\"]\n",
        "# print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = df.iloc[0]['text']\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Classificeer de gegeven tekst in 1 van de categoriën.\n",
        "Geef als reactie enkel de naam van de categorie\n",
        "Categorieën: ['Voordracht', 'Besluit', 'Schriftelijke Vragen', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheets']\n",
        "Tekst: \n",
        "\n",
        "{txt}\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "start_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Jouw enige taak is om teksten te classificeren. Je geeft geen uitleg voor je keuzes.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chatbot(Conversation(start_messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = df.loc[df['clean_tokens_count'].idxmax()]['text']\n",
        "print(df.loc[df['clean_tokens_count'].idxmax()]['clean_tokens_count'])\n",
        "\n",
        "print(pt.simple_prompt(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(chatbot(\n",
        "    Conversation(pt.simple_prompt(text))\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16,\n",
        "                                             low_cpu_mem_usage=True, attn_implementation='eager',\n",
        "                                             device_map=device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate(conversation, temperature=0.2, top_k=50, max_new_tokens=1_000):\n",
        "    tokenized = tokenizer.apply_chat_template(conversation, add_generation_prompt=True,\n",
        "                                              return_tensors='pt').to(device)\n",
        "    outputs = model.generate(tokenized, do_sample=True, temperature=temperature,\n",
        "                             top_k=top_k, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?'\n",
        "    }\n",
        "]\n",
        "print(generate(conversation))\n",
        "# <|user|>\n",
        "# Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"? \n",
        "# <|assistant|>\n",
        "# Het woord dat niet op zijn plaats staat is 'geit'. Een geit zou niet tussen een lijst van vervoersmiddelen moeten staan. Het past beter bij een boerderijthema of dierenlijst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BACK-UP CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import pytz\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\"\"\" Given the string response, extract the prediction \"\"\"\n",
        "def get_prediction_from_response(response):\n",
        "    # get a list of the possible classes\n",
        "    classes_list = pt.get_class_list()\n",
        "\n",
        "    predictions = [True if category.lower() in response.lower() else False for category in classes_list]\n",
        "\n",
        "    # check if multiple classes were named, this is a prediction error\n",
        "    if Counter(predictions)[True] > 1:\n",
        "        return \"PredictionError\"\n",
        "\n",
        "    # check if exactly one class is named, this is the prediction\n",
        "    elif Counter(predictions)[True] == 1:\n",
        "        prediction = [category.lower() for category in classes_list if category.lower() in response.lower()]\n",
        "        return prediction[0]\n",
        "\n",
        "    # if no class is named, then this is a no prediction error\n",
        "    else:\n",
        "        return 'NoPrediction'\n",
        "\n",
        "\"\"\" Extract the promptfunction name \"\"\"\n",
        "def get_promptfunction_name(prompt_function):\n",
        "    string = f\"{prompt_function}\"\n",
        "    match = re.search(r'<function\\s+(\\w+)', string)\n",
        "    if match:\n",
        "        function_name = match.group(1)\n",
        "        return function_name\n",
        "    else:\n",
        "        return f\"{prompt_function}\"\n",
        "    \n",
        "\"\"\" Get the current time in the Netherlands \"\"\"\n",
        "def get_datetime():\n",
        "    current_datetime_utc = datetime.datetime.now(pytz.utc)\n",
        "\n",
        "    # Convert UTC time to Dutch time (CET)\n",
        "    dutch_timezone = pytz.timezone('Europe/Amsterdam')\n",
        "    current_datetime_dutch = current_datetime_utc.astimezone(dutch_timezone)\n",
        "    return current_datetime_dutch\n",
        "        \n",
        "\"\"\" Get the new runid \"\"\"\n",
        "def get_runid(path):\n",
        "\n",
        "    # if not first run, set runid to most recent run+1\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_pickle(path)\n",
        "        return max(df['run_id'])+1, df\n",
        "\n",
        "    # if first run, set runid to 0\n",
        "    else:\n",
        "        return 0, pd.DataFrame()\n",
        "    \n",
        "\"\"\" Save evaluation metrics of a run \"\"\"\n",
        "def update_overview_results(df, model_name, subset=None):\n",
        "    # df= dataframe with predictions for each do, one row per doc/prediction\n",
        "    # model_name = string with the name of the model\n",
        "    # subset = can be train, val, or test, or left open\n",
        " \n",
        "    # get evalaution scores\n",
        "    evaluation_dict = classification_report(df['label'], df['prediction'], output_dict=True)\n",
        "    evaluation = pd.DataFrame(evaluation_dict).transpose()\n",
        "    \n",
        "    new_row = {\n",
        "        # stuff about the run\n",
        "        'run_id':df.iloc[0]['run_id'],\n",
        "        'model':model_name,\n",
        "        'prompt_function':df.iloc[0]['prompt_function'],\n",
        "        'text_column':df.iloc[0]['text_column'],\n",
        "        'date': get_datetime(),\n",
        "        'runtime':sum(df['runtime']),\n",
        "        'set':subset,\n",
        "        'support':evaluation.iloc[-1]['support'],\n",
        "\n",
        "        # evaluation\n",
        "        'accuracy': evaluation_dict['accuracy'],\n",
        "\n",
        "        'recall_weighted_avg':evaluation.loc[evaluation.index=='weighted avg']['recall'].values[0],\n",
        "        'precision_weighted_avg': evaluation.loc[evaluation.index=='weighted avg']['precision'].values[0],\n",
        "        'f1_weighted_avg': evaluation.loc[evaluation.index=='weighted avg']['f1-score'].values[0],\n",
        "\n",
        "        'recall_macro_avg':evaluation.loc[evaluation.index=='macro avg']['recall'].values[0],\n",
        "        'precision_macro_avg': evaluation.loc[evaluation.index=='macro avg']['precision'].values[0],\n",
        "        'f1_macro_avg': evaluation.loc[evaluation.index=='macro avg']['f1-score'].values[0],\n",
        "\n",
        "\n",
        "        'recall_classes': dict(zip(evaluation.index[0:-3], evaluation['recall'][0:-3])),\n",
        "        'precision_classes': dict(zip(evaluation.index[0:-3], evaluation['precision'][0:-3])),\n",
        "        'f1_classes': dict(zip(evaluation.index[0:-3], evaluation['f1-score'][0:-3])),\n",
        "        'support_classes': dict(zip(evaluation.index[0:-3], evaluation['support'][0:-3])),\n",
        "\n",
        "        # docs that were predicted\n",
        "        'doc_paths':list(df['path'].values)\n",
        "        \n",
        "    }\n",
        "\n",
        "    # create a new dataframe with the evaluation, each run has one row\n",
        "    results = pd.DataFrame(columns=new_row.keys())\n",
        "    results.loc[len(results)] = new_row\n",
        "   \n",
        "    # if not the first run, get results from previous runs\n",
        "    path = f\"{cf.output_path}/overview_results.pkl\"\n",
        "    if os.path.exists(path):\n",
        "        earlier_results = pd.read_pickle(path)\n",
        "\n",
        "        # combine evaluation of previous runs with current run\n",
        "        results = pd.concat([earlier_results, results])\n",
        "\n",
        "    # save to overview_results.pkl\n",
        "    results.to_pickle(path)\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# update_overview_results(res, 'geitje')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# yeet = pd.read_pickle(f\"{cf.output_path}/overview_results.pkl\")\n",
        "# display(yeet)\n",
        "\n",
        "# yeet = pd.read_pickle(f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\")\n",
        "# display(yeet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template -> ONLY prompt templates that take doc as input (ZERO SHOT)\n",
        "\n",
        "def zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date'])\n",
        "    \n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "        prompt = prompt_function(txt)\n",
        "\n",
        "        # prompt and get the response\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = get_prediction_from_response(response)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : text_column,\n",
        "            'prompt_function': get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': get_datetime()\n",
        "        }\n",
        "    return results_df\n",
        "\n",
        "\"\"\" Run a prediction function -> can be ZeroShot or FewShot \"\"\"\n",
        "def run_prediction(docs_df, text_column, prompt_function, subset=None, learning='ZeroShot'):\n",
        "    if learning == 'ZeroShot':\n",
        "        # get the predictions\n",
        "        res = zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function)\n",
        "\n",
        "        # INSERT ELSE STATEMENT HERE FOR FEWSHOT\n",
        "\n",
        "        # get run_id\n",
        "        path = f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\"\n",
        "        res['run_id'], predictions_df = get_runid(path)\n",
        "\n",
        "        # combine earlier predictions with new ones\n",
        "        all_predictions = pd.concat([predictions_df, res])\n",
        "\n",
        "        # save predictions\n",
        "        all_predictions.to_pickle(path)\n",
        "\n",
        "        # save the evaluation metrics for each run\n",
        "        update_overview_results(res, 'Rijgersberg/GEITje-7B-chat-v2')\n",
        "        return res\n",
        "\n",
        "res = run_prediction(df, 'text_trunc', pt.simple_prompt)\n",
        "display(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "amsterdamincontextlearning"
    },
    "kernelspec": {
      "display_name": "AmsterdamInContextLearning",
      "language": "python",
      "name": "amsterdamincontextlearning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "nl"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
