{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1712584227159
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Select where to run notebook: \"azure\" or \"local\"\n",
        "my_run = \"azure\"\n",
        "\n",
        "import my_secrets as sc\n",
        "import settings as st\n",
        "\n",
        "if my_run == \"azure\":\n",
        "    import config_azure as cf\n",
        "elif my_run == \"local\":\n",
        "    import config as cf\n",
        "\n",
        "\n",
        "import os\n",
        "if my_run == \"azure\":\n",
        "    if not os.path.exists(cf.HUGGING_CACHE):\n",
        "        os.mkdir(cf.HUGGING_CACHE)\n",
        "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
        "\n",
        "# set-up environment - GEITje-7b-chat InContextLearning:\n",
        "# - install blobfuse -> sudo apt-get install blobfuse\n",
        "# - pip install transformers\n",
        "# - pip install torch\n",
        "# - pip install accelerate\n",
        "# - pip install jupyter\n",
        "# - pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook overview\n",
        "- Goal: Run experiment for InContext Learning GEITje\n",
        "- Trial run model -> prompt GEITje using, example prompt\n",
        "- Zeroshot prompts\n",
        "- Fewshot prompts\n",
        "\n",
        "Load data and functions:\n",
        "- data is already split\n",
        "- text is already converted to tokens using model tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# df = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "import sys\n",
        "sys.path.append('../scripts/') \n",
        "import prompt_template as pt\n",
        "import prediction_helperfunctions as ph\n",
        "import truncation as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Trial run Models \n",
        "Code to run the models with a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='meta-llama/Llama-2-7b-chat-hf',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "## EXAMPLE PROMPT\n",
        "# print(chatbot(\n",
        "    # Conversation('Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?')\n",
        "# ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Experiment functions\n",
        "Prompt GEITje for each document and save the prediction, return response, response time and the prompt version\n",
        "\n",
        "Code structure:\n",
        "- 2 functions/cells:\n",
        "- predictions_incontextlearning -> given a df with docs that need to be predicted, prompt the model\n",
        "- run the experiment -> built in failsaves (df run in parts, with saves in between)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from bm25 import BM25\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template \n",
        "# train_df = dataframe with docs, which can be used as examples/training data/context data\n",
        "# num_examples = number of examples in the prompt\n",
        "\n",
        "def predictions_incontextlearning(chatbot, docs_df, text_column, prompt_function, train_df, num_examples):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date', 'prompt'])\n",
        "\n",
        "\n",
        "    if prompt_function == pt.fewshot_prompt_bm25 or prompt_function== pt.OldFewshot_prompt_bm25:\n",
        "        BM25_model = BM25()\n",
        "        BM25_model.fit(train_df[text_column])\n",
        "    \n",
        "    # elif prompt_function == fewshot_prompt_bm25:\n",
        "    #     BM25_model = BM25()\n",
        "    #     BM25_model.fit(train_df[text_column])\n",
        "\n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        if (index + 1) % 200 == 0:\n",
        "            print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "\n",
        "        # each prompt function takes different arguments\n",
        "        # simple function is zeroshot+simple instruction\n",
        "        if prompt_function == pt.simple_prompt:\n",
        "            prompt = prompt_function(txt)\n",
        "\n",
        "        elif prompt_function == pt.simple_prompt_v2:\n",
        "            prompt = prompt_function(txt)\n",
        "\n",
        "        elif prompt_function == pt.OldSimple_prompt:\n",
        "            prompt = prompt_function(txt)\n",
        "\n",
        "        elif prompt_function == pt.geitje_simple_prompt:\n",
        "            prompt = prompt_function(txt)\n",
        "      \n",
        "        # select fewshot examples using bm25\n",
        "        elif prompt_function == pt.fewshot_prompt_bm25:\n",
        "            prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "        \n",
        "        elif prompt_function == pt.OldFewshot_prompt_bm25:\n",
        "            prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "\n",
        "        # elif prompt_function == fewshot_prompt_bm25:\n",
        "        #     prompt = prompt_function(txt, train_df, num_examples, text_column, BM25_model)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Prompt function not recognised. Check if prompt function is in prompt_template.py and included in the options above.\")\n",
        "\n",
        "        # prompt and get the response\n",
        "        # print(prompt)\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "        print(\"label: \", row['label'].lower())\n",
        "        print(\"response: \", response)\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = ph.get_prediction_from_response(response)\n",
        "        print(\"prediction:\", prediction)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : docs_df.iloc[0]['trunc_col'],\n",
        "            'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': ph.get_datetime(),\n",
        "            'prompt':prompt\n",
        "        }\n",
        "    return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\"\"\"\n",
        "Function to run GEITje In-Context Learning experiment. \n",
        "The function allows to resume experiment, if run_id matches.\n",
        "\"\"\"\n",
        "# df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# run_id = unqiue for each experiment. \n",
        "# prompt_function = which prompt from prompt_template.py to use\n",
        "# text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# label_col = column with the true label\n",
        "# prediction_path = path to file where predictions need to be saved.\n",
        "# overview_path = path to file where results of each run need to be saved.\n",
        "# model_name = name of the model. string.\n",
        "# num_exmples = number of exaples given to prompt. zero in case of zeroshot. \n",
        "\n",
        "def run_experiment(chatbot, df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "    start_time = time.time()\n",
        "    test_df = df.loc[df[split_col]==subset_test]\n",
        "    train_df = df.loc[df[split_col]==subset_train]\n",
        "    \n",
        "    # get rows of df that still need to be predicted for the specific run_id\n",
        "    to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "    # devide to_predict into subsection of 50 predictions at a time. \n",
        "    # Allows to rerun without problem. And save subsections of 50 predictions.\n",
        "    step_range = list(range(0, len(to_predict), 10))\n",
        "\n",
        "    for i in range(len(step_range)):\n",
        "        try:\n",
        "            sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "            print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "        except Exception as e:\n",
        "            sub_to_predict = to_predict[step_range[i]:]\n",
        "            print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "        # prompt geitje\n",
        "        predictions = predictions_incontextlearning(chatbot, sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
        "\n",
        "        # save info\n",
        "        predictions['run_id'] = run_id\n",
        "        predictions['train_set'] = subset_train\n",
        "        predictions['test_set'] = subset_test\n",
        "        predictions['shots'] = num_examples\n",
        "\n",
        "        # save new combinations in file\n",
        "        print(\"Dont interrupt, saving predictions...\")\n",
        "        ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "        # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "        try:\n",
        "            predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "            previous_predictions = predictions\n",
        "        except Exception as e:\n",
        "            # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "            previous_predictions = predictions\n",
        "\n",
        "        # save results in overview file\n",
        "        date = ph.get_datetime()\n",
        "        y_test = predictions['label']\n",
        "        y_pred = predictions['prediction']\n",
        "        report = classification_report(y_test, y_pred)\n",
        "\n",
        "        overview = pd.DataFrame(\n",
        "            [{\n",
        "                'model':model_name,\n",
        "                'run_id':run_id,\n",
        "                'date': date,\n",
        "                'train_set': subset_train,\n",
        "                'test_set': subset_test,\n",
        "                'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "                'test_set_support':len(predictions),\n",
        "                'split_col':split_col,\n",
        "                'text_col':df.iloc[0]['trunc_col'],\n",
        "                'runtime':sum(predictions['runtime']),\n",
        "                'accuracy': accuracy_score(y_test, y_pred),\n",
        "                'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "                'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "                'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "                'classification_report':report\n",
        "            }   ]\n",
        "        )\n",
        "        # remove previous results of run_id, replace with new/updated results\n",
        "        ph.replace_and_save_df(overview, overview_path, run_id)\n",
        "        print(\"Saving done! Interrupting is allowed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up variables that are the same for each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "TEXT_COLUMN = 'trunc_txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GEITje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - In-context learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_geitje = pipeline(task='conversational', model='FemkeBakker/GEITjeSmallData200Tokens',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'GEITjeSmallData200Tokens'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictions/trialfewShotGeitjepredictions.pkl\"\n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/overview/trialfewShotGeitjepredictions.pkl\"\n",
        "\n",
        "\n",
        "PROMPT = pt.geitje_simple_prompt\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT == pt.simple_prompt or PROMPT==pt.simple_prompt_v2 or PROMPT==pt.OldSimple_prompt or PROMPT==pt.geitje_simple_prompt:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "\n",
        "    \n",
        "OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/GEITje/{PROMPT_NAME}/overview.pkl\"\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/GEITje/{PROMPT_NAME}/predictions.pkl\"\n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "print (run_id)\n",
        "\n",
        "# small = txt.iloc[16:22]\n",
        "# small['4split']=['val', 'dev', 'dev', 'dev', 'dev', 'dev']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- EXPERIMENT --------\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_geitje, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "# pred_run = pred.loc[pred['run_id']==f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}']\n",
        "display(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(OVERVIEW_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Llama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - In Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "953174ab612e47eabb53102d417461b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f96b7049b2684d748f4332ca18a2ac49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='meta-llama/Llama-2-7b-chat-hf',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "# load llama using cpu, else will give cuda out of memory error when running fewshot bm25 prompt.\n",
        "\n",
        "MODEL_NAME = 'Llama-2-7b-chat-hf'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_llama = pipeline(task='conversational', model='FemkeBakker/LlamaSmallData200Tokens',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'LlamaSmallData200Tokens'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IC_Llama-2-7b-chat-hfsimple_prompt_v2LlamaTokens200_0devval_numEx0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "PROMPT = pt.simple_prompt_v2\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT == pt.simple_prompt or PROMPT==pt.simple_prompt_v2 or PROMPT==pt.OldSimple_prompt:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "    \n",
        "OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/Llama/{PROMPT_NAME}/overview.pkl\"\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/Llama/{PROMPT_NAME}/predictions.pkl\"\n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "print(run_id)\n",
        "# small = txt.iloc[16:22]\n",
        "# small['4split']=['val', 'dev', 'dev', 'dev', 'dev', 'dev']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run-id already known, resuming predictions...\n",
            "Starting...0:10 out of 129\n",
            "label:  onderzoeksrapport\n",
            "response:   Based on the content of the document provided, I would classify it as a 'Raadsnotulen' (Dutch for 'Agenda').\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsnotulen\"\n",
            "}\n",
            "\n",
            "The document appears to be a meeting agenda for a council or committee, with items related to a quick scan of emissions of chemical substances. The document includes an introduction, scope of the quick scan, inventory, and conclusions, which are all typical elements of a meeting agenda. Therefore, I have classified it as a 'Raadsnotulen'.\n",
            "prediction: raadsnotulen\n",
            "label:  raadsnotulen\n",
            "response:   Sure, I can classify the document for you. Based on the content provided, the document belongs to the category \"Raadsnotulen\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsnotulen\"\n",
            "}\n",
            "prediction: raadsnotulen\n",
            "label:  voordracht\n",
            "response:   Based on the provided document title and content, I would classify it under the category of \"Voordracht\" or \"Presentation\".\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "prediction: voordracht\n",
            "label:  agenda\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Wetenschappelijke Raad voor het Regeringsbeleid (wRR)\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Wetenschappelijke Raad voor het Regeringsbeleid (wRR)\"\n",
            "}\n",
            "prediction: NoPredictionInOutput\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Motie' (Motion) with the following output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The document is a motion presented by three city councilors to the Gemeenteraad (City Council) of Amsterdam, regarding an evaluation of a note titled \"Ruimte voor Woningdelers\" (Space for Homeowners) after two years. The motion is addressed to the city council and includes the names of the councilors who have signed it. Therefore, it falls under the category of a 'Motie' in the Gemeenteblad (City Council Agenda) of Amsterdam.\n",
            "prediction: motie\n",
            "label:  besluit\n",
            "response:   Sure, I can classify the document for you. Based on the information provided, the document belongs to the category: 'Besluit'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "\n",
            "The reason for this classification is that the document contains a formal decision or resolution made by the Bestuurscommissie Oost of the Gemeente Amsterdam, which is a type of official document that falls under the category of 'Besluit'.\n",
            "prediction: besluit\n",
            "label:  onderzoeksrapport\n",
            "response:   Based on the provided information, I would classify the document \"WPI KWARTAAL FACTSHEET DECEMBER 2022\" as a \"Besluit\".\n",
            "\n",
            "The document appears to be a summary of key figures related to the directives of Work, Participation, and Income, which is a decision or policy document. The document provides information on the development of these directives over time, and the cijfers are presented in a JSON file.\n",
            "\n",
            "Therefore, the category for this document is \"Besluit\".\n",
            "prediction: NoPredictionFormat\n",
            "label:  raadsadres\n",
            "response:   Based on the content of the document, I would classify it as a 'Besluit'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "prediction: besluit\n",
            "label:  brief\n",
            "response:   Based on the content of the document you provided, I would classify it as a \"Besluit\" (Decision).\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "\n",
            "The reason for this classification is that the document appears to be a formal decision made by the Gemeente Raad van Amsterdam, with a clear subject matter (the handling of Motie 576), a list of parties involved (including the wethouder responsible for the portfolio), and a specific outcome or recommendation (the afdoening of the motion). All of these elements are typical of a \"Besluit\" document.\n",
            "prediction: besluit\n",
            "label:  actualiteit\n",
            "response:   Based on the content of the document, I would classify it as a \"Raadsagenda\" (category: ['Raadsagenda']).\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsagenda\"\n",
            "}\n",
            "prediction: agenda\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...10:20 out of 129\n",
            "label:  brief\n",
            "response:   Based on the content of the document, I would categorize it as a \"Besluit\" (Decision).\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "prediction: besluit\n",
            "label:  raadsadres\n",
            "response:   Based on the content of the email, I would classify it under the category of \"Brief\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Brief\"\n",
            "}\n",
            "prediction: brief\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the provided information, I would classify the document as follows:\n",
            "\n",
            "{'categorie': 'Schriftelijke Vraag'}\n",
            "\n",
            "This document is a written question submitted to the Gemeenteraad of Amsterdam, and it pertains to the topic of Formula-1 teams riding through nature reserves. Therefore, it falls under the category of 'Schriftelijke Vraag' or 'Written Question'.\n",
            "prediction: schriftelijke vraag\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the content of the document provided, I would classify it as a 'Schriftelijke Vraag'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Schriftelijke Vraag\"\n",
            "}\n",
            "\n",
            "The document is a written question addressed to the Gemeente Amsterdam, in which the author asks for additional information in response to the councilor's written questions from November 26, 2013, regarding the handling of objections to the valuation of real estate according to the Wet waardering onroerende zaken (WOZ-bezwaarschriften).\n",
            "prediction: schriftelijke vraag\n",
            "label:  voordracht\n",
            "response:   Based on the provided document title and content, I would categorize it as a \"Voordracht\" (Dutch for \"Motion\") with a category of \"Agenda punt 7\" and a subcategory of \"Ongevraagd advies stadsdeelcommissie Oost - Parkeren Java-eiland en tekstvoorstel motie\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "\"subcategorie\": \"Agenda punt 7\"\n",
            "\"subsubcategorie\": \"Ongevraagd advies stadsdeelcommissie Oost - Parkeren Java-eiland en tekstvoorstel motie\"\n",
            "}\n",
            "prediction: MultiplePredictionErrorInOutput\n",
            "label:  raadsadres\n",
            "response:   Sure, I can classify the document for you. Based on the content provided, the document can be classified under the category of \"Raadsadres\" (Dutch for \"Council Address\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsadres\"\n",
            "}\n",
            "prediction: raadsadres\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the provided document, I would classify it as a \"Schriftelijke Vraag\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Schriftelijke Vraag\"\n",
            "}\n",
            "prediction: schriftelijke vraag\n",
            "label:  agenda\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Besluit'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "\n",
            "The document appears to be a meeting agenda for a stadsdeelcommissie in Amsterdam, with the main topics discussed including the opening and establishment of the agenda, as well as the consideration and adoption of decisions and resolutions related to the stadsdeel. Based on the content of the document, it seems to be a formal document that outlines the actions and decisions taken during a meeting, which is consistent with the definition of a 'Besluit'.\n",
            "prediction: besluit\n",
            "label:  actualiteit\n",
            "response:   Based on the content of the document you provided, I would classify it as a 'Brief'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Brief\"\n",
            "}\n",
            "prediction: brief\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Motie' (Motion) with the following output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The document is a motion presented by the members Groen and Van den Berg to the Gemeenteraad of Amsterdam regarding the subsidy regulation for aardgasloos II. The motion is discussed and voted upon by the Gemeenteraad. Therefore, it falls under the category of 'Motie' in the provided list of categories.\n",
            "prediction: motie\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...20:30 out of 129\n",
            "label:  brief\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Voordracht\" or \"Besluit\".\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "\n",
            "The document is a briefing for the members of the Amsterdam city council on a motion (motie 105) adopted in a previous council meeting. The briefing provides information on the motion and its implications, and is therefore a type of voordracht or presentation to the council.\n",
            "prediction: voordracht\n",
            "label:  actualiteit\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Raadsagenda supplement\".\n",
            "prediction: NoPredictionFormat\n",
            "label:  onderzoeksrapport\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Agenda\".\n",
            "\n",
            "Here's the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Agenda\"\n",
            "}\n",
            "prediction: agenda\n",
            "label:  voordracht\n",
            "response:   Based on the provided document title, I would classify it as a \"Voordracht\" (Dutch for \"Speech\"). The document is titled \"VN2023-014776 - Eet '...'. Ruimte en > 4 Gemeente Raadscommissie voor Ruimtelijke Ordening en Grond en Ontwikkeling RO\", which suggests that it is a speech or presentation given to a committee on spatial planning and development. Therefore, the category for this document is:\n",
            "\n",
            "{ \"categorie\": \"Voordracht\" }\n",
            "prediction: voordracht\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the content of the document, I would classify it as a 'Schriftelijke Vraag' (Written Question) as it is a question submitted to the Gemeente Amsterdam by a member of the council, Naoum Néhmé, Marttin and De Grave-Verkerk, regarding the Amsterdamse colleges in cultural institutions.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Schriftelijke Vraag\"\n",
            "}\n",
            "prediction: schriftelijke vraag\n",
            "label:  actualiteit\n",
            "response:   Sure, I can classify the document for you. Based on the content provided, I would categorize the document as follows:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "\n",
            "The document appears to be a statement or a speech given by a political party (D66) regarding the recent incident of violence in a youth football match in Nieuw-West, Amsterdam. The document expresses concern and raises questions about the actions that the city council should take to address the issue. Therefore, it falls under the category of a \"Voordracht\" or a speech.\n",
            "prediction: voordracht\n",
            "label:  raadsadres\n",
            "response:   Sure, I can classify the document you provided into one of the categories. Based on the content of the document, I would categorize it as a \"Brief\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Brief\"\n",
            "}\n",
            "\n",
            "The reason for this categorization is that the document appears to be a formal communication from the City of Amsterdam to an unknown recipient, containing information about the disappearance of a council address. The document includes the sender's and recipient's contact information, as well as a reference number and a date. It also includes a brief explanation of the issue being addressed and how it was handled. All of these elements are typical of a brief document used for official communication within an organization or government agency.\n",
            "prediction: brief\n",
            "label:  raadsadres\n",
            "response:   Based on the content of the document, I would classify it as a 'Voordracht' (Dutch for 'Speech'). The document appears to be a proposal or suggestion for a policy or regulation related to the use of hybrid vehicles in urban areas, and it contains a call to action for further discussion or consideration. Therefore, the category 'Voordracht' is the most appropriate.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "prediction: voordracht\n",
            "label:  agenda\n",
            "response:   Based on the content of the document provided, I would classify it as a 'Agenda' document.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Agenda\"\n",
            "}\n",
            "prediction: agenda\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Schriftelijke Vraag' (Written Question).\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Schriftelijke Vraag\"\n",
            "}\n",
            "prediction: schriftelijke vraag\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...30:40 out of 129\n",
            "label:  agenda\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Agenda\".\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Agenda\"\n",
            "}\n",
            "prediction: agenda\n",
            "label:  voordracht\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Agenda\" with a score of 95%. The document contains several agenda points related to the portfolio of Zorg en Maatschappelijke Ontwikkeling, including Opvang (MO/BW, Ongedocumenteerden en Vluchtelingen). The document also mentions the date of the decision (7 November 2023) and the topic of the agenda point (Omgang wachtlijsten Wmo).\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Agenda\"\n",
            "}\n",
            "prediction: agenda\n",
            "label:  motie\n",
            "response:   Sure! Based on the content of the document provided, I would classify it as a \"Motie\" (Dutch for \"Motion\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the content of the document provided, I would categorize it as a 'Motie' (Motion) document.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "prediction: motie\n",
            "label:  raadsnotulen\n",
            "response:   Based on the content of the document provided, I would classify it as a \"Raadsnotulen\" document.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsnotulen\"\n",
            "}\n",
            "prediction: raadsnotulen\n",
            "label:  motie\n",
            "response:   Based on the content of the document provided, I would classify it under the category of \"Motie\" (Motion).\n",
            "\n",
            "Here is the output in the form of a JSON file:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "prediction: motie\n",
            "label:  agenda\n",
            "response:   Based on the information provided in the document, I would classify it as a \"Agenda\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Agenda\"\n",
            "}\n",
            "prediction: agenda\n",
            "label:  raadsadres\n",
            "response:   Based on the content of the document you provided, I would categorize it as a 'RAADSADRES'.\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"RAADSADRES\"\n",
            "}\n",
            "\n",
            "Explanation:\n",
            "\n",
            "A Raadsadres is a formal letter or memo addressed to the city council or other government bodies, typically used for official correspondence and communication. The document you provided meets this criteria, as it is an official letter addressed to the city council and other government bodies, regarding a specific topic or issue.\n",
            "prediction: raadsadres\n",
            "label:  actualiteit\n",
            "response:   Sure, I can classify the document for you. Based on the information provided in the input, the document can be classified as a 'Termijnagenda' with the following details:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Termijnagenda\"\n",
            "}\n",
            "\n",
            "The reason for this classification is that the document is a term agenda for a council meeting, with details of the items to be discussed and the organizational structure of the meeting. The document includes information on the date, time, location, and agenda items, as well as the name of the council member responsible for each item.\n",
            "prediction: agenda\n",
            "label:  onderzoeksrapport\n",
            "response:   Based on the content of the document you provided, I would classify it as a 'Besluit' (Decision).\n",
            "\n",
            "Here's why:\n",
            "\n",
            "* The document is titled \"Programmaplan Gezondheidspreventief Drugsbeleid\" which indicates that it outlines a plan or decision related to drug prevention and health policy.\n",
            "* The document begins with an introduction that sets the context for the plan, including the prevalence of drug use in Amsterdam and the goals of the plan.\n",
            "* The document then provides data and statistics on drug use in Amsterdam, including the number of drug-related deaths and the impact of drug use on public health.\n",
            "* The document outlines the goals and objectives of the plan, including the prevention of drug use and the reduction of drug-related harm.\n",
            "* The document also includes an action plan with specific measures to achieve the goals of the plan.\n",
            "\n",
            "Overall, the document appears to be a decision or plan related to drug prevention and health policy, rather than a speech, question, written request, brief, advice, or agenda. Therefore, the category 'Besluit' (Decision) is the most appropriate.\n",
            "\n",
            "prediction: NoPredictionFormat\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...40:50 out of 129\n",
            "label:  raadsnotulen\n",
            "response:   Based on the information provided in the text, I would classify the document as:\n",
            "\n",
            "{'categorie': 'Raadsnotulen'}\n",
            "\n",
            "The document is a meeting minutes of the Gemeenteraad (City Council) of Amsterdam, specifically the meeting held on May 29, 2019. It includes the agenda, reports, and decisions made during the meeting, as well as the names of the council members present and the chairperson. Therefore, it falls under the category of 'Raadsnotulen' (City Council Minutes).\n",
            "prediction: raadsnotulen\n",
            "label:  motie\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Motie\" (Motion).\n",
            "prediction: NoPredictionFormat\n",
            "label:  voordracht\n",
            "response:   Based on the content of the document VN2022-042748, I would classify it under the category of \"Voordracht\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "prediction: voordracht\n",
            "label:  voordracht\n",
            "response:   Based on the provided document title and content, I would categorize it as a \"Voordracht\" (Dutch for \"Speech\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "prediction: voordracht\n",
            "label:  agenda\n",
            "response:   Based on the content of the document you provided, I would classify it under the category of \"Besluitvormend AB\" (Decision-Making Agenda).\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluitvormend AB\"\n",
            "}\n",
            "prediction: besluit\n",
            "label:  besluit\n",
            "response:   Sure, I can classify the document for you. Based on the information provided, I would classify the document as:\n",
            "\n",
            " {'categorie': 'Besluit'}\n",
            "\n",
            "This is because the document appears to be a municipal decision or resolution, specifically the 'Besluit Algemeen Bestuur' (Municipal Ordinance) of the City of Amsterdam, Stadsdeel West, regarding the 'Terinzagelegging ontwerpbestemmingsplan Taxiopstelstrook Leidsebosje' (Examination of the Draft Zoning Plan Taxiopstelstrook Leidsebosje). The document includes information such as the date of handling, the behandeling ambtenaar (handling officer), and the besluitnummer (decision number), which are all consistent with the format of a municipal decision or resolution in the Netherlands.\n",
            "prediction: besluit\n",
            "label:  voordracht\n",
            "response:   Based on the content of the document VN2023-013120, I would classify it under the category of \"Voordracht\" (Dutch for \"Motion\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "prediction: voordracht\n",
            "label:  onderzoeksrapport\n",
            "response:   Based on the content of the document, I would classify it as a \"Raadsnotulen\" (Dutch for \"Council Agenda\").\n",
            "\n",
            "Here's the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsnotulen\"\n",
            "}\n",
            "prediction: raadsnotulen\n",
            "label:  agenda\n",
            "response:   Based on the content of the document provided, I would classify it as a 'Agenda'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Agenda\"\n",
            "}\n",
            "prediction: agenda\n",
            "label:  voordracht\n",
            "response:   Based on the content of the document, I would classify it as a \"Voordracht\" (Dutch for \"Speech\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "\n",
            "The document is a speech given to the Commissie WB (Housing and Environment Committee) of the Amsterdam City Council on March 24, 2021, for the agenda point 10 of the committee. The speech is related to the topic of housing and urban development in Amsterdam, specifically the Zuidas and Marineterrein area, and the Ground Matters (Grondzaken) department. Therefore, it falls under the category of \"Voordracht\" or speech.\n",
            "prediction: voordracht\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...50:60 out of 129\n",
            "label:  agenda\n",
            "response:   Based on the provided document, I would classify it under the category of \"Agenda\" or \"Meeting Agenda\".\n",
            "\n",
            "Here's the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Agenda\"\n",
            "}\n",
            "\n",
            "The document is an agenda for a meeting of the Raadscommissie Welzijn en Onderwijs in 2013, which includes several items related to the council's responsibilities, such as:\n",
            "\n",
            "* Terms of reference for the committee\n",
            "* Time and place of the meeting\n",
            "* Items to be discussed and decided upon\n",
            "* Names of attendees and their roles\n",
            "* Responsibilities of the attendees\n",
            "\n",
            "Therefore, the document can be classified as an agenda for a meeting, which is a common type of document in the category \"Agenda\".\n",
            "prediction: agenda\n",
            "label:  voordracht\n",
            "response:   Based on the content of the document you provided, I would classify it as a \"Voordracht\" (Dutch for \"Presentation\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "\n",
            "The document is a presentation for the Commissie WV (Housing and Monument Preservation Committee) of the Amsterdam City Council, and it contains information and analysis related to the Amsterdamse Aanpak Volkshuisvesting (AAV) policy. Therefore, it falls under the category of a Voordracht, which is a type of document that presents information or proposals to a committee or council.\n",
            "prediction: voordracht\n",
            "label:  raadsadres\n",
            "response:   Based on the content of the document, I would classify it under the category:\n",
            "\n",
            "1.1 Formulier raadsadres\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"1.1 Formulier raadsadres\"\n",
            "}\n",
            "prediction: raadsadres\n",
            "label:  brief\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Besluit\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "prediction: besluit\n",
            "label:  besluit\n",
            "response:   Sure, I can classify the document you provided into one of the categories. Based on the content of the document, I would classify it as a 'Besluit' (Decision).\n",
            "\n",
            "Here is the output in the form of a JSON file:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "\n",
            "The reason for this classification is that the document contains a formal decision made by the Bestuurscommissie (Executive Committee) of the stadsdeel Nieuw-West in Amsterdam, regarding the OLO2024613 project at Maroastraat 39, Meet Inn. The document includes the decision made by the committee, as well as the date and number of the decision.\n",
            "prediction: besluit\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Motie' (Motion) with the following details:\n",
            "\n",
            "{\n",
            "'categorie': 'Motie'\n",
            "}\n",
            "\n",
            "The document is a motion presented by the members Kaya, Duijndam, Roosma, and Poorter to the Gemeenteraad of Amsterdam regarding the winter shelter provision for the homeless during the winter season. The motion is dated November 13, 2015, and was received by the Gemeenteraad on November 5, 2015, for discussion.\n",
            "prediction: motie\n",
            "label:  onderzoeksrapport\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Raadsnotulen\".\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsnotulen\"\n",
            "}\n",
            "prediction: raadsnotulen\n",
            "label:  onderzoeksrapport\n",
            "response:   Based on the content of the document, I would classify it as a 'Besluit' (Decision).\n",
            "\n",
            "Here is the output in the form of a JSON file:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "prediction: besluit\n",
            "label:  raadsadres\n",
            "response:   Based on the content of the document you provided, I would classify it as a 'Besluit' (Decision).\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "prediction: besluit\n",
            "label:  actualiteit\n",
            "response:   Based on the provided text, I would classify the document as:\n",
            "\n",
            " {'categorie': 'Actualiteit'}\n",
            "\n",
            "The document is an update on the latest news and developments in the field of finance, economics, and environmental issues, which falls under the category of 'Actualiteit' (or 'Current Affairs' in English).\n",
            "prediction: actualiteit\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...60:70 out of 129\n",
            "label:  factsheet\n",
            "response:   Based on the content of the document you provided, I would classify it under the category of \"Factsheet\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Factsheet\"\n",
            "}\n",
            "\n",
            "Explanation:\n",
            "\n",
            "A factsheet is a document that provides concise and accurate information on a specific topic. In this case, the document provides information on Global Handwashing Day, including the purpose, theme, and importance of hand hygiene. The document is intended to educate and inform readers about the topic, rather than persuade or make a decision. Therefore, it fits under the category of factsheet.\n",
            "prediction: factsheet\n",
            "label:  brief\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Raadsnotulen\" or \"Agenda\".\n",
            "\n",
            "The document appears to be a briefing note for the members of the municipal council of Amsterdam, providing them with an update on the implementation of the Environmental Act, which is scheduled to come into effect on January 1, 2024. The document includes information on the introduction of the act, the main changes it will bring, and the responsibilities of the city council and other parties involved.\n",
            "\n",
            "Therefore, the most appropriate category for this document would be \"Raadsnotulen\", as it is a formal document prepared by the city council for its members to inform them about an important issue related to the council's work.\n",
            "prediction: NoPredictionFormat\n",
            "label:  raadsadres\n",
            "response:   Based on the information provided in the document, the category that best fits it is \"Raadsadres\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsadres\"\n",
            "}\n",
            "\n",
            "Explanation:\n",
            "\n",
            "A Raadsadres is a formal letter or report written by a local government agency, such as a city council or municipality, to inform the public about a specific issue or matter. The document provided in the question meets this criteria as it is a formal letter addressed to the inhabitants of a particular area, informing them about the extreme discomfort caused by the heat and the resulting problems, such as public drunkenness, noise, and littering. The letter is also signed by a named official, which further supports the classification of the document as a Raadsadres.\n",
            "prediction: raadsadres\n",
            "label:  besluit\n",
            "response:   Sure, I can classify the document for you. Based on the content provided, the document appears to be a 'Besluit' (Decision) from the Amsterdam city council, specifically from the General Administration department. Therefore, the category for this document is:\n",
            "\n",
            "{'categorie': 'Besluit'}\n",
            "prediction: besluit\n",
            "label:  onderzoeksrapport\n",
            "response:   Based on the content of the document you provided, I would classify it as a \"Raadsnotulen\" (Dutch for \"Council Agenda\").\n",
            "\n",
            "Here's the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsnotulen\"\n",
            "}\n",
            "\n",
            "The document appears to be an agenda for a council meeting in Amsterdam, with items related to the city's approach to social housing, status holders and integration, and a yearly report for 2020. The document includes an outline of the meeting's content, with sections on introduction, background information, policy frameworks, important developments, activities, and more.\n",
            "prediction: raadsnotulen\n",
            "label:  actualiteit\n",
            "response:   Based on the provided information, the document can be classified into the category of \"Actualiteit\" with a percentage of 75% (Gemeente Amsterdam).\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Actualiteit\"\n",
            "}\n",
            "prediction: actualiteit\n",
            "label:  actualiteit\n",
            "response:   Based on the content of the document, I would classify it as:\n",
            "\n",
            "{'categorie': 'Raadsactualiteit'}\n",
            "\n",
            "This document appears to be a report from the Gemeente Amsterdam Rathaus, discussing the actuality of roving asylum seekers in the city's inner city. The document is titled \"Actualiteit van de leden Poot, Nanninga en Boomsma inzake rovende asielzoekers in de binnenstad\" and is dated November 4, 2019. It is addressed to the Gemeenteraad and includes information about the number of criminal asylum seekers in the inner city, as reported by the Telegraaf newspaper. The document appears to be a formal communication from the municipality to the council, providing information on a current issue and seeking their attention to the matter. Therefore, I would categorize it as 'Raadsactualiteit' or 'Council Agenda'.\n",
            "prediction: actualiteit\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the provided text, I would classify the document as a 'Schriftelijke Vraag' (Written Question).\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Schriftelijke Vraag\"\n",
            "}\n",
            "\n",
            "Explanation:\n",
            "The document is a written question addressed to the College of Burgemeester and Wethouders of Amsterdam, asking for clarification on the wapenbezit (weapon ownership) of Amsterdamse jongeren (Amsterdam youth). The document includes the questions asked by the member Taimounti and the answers provided by the college. Therefore, it falls under the category of a written question.\n",
            "prediction: schriftelijke vraag\n",
            "label:  onderzoeksrapport\n",
            "response:   Sure, I can help you classify the document and provide the output in JSON format.\n",
            "\n",
            "Based on the content of the document, I would categorize it as a 'Besluit' (Decision). The document appears to be a report evaluating the process of subsidy regulation for diversity and inclusion in alliances in Amsterdam, and it includes experiences and observations from various stakeholders involved in the process.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "\n",
            "I hope this helps! Let me know if you have any further questions.\n",
            "prediction: besluit\n",
            "label:  motie\n",
            "response:   Based on the content of the document provided, I would classify it as a 'Motie' (Motion) document.\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "prediction: motie\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...70:80 out of 129\n",
            "label:  raadsadres\n",
            "response:   Based on the content of the document, I would classify it as a 'Brief'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Brief\"\n",
            "}\n",
            "prediction: brief\n",
            "label:  brief\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Raadsnotulen\".\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsnotulen\"\n",
            "}\n",
            "prediction: raadsnotulen\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the provided information, I would classify the document as:\n",
            "\n",
            "{'categorie': 'Schriftelijke Vraag'}\n",
            "\n",
            "This document appears to be a response to two written questions submitted by council members J.M. Paternotte and G.J. Bouwmeester on February 7, 2013, regarding the Netherlands-Russia year in relation to Russia's anti-homosexuality law. The document provides a detailed answer to the questions and includes an introduction from the council members. Therefore, it falls under the category of 'Schriftelijke Vraag' or 'Written Question'.\n",
            "prediction: schriftelijke vraag\n",
            "label:  raadsadres\n",
            "response:   Based on the content of the document you provided, I would classify it as a 'Schriftelijke Vraag'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Schriftelijke Vraag\"\n",
            "}\n",
            "\n",
            "The reason for this classification is that the document is a written question addressed to the Mayor of Amsterdam, Mr. E. van der Laan, and the College of B&W, regarding several aspects of the problematic surrounding the Names monument. The document is a formal written inquiry, rather than a speech or decision, and does not contain any of the other categories you provided.\n",
            "prediction: schriftelijke vraag\n",
            "label:  raadsadres\n",
            "response:   Based on the information provided, I would classify the document as a \"Besluit\" with the following output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "\n",
            "The document contains elements that are typical of a decision-making document, such as the address of the municipality, the date, the signature of the author, and the reference number. The content of the document appears to be related to a proposal or recommendation for traffic measures, which is a common topic in municipal decision-making.\n",
            "prediction: besluit\n",
            "label:  voordracht\n",
            "response:   Based on the content of the document VN2021-000427, I would classify it under the category of \"Voordracht\" (Dutch for \"Presentation\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "prediction: voordracht\n",
            "label:  voordracht\n",
            "response:   Based on the content of the document VN2023-015548, I would classify it under the category of \"Voordracht\" (Dutch for \"Motivation\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "\n",
            "The document is a motivation for the Commissie DC of the Amsterdam City Council, specifically for the agenda point 7 on the topic of opportunities related to WKO-concessions. The document provides information and asks the commissie to take note of the possibilities offered by WKO-concessions.\n",
            "prediction: voordracht\n",
            "label:  raadsadres\n",
            "response:   Sure, I can classify the document for you. Based on the content of the email, the category that best fits it is:\n",
            "\n",
            "{'categorie': 'Klacht'}\n",
            "\n",
            "This category is used to classify documents that contain a complaint or a request for redress, such as the one in the email you provided.\n",
            "prediction: NoPredictionInOutput\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the content of the document, I would classify it as a 'Schriftelijke Vraag'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Schriftelijke Vraag\"\n",
            "}\n",
            "prediction: schriftelijke vraag\n",
            "label:  motie\n",
            "response:   Based on the content of the document provided, I would classify it under the category of \"Motie\" (Dutch for \"Motion\").\n",
            "\n",
            "Therefore, the output in JSON format would be:\n",
            "\n",
            "{\n",
            "\"categorie\": \"D Motie\"\n",
            "}\n",
            "prediction: motie\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...80:90 out of 129\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Motie' (Motion) with the following JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The document is a motion presented by a council member regarding the budget for 2015, which is a typical category for motions in local government.\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the content of the document provided, I would classify it as a \"Motie\" (Motion) document.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the provided information, I would classify the document as a 'Motie' (Motion) with the following output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "Explanation:\n",
            "\n",
            "* The document is a motion, as it is a proposal put forward by members of the Gemeenteraad (City Council) for consideration and vote by the council.\n",
            "* The document is titled \"Motie van de leden Flentge en Paternotte inzake de ruimtebehoefte voor internationaal geaccrediteerd onderwijs Amsterdam (gedeelde voorzieningen)\" which translates to \"Motion of the members Flentge and Paternotte regarding the space need for internationally accredited education Amsterdam (shared facilities)\".\n",
            "* The document is dated January 6, 2017, and was approved by the Gemeenteraad on December 22, 2016.\n",
            "* The document is classified as a 'Motie' because it is a proposal put forward by members of the Gemeenteraad for consideration and vote by the council, rather than a formal\n",
            "prediction: motie\n",
            "label:  voordracht\n",
            "response:   Based on the provided document, I would categorize it as a \"Voordracht\" (Dutch for \"Speech\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "\n",
            "The document is a speech given to the Gemeente Raadscommissie voor Algemene Zaken, specifically the Openbare Orde en Veiligheid (Public Order and Safety) committee, on January 13, 2022. The speech is related to the agenda point 6 of the committee and provides information on the conclusion of measures related to public order and safety.\n",
            "prediction: voordracht\n",
            "label:  motie\n",
            "response:   Based on the content of the document, I would classify it as a 'Besluit'.\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "prediction: besluit\n",
            "label:  voordracht\n",
            "response:   Based on the content of the document, I would classify it as a \"Voordracht\" (Dutch for \"Speech\") with the following JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "\n",
            "The document is a speech given to the Commissie OZA (Commission on Social Affairs) of the Amsterdam City Council on December 13, 2023, for advisory purposes. It discusses the portfolio of Zorg en Maatschappelijke Ontwikkeling (Care and Social Development) and specifically focuses on the topic of opvang (accommodation) for different groups, including undocumented individuals and refugees.\n",
            "prediction: voordracht\n",
            "label:  agenda\n",
            "response:   Based on the provided document, the category that best fits it is:\n",
            "\n",
            "{'categorie': 'Agenda'}\n",
            "\n",
            "This document is an agenda for a public meeting of the Raadscommissie voor Bouwen, Wonen, Wijkaanpak en Dierenwelzijn in Amsterdam, held on October 10, 2017. The document outlines the schedule, location, and topics to be discussed during the meeting, including the opening procedure, meditations, and open discussion. Therefore, it belongs to the category of \"Agenda\".\n",
            "prediction: agenda\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Motie' (Motion) with the following details:\n",
            "\n",
            "{'categorie': 'Motie'}\n",
            "\n",
            "Here's why:\n",
            "\n",
            "* The document is addressed to the Gemeenteraad (City Council) of Amsterdam.\n",
            "* The document is a motion (Motie) presented by a member of the council, Van Lammeren, regarding the city's noise policy for events and location profiles (gevelnorm 70 dB(A)).\n",
            "* The motion is discussed and voted on by the council.\n",
            "* The document includes the date of presentation (21 February 2018), the date of consideration (14 February 2018), and the status of the motion (verworpen, or rejected).\n",
            "\n",
            "Therefore, the document can be classified as a 'Motie' under the category 'Gemeenteraad' and 'Jaar 2018'.\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the provided document, the category of the document is:\n",
            "\n",
            "{'categorie': 'Voordracht'}\n",
            "\n",
            "This document is a motion (Motie) presented to the Gemeente Amsterdam Gemeenteraad in 2013, proposing a new administrative system for the city starting from 2014, replacing the daily boards of the various committees.\n",
            "prediction: voordracht\n",
            "label:  motie\n",
            "response:   Based on the content of the document you provided, I would classify it as a 'Motie' (Motion) with the following JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The document is a motion presented by several members of the Gemeenteraad (City Council) of Amsterdam, regarding the Housing Ordinance 2020 and its implications for tenants and landlords. The motion is addressed to the Gemeenteraad and contains information about the new regulations for housing. Therefore, it falls under the category of 'Motie' in the list of categories you provided.\n",
            "prediction: motie\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...90:100 out of 129\n",
            "label:  motie\n",
            "response:   Based on the content of the document, the category that best fits it is:\n",
            "\n",
            "{'categorie': 'Voordracht'}\n",
            "\n",
            "This document appears to be a formal proposal or motion put forward by a member of the Gemeente Amsterdam council, Yilmaz, regarding the allocation of 25 million euros from the SMF (Stadsmiddle Fonds) for future unforeseen expenses. The document outlines the reasons for the proposal, the discussion held during the council meeting, and the decision made by the council. Therefore, it can be classified as a 'Voordracht' or motion, as it is a formal proposal put forward for consideration and debate by the council.\n",
            "prediction: voordracht\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Schriftelijke Vraag' (Written Question) with the following output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Schriftelijke Vraag\"\n",
            "}\n",
            "\n",
            "This document is a written question submitted to the Gemeente Amsterdam by a council member, El Ksaihi, regarding the supervision of the toeslagenaffaire (child benefit affair) and the treatment of affected parents by the Tax and Customs Administration. The document includes the question, the date of submission, and the answer provided by the municipality.\n",
            "prediction: schriftelijke vraag\n",
            "label:  motie\n",
            "response:   Based on the content of the document provided, I would classify it as a 'Motie' (Motion) under the category 'Gemeente Amsterdam'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Sure! Based on the content of the document provided, I would classify it as a 'Besluit'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "\n",
            "The reason for this classification is that the document appears to be a formal decision or resolution made by the Gemeente Amsterdam, outlining a specific proposal or change related to the local education policy in Amsterdam. The document includes information such as the date of publication, the number of the proposal, the subject matter, and the status of the proposal (in this case, \"Verworpen\" or \"Rejected\"). All of these elements suggest that the document is a formal, official document that would be classified as a 'Besluit' in the context of the Gemeente Amsterdam.\n",
            "prediction: besluit\n",
            "label:  motie\n",
            "response:   Based on the provided document, the category that best fits it is:\n",
            "\n",
            "{'categorie': 'Motie'}\n",
            "\n",
            "This document is a motion submitted by a council member, Mevrouw Moorman, regarding the criticism of the Broad Administrative Service (BBO) on the education plans of the coalition.\n",
            "prediction: motie\n",
            "label:  voordracht\n",
            "response:   Based on the provided document, I would classify it as a 'Voordracht' (Dutch for 'Speech').\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "\n",
            "The document is a speech given to the Gemeente Raadscommissie voor Publieke Gezondheid en Preventie, Zorg en OZA, and it is related to the topic of armoedebestrijding and Schuldhulpverlening. Therefore, it falls under the category of 'Voordracht' in the provided list of categories.\n",
            "prediction: voordracht\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the information provided in the document, I would classify it under the category of \"Schriftelijke Vragen\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Schriftelijke Vragen\"\n",
            "}\n",
            "\n",
            "Explanation:\n",
            "The document is a written question submitted to the Gemeenteraad of Amsterdam, asking for information about the relocation of rabbits from the Domela Nieuwenhuisplantsoen. The document contains the question, the answer provided by the municipality, and the dates related to the submission and consideration of the question. Based on this information, the document can be classified as a written question, which is a type of correspondence that is commonly used in local governance to seek information or clarification from the municipality.\n",
            "prediction: NoPredictionInOutput\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Schriftelijke Vraag' (Written Question).\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Schriftelijke Vraag\"\n",
            "}\n",
            "prediction: schriftelijke vraag\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Schriftelijke Vraag\" (Written Question).\n",
            "prediction: NoPredictionFormat\n",
            "label:  motie\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Voordracht\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "prediction: voordracht\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...100:110 out of 129\n",
            "label:  motie\n",
            "response:   Based on the content of the document provided, the category that best fits it is:\n",
            "\n",
            "{'categorie': 'Voordracht'}\n",
            "\n",
            "This document appears to be a motion or proposal presented to the council, outlining a specific issue or topic for discussion and providing supporting arguments or evidence. The document includes information on the agenda item, the status of the proposal, and the reasoning behind the proposal. It is a typical document used in a council meeting to initiate discussion and decision-making on a particular topic.\n",
            "prediction: voordracht\n",
            "label:  motie\n",
            "response:   Based on the provided document, the category that best fits it is:\n",
            "\n",
            "{'categorie': 'Voordracht'}\n",
            "\n",
            "This document is a motion (Motie) presented to the Gemeente Amsterdam council on January 25, 2023, regarding the clarification of rules for green connections. The document includes the title of the motion, the date of the council meeting, the number of the agenda item, the status of the motion (Aangenomen), and the topic of the motion (duidelijk kwantificeren van regels voor groene verbindingen). The document also includes the signature of the person presenting the motion. Therefore, the category that best fits this document is \"Voordracht\".\n",
            "prediction: voordracht\n",
            "label:  motie\n",
            "response:   Sure, I can classify the document \"Gemeente Bezoekadres\" into one of the categories you provided. Based on the content of the document, I would categorize it as:\n",
            "\n",
            "{'categorie': 'Agenda'}\n",
            "\n",
            "The document appears to be an agenda for a meeting of the Commissie Algemene Zaken (Commission on General Affairs) of the municipality of Amsterdam. It includes the date, time, location, and attendees of the meeting, as well as the topics to be discussed and the decisions to be made.\n",
            "prediction: agenda\n",
            "label:  motie\n",
            "response:   Based on the provided document, I would classify it as a 'Motie' (Motion) with the following information:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The document is a motion proposed by Naoum Néhmé and Van Schijndel regarding the Reparative Ordinance and the abolition of legal grey areas in the Housing Act 2014.\n",
            "prediction: motie\n",
            "label:  agenda\n",
            "response:   Based on the provided document, I would classify it as a 'Raadscommissie Vereniging' or 'Council Meeting Agenda'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadscommissie Vereniging\"\n",
            "}\n",
            "\n",
            "Explanation:\n",
            "\n",
            "The document is a agenda for a council meeting of the Raadscommissie voor Algemene Zaken, Openbare Orde en Veiligheid (Council Committee for General Affairs, Public Order and Safety) of the Gemeente Amsterdam (City of Amsterdam). The agenda includes items related to evaluations, regulations, and communication, which are all typical topics discussed in a council meeting. Therefore, the document can be classified as a 'Raadscommissie Vereniging' or 'Council Meeting Agenda'.\n",
            "prediction: NoPredictionInOutput\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the provided document, the category that best fits it is:\n",
            "\n",
            "{'categorie': 'Schriftelijke Vraag'}\n",
            "\n",
            "This document is a written question submitted to the Gemeente Amsterdam by a member of the council, containing questions related to the serious damage of stained glass windows of the Oude Kerk in Amsterdam. The document includes the questions asked by the member, the answers provided by the municipality, and the date of indention, approval, and publication. Therefore, it can be classified under the category of 'Schriftelijke Vraag' (Written Question) as it is a formal document that seeks information or clarification from the municipality through written means.\n",
            "prediction: schriftelijke vraag\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the content of the document you provided, I would classify it as a \"Raadsnotulen\".\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Raadsnotulen\"\n",
            "}\n",
            "\n",
            "The document appears to be a report or minutes of a meeting related to the Mystic Garden Festival held in Sloterpark Amsterdam, and it includes information such as the date, title, author, and project number, as well as details about the festival itself and the evaluation conducted. All of these elements suggest that the document is a formal record or minutes of a meeting, which is consistent with the category of \"Raadsnotulen\".\n",
            "prediction: raadsnotulen\n",
            "label:  motie\n",
            "response:   Sure! Based on the content of the document, I would classify it as a 'Motie'.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The reason for this classification is that the document is a motion put forward by several members of the Gemeente Amsterdam city council, addressed to the city council itself, and it deals with a specific topic or issue (in this case, the formation of BlZ-en in all city districts as part of the city's economic recovery and investment plan).\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Motie' (Motion) with the following output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The document is a motion presented by a member of the Gemeenteraad (City Council) of Amsterdam, regarding the 'Plan voor een groene Zuidas' (Plan for a Green South Axis), the abolition of the Groeifonds Groen (Growth Fund Green) and the Herplantfonds Zuidas (Replant Fund South Axis) (green management at the top level).\n",
            "prediction: motie\n",
            "label:  agenda\n",
            "response:   Based on the content of the document \"Termijnagenda stadsdeelcommissie Noord\", I would classify it under the category of \"Agenda\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Agenda\"\n",
            "}\n",
            "prediction: agenda\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...110:120 out of 129\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a \"Motie\" (Motion) with the following categorization:\n",
            "\n",
            "{ \"categorie\": \"Motie\" }\n",
            "\n",
            "Here's why:\n",
            "\n",
            "* The document is addressed to the Gemeenteraad (City Council) of Amsterdam.\n",
            "* The document is titled \"Motie van het lid Van Soest inzake de Parkeerverordening 2013 (betaald parkeren zone Van der Pekbuurt)\" which indicates that it is a motion proposed by a member of the council.\n",
            "* The document discusses a specific issue related to the parking regulations in the Van der Pekbuurt area of Amsterdam.\n",
            "* The document includes information about the discussion held during the council meeting where the motion was proposed and voted upon.\n",
            "* The status of the motion is \"Verworpen\" (Rejected) which further supports the categorization of the document as a motion.\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Motie' (Motion).\n",
            "\n",
            "Here is the output in JSON format: {'categorie': 'Motie'}\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the information provided, I would classify the document as a \"Motie\" (Motion) with the following categories:\n",
            "\n",
            "{ \"categorie\": \"Motie\" }\n",
            "\n",
            "The document is a motion presented to the Gemeenteraad of Amsterdam, titled \"Motie van de raadsleden de heer Blom en de heer Mbarki inzake de begroting voor 2015 (evalueren van de communicatie en de veranderingen in de jeugdzorg)\". This category is consistent with the format of a motion, which is a proposal or proposal for action that is presented to the council or assembly for consideration and debate.\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Motie' (Motion) with the following details:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "Explanation:\n",
            "\n",
            "* The document is titled \"Motie van de leden Groen en De Heer inzake de vergunningverlening voor rondvaartboten (extern onderzoek passagiersvaart)\" which indicates that it is a motion proposed by members of the council.\n",
            "* The document is addressed to the Gemeenteraad (City Council) and contains information about the motion, including the date it was received, discussed, and adopted.\n",
            "* The topic of the motion is related to the granting of permission for round-trip boats (extern underground passenger traffic), which suggests that it is a motion related to transportation or infrastructure.\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the content of the document you provided, I would classify it as a 'Motie' (Motion) with the following output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The document is a motion presented by a member of the Gemeenteraad (City Council) of Amsterdam, N.T. Bakker, regarding the allocation of funds for the 'Digital Society School'. The document includes the motion itself, the date of presentation, and the status of the motion (verworpen, rejected). Therefore, it belongs to the category of 'Motie' (Motion) in the Gemeenteblad (City Council agenda) of Amsterdam.\n",
            "prediction: motie\n",
            "label:  schriftelijke vraag\n",
            "response:   Based on the information provided, I would classify the document as follows:\n",
            "\n",
            "{'categorie': 'Schriftelijke Vraag'}\n",
            "\n",
            "This document is a written question submitted to the Gemeenteraad of Amsterdam by a member named Duijndam, regarding the rental of municipal workshops at the Lutkemeerweg. It contains the question, the answer provided by the municipality, and related information. Therefore, it belongs to the category of 'Schriftelijke Vraag' (Written Question) in the provided list of categories.\n",
            "prediction: schriftelijke vraag\n",
            "label:  motie\n",
            "response:   Based on the content of the document you provided, I would classify it under the category of \"Voordracht\" (Dutch for \"Motion\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "prediction: voordracht\n",
            "label:  motie\n",
            "response:   Based on the content of the document, I would classify it as a 'Motie' (Motion) with the following information:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The document is a motion presented to the Gemeenteraad of Amsterdam in 2021, regarding the Regional Energy Strategy and the inclusion of wind turbines in political party manifestos. The motion is proposed by a member of the council, Kreuger, and is discussed and voted on by the council.\n",
            "prediction: motie\n",
            "label:  agenda\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Agenda\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Agenda\"\n",
            "}\n",
            "prediction: agenda\n",
            "label:  motie\n",
            "response:   Based on the content of the document you provided, I would classify it as a \"Motie\" (Dutch for \"Motion\").\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "prediction: motie\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n",
            "Starting...last 9 docs\n",
            "label:  motie\n",
            "response:   Based on the content of the document, I would classify it under the category \"Voordracht\" or \"Besluit\".\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "prediction: voordracht\n",
            "label:  motie\n",
            "response:   Based on the content of the document, I would classify it as a 'Motie' (Motion) document.\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "prediction: motie\n",
            "label:  agenda\n",
            "response:   Based on the content of the document you provided, I would classify it under the category of \"Agenda\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Agenda\"\n",
            "}\n",
            "\n",
            "The document is an agenda for a meeting of the public commission of the city of Amsterdam, specifically for the district of Zuid. It outlines the items to be discussed and the time allocated to each item, including proposals for the management of public education, a speech by a new board member of the Foundation for Public Education at the Amstel, and a strategic plan for 2011-2020. Therefore, it falls under the category of an agenda, which is a document that outlines the items to be discussed and the schedule of a meeting or conference.\n",
            "prediction: agenda\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Motie' (Motion) with the following details:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The document is a motion presented by a member of the Gemeenteraad (City Council) of Amsterdam, titled \"Motie van het lid A.L. Bakker inzake de conceptnota Duurzaam Landschap, de Amsterdamse inzet voor het metropolitane landschap en bouwsteen voor de omgevingsvisie (geef natuur een stem)\" (Motion of member A.L. Bakker regarding the concept note Sustainable Landscape, the Amsterdam metropolitan area and building block for the environmental vision (give nature a voice)). It is dated September 27, 2019, and was rejected by the council.\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Sure, I can classify the document for you. Based on the provided text, the document can be classified into the category of \"Motie\" (Motion).\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the content of the document you provided, I would classify it as a 'Besluit' (Dutch for 'Decision').\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "\n",
            "The reason for this classification is that the document appears to be a formal decision made by the Gemeente Amsterdam Gemeenteraad, with a clear subject matter (the amendment of a note regarding the cleaning of large boats in Amsterdam), a specific date and time of discussion, and a final status of rejection. All of these elements are typical of a 'Besluit' document in the Netherlands.\n",
            "prediction: besluit\n",
            "label:  motie\n",
            "response:   Based on the content of the document, I would classify it under the category of \"Besluit\".\n",
            "\n",
            "Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Besluit\"\n",
            "}\n",
            "prediction: besluit\n",
            "label:  motie\n",
            "response:   Based on the information provided in the document, I would classify it as a 'Motie' (Motion) with the following JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Motie\"\n",
            "}\n",
            "\n",
            "The document is a motion presented by a member of the Gemeenteraad (City Council) of Amsterdam, Ernsting, regarding the abatement of parking and bicycle parking garage Rokin. The motion is addressed to the Gemeenteraad and includes the date of presentation, the topic of the motion, and the status of the motion (verworpen, rejected).\n",
            "prediction: motie\n",
            "label:  motie\n",
            "response:   Based on the content of the document provided, I would classify it as a 'Voordracht'.\n",
            "\n",
            "Here is the JSON output:\n",
            "\n",
            "{\n",
            "\"categorie\": \"Voordracht\"\n",
            "}\n",
            "prediction: voordracht\n",
            "Dont interrupt, saving predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving done! Interrupting is allowed.\n"
          ]
        }
      ],
      "source": [
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_llama, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>run_id</th>\n",
              "      <th>date</th>\n",
              "      <th>train_set</th>\n",
              "      <th>test_set</th>\n",
              "      <th>train_set_support</th>\n",
              "      <th>test_set_support</th>\n",
              "      <th>split_col</th>\n",
              "      <th>text_col</th>\n",
              "      <th>runtime</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_avg_precision</th>\n",
              "      <th>macro_avg_recall</th>\n",
              "      <th>macro_avg_f1</th>\n",
              "      <th>classification_report</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>IC_Llama-2-7b-chat-hfsimple_prompt_v2LlamaToke...</td>\n",
              "      <td>2024-05-15 20:49:11.055823+02:00</td>\n",
              "      <td>dev</td>\n",
              "      <td>val</td>\n",
              "      <td>832</td>\n",
              "      <td>209</td>\n",
              "      <td>4split</td>\n",
              "      <td>TruncationLlamaTokensFront200Back0</td>\n",
              "      <td>29690.987126</td>\n",
              "      <td>0.583732</td>\n",
              "      <td>0.475256</td>\n",
              "      <td>0.439337</td>\n",
              "      <td>0.371228</td>\n",
              "      <td>precision    ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model                                             run_id  \\\n",
              "0  Llama-2-7b-chat-hf  IC_Llama-2-7b-chat-hfsimple_prompt_v2LlamaToke...   \n",
              "\n",
              "                              date train_set test_set  train_set_support  \\\n",
              "0 2024-05-15 20:49:11.055823+02:00       dev      val                832   \n",
              "\n",
              "   test_set_support split_col                            text_col  \\\n",
              "0               209    4split  TruncationLlamaTokensFront200Back0   \n",
              "\n",
              "        runtime  accuracy  macro_avg_precision  macro_avg_recall  \\\n",
              "0  29690.987126  0.583732             0.475256          0.439337   \n",
              "\n",
              "   macro_avg_f1                              classification_report  \n",
              "0      0.371228                                   precision    ...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "display(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mistral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - in context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot_mistral = pipeline(task='conversational', model='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "                   device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'Mistral-7B-Instruct-v0.2'\n",
        "SUBFOLDER = 'in_context'\n",
        "SHORT_ID = 'IC'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model - finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "# chatbot_mistral = pipeline(task='conversational', model='FemkeBakker/MistralSmallData200Tokens',\n",
        "#                    device_map='cpu', model_kwargs={'offload_buffers':True})\n",
        "\n",
        "MODEL_NAME = 'MistralSmallData200Tokens'\n",
        "SUBFOLDER = 'finetuning'\n",
        "SHORT_ID = 'FT'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "PROMPT = pt.simple_prompt\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens' # column with text split using tokenizer of either mistral (MistralTokens) or Llama (LlamaTokens). Using Llama, because Llama split into more tokens. \n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "\n",
        "if PROMPT == pt.simple_prompt or PROMPT==pt.simple_prompt_v2 or PROMPT==pt.OldSimple_prompt:\n",
        "    NUMBER_EXAMPLES = 0\n",
        "elif PROMPT == pt.fewshot_prompt_bm25:\n",
        "    NUMBER_EXAMPLES = 2\n",
        "    \n",
        "OVERVIEW_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/Mistral/{PROMPT_NAME}/overview.pkl\"\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictionsVal/{SUBFOLDER}/Mistral/{PROMPT_NAME}/predictions.pkl\"\n",
        "\n",
        "run_id = f'{SHORT_ID}_{MODEL_NAME}{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}{TRAIN_SET}{TEST_SET}_numEx{NUMBER_EXAMPLES}'\n",
        "print(run_id)\n",
        "# small = txt.iloc[16:22]\n",
        "# small['4split']=['val', 'dev', 'dev', 'dev', 'dev', 'dev']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run experiment\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(chatbot_mistral, trunc_df, run_id, PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "display(pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(OVERVIEW_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gibberish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fewshot Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictions/fewShotGeitjepredictions.pkl\"\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/overview/fewShotGeitjepredictions.pkl\"\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictions/trialfewShotGeitjepredictions.pkl\"\n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/overview/trialfewShotGeitjepredictions.pkl\"\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "TEXT_COLUMN = 'trunc_txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictions/fewShotLlamapredictions.pkl\"\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/overview/fewShotLlamapredictions.pkl\"\n",
        "# PREDICTION_PATH = f\"{cf.output_path}/predictions/trialfewShotLlamapredictions.pkl\"\n",
        "# OVERVIEW_PATH = f\"{cf.output_path}/overview/trialfewShotLlamapredictions.pkl\"\n",
        "MODEL_NAME = 'Llama-2-7b-chat-hf'\n",
        "TEXT_COLUMN = 'trunc_txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----- EXPERIMENT: ?? --------\n",
        "\n",
        "# run experiment\n",
        "PROMPT = pt.fewshot_prompt_bm25\n",
        "PROMPT_NAME = ph.get_promptfunction_name(PROMPT)\n",
        "TOKENS_COL = 'LlamaTokens'\n",
        "FRONT_THRESHOLD = 200\n",
        "BACK_THRESHOLD = 0\n",
        "NUMBER_EXAMPLES = 2\n",
        "# small = txt.iloc[0:5]\n",
        "# small['4split']='val'\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(txt,'text', TOKENS_COL, FRONT_THRESHOLD, BACK_THRESHOLD)\n",
        "\n",
        "# if new run MAKE SURE RUN_ID IS UNIQUE, if want to resume run, pass in that run_id\n",
        "run_experiment(trunc_df, f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}', PROMPT, TEXT_COLUMN, SPLIT_COLUMN, TRAIN_SET, TEST_SET, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH, MODEL_NAME, NUMBER_EXAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(OVERVIEW_PATH)\n",
        "# pred_run = pred.loc[pred['run_id']==f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}']\n",
        "# print(sum(pred_run['runtime']))\n",
        "# pred['runtime'] = sum(pred_run['runtime'])\n",
        "display(pred.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = pd.read_pickle(PREDICTION_PATH)\n",
        "pred_run = pred.loc[pred['run_id']==f'{PROMPT_NAME}{TOKENS_COL}{FRONT_THRESHOLD}_{BACK_THRESHOLD}']\n",
        "print(sum(pred_run['runtime']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## End notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_class_list():\n",
        "#     return ['Voordracht', 'Besluit', 'Schriftelijke Vragen', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheets']\n",
        "\n",
        "# def fewshot_prompt_examples(doc, train_df, num_examples, text_column):\n",
        "#     examples = train_df.sample(n=num_examples)\n",
        "\n",
        "#     prompt = f\"\"\"\n",
        "#     Het is jouw taak om een document te categoriseren in één van de categoriën.\n",
        "#     Eerst krijg je een lijst met mogelijke categoriën, daarna {num_examples} voorbeelden van documenten en tot slot het document dat gecategoriseerd moet worden. \n",
        "    \n",
        "#     Categoriën: {get_class_list()}\n",
        "#     \"\"\"\n",
        "\n",
        "#     for index, row in examples.iterrows():\n",
        "#         mini_prompt = f\"\"\"\n",
        "#     Dit is een voorbeeld document de categorie {row['label']}:\n",
        "#         {row[text_column]}\n",
        "#         \"\"\"\n",
        "\n",
        "#         prompt += mini_prompt\n",
        "\n",
        "#     doc_prompt = f\"\"\"\n",
        "#     Categoriseer dit document:\n",
        "#         {doc}\n",
        "#     \"\"\"\n",
        "\n",
        "#     prompt += doc_prompt\n",
        "#     return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def simple_prompt(doc,train_df, num_examples, text_column):\n",
        "#     prompt = f\"\"\"\n",
        "#     Classificeer het document in één van de categoriën.\n",
        "#     Houd het kort, geef enkel de naam van de categorie als response.\n",
        "    \n",
        "#     Categoriën: {get_class_list()}\n",
        "    \n",
        "#     Document: \n",
        "#     {doc}\n",
        "    \n",
        "#     \"\"\"\n",
        "#     return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import time\n",
        "# import os\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# \"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# # docs_df = dataframe with the documents that need to be predicted\n",
        "# # text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# # prompt_function = prompt template \n",
        "\n",
        "# def predictions_incontextlearning(docs_df, text_column, prompt_function, train_df, num_examples):\n",
        "#     results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date', 'prompt'])\n",
        "    \n",
        "#     # prompt each document\n",
        "#     for index, row in docs_df.iterrows():\n",
        "#         if (index + 1) % 200 == 0:\n",
        "#             print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         # get the prompt, with the doc filled in\n",
        "#         txt = row[text_column]\n",
        "\n",
        "#         # always give these as input, however not every template uses all of them\n",
        "#         prompt = prompt_function(txt, train_df, num_examples, text_column)\n",
        "\n",
        "#         # prompt and get the response\n",
        "#         converse = chatbot(Conversation(prompt))\n",
        "#         response = converse[1]['content']\n",
        "\n",
        "#         # extract prediction from response\n",
        "#         prediction = ph.get_prediction_from_response(response)\n",
        "\n",
        "#         # save results in dataframe\n",
        "#         results_df.loc[len(results_df)] = {\n",
        "#             'id': row['id'],\n",
        "#             'path' : row['path'],\n",
        "#             'text_column' : text_column,\n",
        "#             'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "#             'response':response,\n",
        "#             'prediction':prediction,\n",
        "#             'label':row['label'].lower(),\n",
        "#             'runtime':time.time()-start_time,\n",
        "#             'date': ph.get_datetime(),\n",
        "#             'prompt':prompt\n",
        "#         }\n",
        "#     return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import time\n",
        "# from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# \"\"\"\n",
        "# Function to run GEITje In-Context Learning experiment. \n",
        "# The function allows to resume experiment, if run_id matches.\n",
        "# \"\"\"\n",
        "# # df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# # run_id = unqiue for each experiment. \n",
        "# # prompt_function = which prompt from prompt_template.py to use\n",
        "# # text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# # split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# # subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# # subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# # label_col = column with the true label\n",
        "# # prediction_path = path to file where predictions need to be saved.\n",
        "# # overview_path = path to file where results of each run need to be saved.\n",
        "# # model_name = name of the model. string.\n",
        "# # num_exmples = number of exaples given to prompt. zero in case of zeroshot. \n",
        "\n",
        "# def run_experiment(df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "#     print(num_examples)\n",
        "#     start_time = time.time()\n",
        "#     test_df = df.loc[df[split_col]==subset_test]\n",
        "#     train_df = df.loc[df[split_col]==subset_train]\n",
        "    \n",
        "#     # get rows of df that still need to be predicted for the specific run_id\n",
        "#     to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "#     # devide to_predict into subsection of 50 predictions at a time. \n",
        "#     # Allows to rerun without problem. \n",
        "#     step_range = list(range(0, len(to_predict), 3))\n",
        "\n",
        "#     for i in range(len(step_range)):\n",
        "#         try:\n",
        "#             sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "#             print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "#         except Exception as e:\n",
        "#             sub_to_predict = to_predict[step_range[i]:]\n",
        "#             print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "#         # prompt geitje\n",
        "#         predictions = predictions_incontextlearning(sub_to_predict, text_col, prompt_function, train_df, num_examples)\n",
        "\n",
        "#         # save info\n",
        "#         predictions['run_id'] = run_id\n",
        "#         predictions['train_set'] = subset_train\n",
        "#         predictions['test_set'] = subset_test\n",
        "#         predictions['shots'] = num_examples\n",
        "\n",
        "#         # save new combinations in file\n",
        "#         ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "#         # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "#         try:\n",
        "#             predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "#         except Exception as e:\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "\n",
        "#         # save results in overview file\n",
        "#         date = ph.get_datetime()\n",
        "#         y_test = predictions['label']\n",
        "#         y_pred = predictions['prediction']\n",
        "#         report = classification_report(y_test, y_pred)\n",
        "\n",
        "#         overview = pd.DataFrame(\n",
        "#             [{\n",
        "#                 'model':model_name,\n",
        "#                 'run_id':run_id,\n",
        "#                 'date': date,\n",
        "#                 'train_set': subset_train,\n",
        "#                 'test_set': subset_test,\n",
        "#                 'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "#                 'test_set_support':len(predictions),\n",
        "#                 'split_col':split_col,\n",
        "#                 'text_col':text_col,\n",
        "#                 'runtime':time.time()-start_time,\n",
        "#                 'accuracy': accuracy_score(y_test, y_pred),\n",
        "#                 'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "#                 'classification_report':report\n",
        "#             }   ]\n",
        "#         )\n",
        "#         # remove previous results of run_id, replace with new/updated results\n",
        "#         ph.replace_and_save_df(overview, overview_path, run_id)\n",
        "\n",
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "#set  variables, same for each model\n",
        "TRAIN_SET = 'dev' # must be dev or train\n",
        "TEST_SET = 'val' # must be val or test\n",
        "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
        "LABEL_COLUMN = 'label'\n",
        "PREDICTION_PATH = f\"{cf.output_path}/predictions/tryoutGeitjepredictions.pkl\"\n",
        "OVERVIEW_PATH = f\"{cf.output_path}/overview/tryoutGeitjepredictions.pkl\"\n",
        "MODEL_NAME = 'GEITje-7B-chat-v2'\n",
        "TEXT_COLUMN = 'trunc_txt'\n",
        "\n",
        "p_path = f\"{cf.output_path}/predictions/tryoutGeitjepredictions.pkl\"\n",
        "o_path = f\"{cf.output_path}/overview/tryoutGeitjeoverview.pkl\"\n",
        "run_experiment(small, 'tryout_zeroshot', pt.simple_prompt, 'trunc_txt', '4split', 'dev', 'val', 'label', p_path, o_path, 'GEITje-7B-chat-v2', 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yeet = pd.read_pickle(p_path)\n",
        "yeet  = yeet.loc[yeet['run_id']=='tryout_zeroshot']\n",
        "display(yeet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(yeet.iloc[0]['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")\n",
        "\n",
        "small = txt.iloc[0:5]\n",
        "small['4split']=['val', 'dev', 'val', 'dev', 'dev']\n",
        "\n",
        "# add new column with truncated text -> new dataframe with column + new column name\n",
        "trunc_df = tf.add_truncation_column(small,'text', 'LlamaTokens', 200, 200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GIbberish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import time\n",
        "# import os\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# \"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# # docs_df = dataframe with the documents that need to be predicted\n",
        "# # text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# # prompt_function = prompt template -> ONLY prompt templates that take doc as input (ZERO SHOT)\n",
        "\n",
        "# def zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function):\n",
        "#     results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date'])\n",
        "    \n",
        "#     # prompt each document\n",
        "#     for index, row in docs_df.iterrows():\n",
        "#         if (index + 1) % 200 == 0:\n",
        "#             print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         # get the prompt, with the doc filled in\n",
        "#         txt = row[text_column]\n",
        "#         prompt = prompt_function(txt)\n",
        "\n",
        "#         # prompt and get the response\n",
        "#         converse = chatbot(Conversation(prompt))\n",
        "#         response = converse[1]['content']\n",
        "\n",
        "#         # extract prediction from response\n",
        "#         prediction = ph.get_prediction_from_response(response)\n",
        "\n",
        "#         # save results in dataframe\n",
        "#         results_df.loc[len(results_df)] = {\n",
        "#             'id': row['id'],\n",
        "#             'path' : row['path'],\n",
        "#             'text_column' : text_column,\n",
        "#             'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "#             'response':response,\n",
        "#             'prediction':prediction,\n",
        "#             'label':row['label'].lower(),\n",
        "#             'runtime':time.time()-start_time,\n",
        "#             'date': ph.get_datetime()\n",
        "#         }\n",
        "#     return results_df\n",
        "\n",
        "\n",
        "#  import os\n",
        "# import time\n",
        "# from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# \"\"\"\n",
        "# Function to run GEITje ZEROSHOT experiment. \n",
        "# The function allows to resume experiment, if run_id matches.\n",
        "# \"\"\"\n",
        "# # df = dataframe with all docs that need to have a prediction (docs still need to be predict + already predicted)\n",
        "# # run_id = unqiue for each experiment. \n",
        "# # prompt_function = which prompt from prompt_template.py to use\n",
        "# # text_col = colum in df where the text is. (Needs to be already truncated)\n",
        "# # split_col = column with the dataset split. Either '2split' (train and test)or '4split'(train, test, dev and val)\n",
        "# # subset_train = indicates which subset to use as training. either 'train' or 'dev'\n",
        "# # subset_test = indicates which subset to use for testing. either 'test' or 'val'\n",
        "# # label_col = column with the true label\n",
        "# # prediction_path = path to file where predictions need to be saved.\n",
        "# # overview_path = path to file where results of each run need to be saved.\n",
        "# # model_name = name of the model. string.\n",
        "# # num_exmples = number of exaples given to prompt. zero inn case of zeroshot. \n",
        "\n",
        "# def run_experiment(df, run_id, prompt_function, text_col, split_col, subset_train, subset_test, label_col, prediction_path, overview_path, model_name, num_examples=0):\n",
        "#     start_time = time.time()\n",
        "#     test_df = df.loc[df[split_col]==subset_test]\n",
        "    \n",
        "#     # get rows of df that still need to be predicted for the specific run_id\n",
        "#     to_predict, previous_predictions = ph.get_rows_to_predict(test_df, prediction_path, run_id)\n",
        "\n",
        "#     # devide to_predict into subsection of 50 predictions at a time. \n",
        "#     # Allows to rerun without problem. \n",
        "#     step_range = list(range(0, len(to_predict), 3))\n",
        "\n",
        "#     for i in range(len(step_range)):\n",
        "#         try:\n",
        "#             sub_to_predict = to_predict.iloc[step_range[i]:step_range[i+1]]\n",
        "#             print(f'Starting...{step_range[i]}:{step_range[i+1]} out of {len(to_predict)}')\n",
        "#         except Exception as e:\n",
        "#             sub_to_predict = to_predict[step_range[i]:]\n",
        "#             print(f'Starting...last {len(sub_to_predict)} docs')\n",
        "\n",
        "#         # prompt geitje\n",
        "#         predictions = zero_shot_predictions_incontextlearning(sub_to_predict, text_col, prompt_function)\n",
        "\n",
        "#         # save info\n",
        "#         predictions['run_id'] = run_id\n",
        "#         predictions['train_set'] = subset_train\n",
        "#         predictions['test_set'] = subset_test\n",
        "#         predictions['shots'] = num_examples\n",
        "\n",
        "#         # save new combinations in file\n",
        "#         ph.combine_and_save_df(predictions, prediction_path)\n",
        "\n",
        "#         # if previous predictions, combine previous with new predictions, to get update classification report\n",
        "#         try:\n",
        "#             predictions = pd.concat([predictions, previous_predictions])\n",
        "\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "#         except Exception as e:\n",
        "#             # set previous predictions to all predictions made until now. Necessary for next loop\n",
        "#             previous_predictions = predictions\n",
        "\n",
        "#         # save results in overview file\n",
        "#         date = ph.get_datetime()\n",
        "#         y_test = predictions['label']\n",
        "#         y_pred = predictions['prediction']\n",
        "#         report = classification_report(y_test, y_pred)\n",
        "\n",
        "#         overview = pd.DataFrame(\n",
        "#             [{\n",
        "#                 'model':model_name,\n",
        "#                 'run_id':run_id,\n",
        "#                 'date': date,\n",
        "#                 'train_set': subset_train,\n",
        "#                 'test_set': subset_test,\n",
        "#                 'train_set_support':len(df.loc[df[split_col]==subset_train]),\n",
        "#                 'test_set_support':len(predictions),\n",
        "#                 'split_col':split_col,\n",
        "#                 'text_col':text_col,\n",
        "#                 'runtime':time.time()-start_time,\n",
        "#                 'accuracy': accuracy_score(y_test, y_pred),\n",
        "#                 'macro_avg_precision': precision_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_recall': recall_score(y_test, y_pred, average='macro'),\n",
        "#                 'macro_avg_f1': f1_score(y_test, y_pred, average='macro'),\n",
        "#                 'classification_report':report\n",
        "#             }   ]\n",
        "#         )\n",
        "#         # remove previous results of run_id, replace with new/updated results\n",
        "#         ph.replace_and_save_df(overview, overview_path, run_id)\n",
        " \n",
        "# # p_path = f\"{cf.output_path}/predictions/tryoutGeitjepredictions.pkl\"\n",
        "# # o_path = f\"{cf.output_path}/overview/tryoutGeitjeoverview.pkl\"\n",
        "# # run_experiment(txt.iloc[25:30], 'tryout', pt.simple_prompt, trunc_col, '4split', 'dev', 'val', 'label', p_path, o_path, 'GEITje-7B-chat-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n",
        "df = df.loc[df['set']=='val']\n",
        "df['text_trunc_100'] = df['tokens'].apply(text_truncation,100)\n",
        "df['text_trunc_1000'] = df['tokens'].apply(text_truncation,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\"\n",
        "resume_predictions(df, path, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dummy code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_in_subsections(df, path, set_run_id):\n",
        "\n",
        "    iterations = list(range(0, len(df)+50, 50))\n",
        "    for i in range(len(iterations)):\n",
        "        try:\n",
        "            subdf = df.iloc[iterations[i]:iterations[i+1]]\n",
        "\n",
        "        except IndexError:\n",
        "            subdf = df.iloc[iterations[i]:]\n",
        "\n",
        "        # if set_run_id == 'new' and iterations[i]==0:\n",
        "        #     run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'new', path, 'val')\n",
        "        # else:\n",
        "        #     run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'previous', path, 'val')\n",
        "\n",
        "\n",
        "\n",
        "path = f\"{cf.output_path}/predictions/ICgeitje_predictions_tryout.pkl\"\n",
        "run_in_subsections(df, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_in_subsections(df, path):\n",
        "    subdf = df.iloc[0:50]\n",
        "    run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'new', path, 'val')\n",
        "\n",
        "    iterations = list(range(50, len(df)+50, 50))\n",
        "    for i in range(len(iterations)):\n",
        "        if i < len(iterations)-2:\n",
        "            subdf = df.iloc[iterations[i]:iterations[i+1]]\n",
        "            print(\"\\n\", \"iterations\", iterations[i], iterations[i+1], \"\\n\")\n",
        "            run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'previous', path, 'val')\n",
        "\n",
        "        elif i < len(iterations)-1:\n",
        "            subdf = df.iloc[iterations[i]:]\n",
        "            print(\"\\n\", \"iterations\", iterations[i], '\\n')\n",
        "            run_prediction(subdf, 'text_trunc_100', pt.simple_prompt, 'previous', path, 'val')\n",
        "\n",
        "path = f\"{cf.output_path}/predictions/ICgeitje_predictions_tryout.pkl\"\n",
        "run_in_subsections(df, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yeet = pd.read_pickle(f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\")\n",
        "display(yeet)\n",
        "\n",
        "yeet = pd.read_pickle(f\"{cf.output_path}/overview_results.pkl\")\n",
        "display(yeet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import sys\n",
        "sys.path.append('../scripts/') \n",
        "import prompt_template as pt\n",
        "import prediction_helperfunctions as ph\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template -> ONLY prompt templates that take doc as input (ZERO SHOT)\n",
        "\n",
        "def zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date'])\n",
        "    \n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        if (index + 1) % 200 == 0:\n",
        "            print(f\"Iteration {index +1}/{len(docs_df)} completed.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "        prompt = prompt_function(txt)\n",
        "\n",
        "        # prompt and get the response\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = ph.get_prediction_from_response(response)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : text_column,\n",
        "            'prompt_function': ph.get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': ph.get_datetime()\n",
        "        }\n",
        "    return results_df\n",
        "\n",
        "# \"\"\" Run a prediction function -> can be ZeroShot or FewShot \"\"\"\n",
        "# def run_prediction(docs_df, text_column, prompt_function, subset=None, learning='ZeroShot'):\n",
        "#     if learning == 'ZeroShot':\n",
        "#         # get the predictions\n",
        "#         res = zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function)\n",
        "\n",
        "#         # INSERT ELSE STATEMENT HERE FOR FEWSHOT\n",
        "\n",
        "#         # get run_id\n",
        "#         path = f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\"\n",
        "#         res['run_id'], predictions_df = ph.get_runid(path)\n",
        "\n",
        "#         # combine earlier predictions with new ones\n",
        "#         all_predictions = pd.concat([predictions_df, res])\n",
        "\n",
        "#         # save predictions\n",
        "#         all_predictions.to_pickle(path)\n",
        "\n",
        "#         # save the evaluation metrics for each run\n",
        "#         ph.update_overview_results(res, 'Rijgersberg/GEITje-7B-chat-v2')\n",
        "#         return res\n",
        "# gestart om 10.15/\n",
        "# res = run_prediction(df, 'text_trunc_100', pt.simple_prompt, 'val')\n",
        "# display(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yeet = pd.read_pickle(f\"{cf.output_path}/overview_results.pkl\")\n",
        "display(yeet)\n",
        "\n",
        "yeet = pd.read_pickle(f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\")\n",
        "display(yeet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tryout GEITje\n",
        "Load chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "chatbot = pipeline(task='conversational', model='Rijgersberg/GEITje-7B-chat-v2',\n",
        "                   device_map='auto')\n",
        "\n",
        "## simple query\n",
        "print(chatbot(\n",
        "    Conversation(\"Hallo, ik ben Bram. Ik wil vanavond graag een film kijken. Heb je enkele suggesties?\")\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline, Conversation\n",
        "\n",
        "# load_in_8bit: lower precision but saves a lot of GPU memory\n",
        "# device_map=auto: loads the model across multiple GPUs\n",
        "# chatbot = pipeline(\"conversational\", model=\"BramVanroy/GEITje-7B-ultra\",  model_kwargs={\"load_in_8bit\": True}, device_map=\"auto\")\n",
        "chatbot = pipeline(\"conversational\", model=\"BramVanroy/GEITje-7B-ultra\",  device_map=\"auto\")\n",
        "\n",
        "# start_messages = [\n",
        "#     # {\"role\": \"system\", \"content\": \"Je bent een grappige chatbot die Bert heet. Je maakt vaak mopjes.\"},\n",
        "#     {\"role\": \"user\", \"content\": \"Hallo, ik ben Bram. Ik wil vanavond graag een film kijken. Heb je enkele suggesties?\"}\n",
        "# ]\n",
        "# conversation = Conversation(start_messages)\n",
        "# conversation = chatbot(conversation)\n",
        "# response = conversation.messages[-1][\"content\"]\n",
        "# print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = df.iloc[0]['text']\n",
        "prompt = f\"\"\"\n",
        "\n",
        "Classificeer de gegeven tekst in 1 van de categoriën.\n",
        "Geef als reactie enkel de naam van de categorie\n",
        "Categorieën: ['Voordracht', 'Besluit', 'Schriftelijke Vragen', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheets']\n",
        "Tekst: \n",
        "\n",
        "{txt}\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "start_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Jouw enige taak is om teksten te classificeren. Je geeft geen uitleg voor je keuzes.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chatbot(Conversation(start_messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = df.loc[df['clean_tokens_count'].idxmax()]['text']\n",
        "print(df.loc[df['clean_tokens_count'].idxmax()]['clean_tokens_count'])\n",
        "\n",
        "print(pt.simple_prompt(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(chatbot(\n",
        "    Conversation(pt.simple_prompt(text))\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16,\n",
        "                                             low_cpu_mem_usage=True, attn_implementation='eager',\n",
        "                                             device_map=device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate(conversation, temperature=0.2, top_k=50, max_new_tokens=1_000):\n",
        "    tokenized = tokenizer.apply_chat_template(conversation, add_generation_prompt=True,\n",
        "                                              return_tensors='pt').to(device)\n",
        "    outputs = model.generate(tokenized, do_sample=True, temperature=temperature,\n",
        "                             top_k=top_k, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?'\n",
        "    }\n",
        "]\n",
        "print(generate(conversation))\n",
        "# <|user|>\n",
        "# Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"? \n",
        "# <|assistant|>\n",
        "# Het woord dat niet op zijn plaats staat is 'geit'. Een geit zou niet tussen een lijst van vervoersmiddelen moeten staan. Het past beter bij een boerderijthema of dierenlijst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BACK-UP CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import pytz\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\"\"\" Given the string response, extract the prediction \"\"\"\n",
        "def get_prediction_from_response(response):\n",
        "    # get a list of the possible classes\n",
        "    classes_list = pt.get_class_list()\n",
        "\n",
        "    predictions = [True if category.lower() in response.lower() else False for category in classes_list]\n",
        "\n",
        "    # check if multiple classes were named, this is a prediction error\n",
        "    if Counter(predictions)[True] > 1:\n",
        "        return \"PredictionError\"\n",
        "\n",
        "    # check if exactly one class is named, this is the prediction\n",
        "    elif Counter(predictions)[True] == 1:\n",
        "        prediction = [category.lower() for category in classes_list if category.lower() in response.lower()]\n",
        "        return prediction[0]\n",
        "\n",
        "    # if no class is named, then this is a no prediction error\n",
        "    else:\n",
        "        return 'NoPrediction'\n",
        "\n",
        "\"\"\" Extract the promptfunction name \"\"\"\n",
        "def get_promptfunction_name(prompt_function):\n",
        "    string = f\"{prompt_function}\"\n",
        "    match = re.search(r'<function\\s+(\\w+)', string)\n",
        "    if match:\n",
        "        function_name = match.group(1)\n",
        "        return function_name\n",
        "    else:\n",
        "        return f\"{prompt_function}\"\n",
        "    \n",
        "\"\"\" Get the current time in the Netherlands \"\"\"\n",
        "def get_datetime():\n",
        "    current_datetime_utc = datetime.datetime.now(pytz.utc)\n",
        "\n",
        "    # Convert UTC time to Dutch time (CET)\n",
        "    dutch_timezone = pytz.timezone('Europe/Amsterdam')\n",
        "    current_datetime_dutch = current_datetime_utc.astimezone(dutch_timezone)\n",
        "    return current_datetime_dutch\n",
        "        \n",
        "\"\"\" Get the new runid \"\"\"\n",
        "def get_runid(path):\n",
        "\n",
        "    # if not first run, set runid to most recent run+1\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_pickle(path)\n",
        "        return max(df['run_id'])+1, df\n",
        "\n",
        "    # if first run, set runid to 0\n",
        "    else:\n",
        "        return 0, pd.DataFrame()\n",
        "    \n",
        "\"\"\" Save evaluation metrics of a run \"\"\"\n",
        "def update_overview_results(df, model_name, subset=None):\n",
        "    # df= dataframe with predictions for each do, one row per doc/prediction\n",
        "    # model_name = string with the name of the model\n",
        "    # subset = can be train, val, or test, or left open\n",
        " \n",
        "    # get evalaution scores\n",
        "    evaluation_dict = classification_report(df['label'], df['prediction'], output_dict=True)\n",
        "    evaluation = pd.DataFrame(evaluation_dict).transpose()\n",
        "    \n",
        "    new_row = {\n",
        "        # stuff about the run\n",
        "        'run_id':df.iloc[0]['run_id'],\n",
        "        'model':model_name,\n",
        "        'prompt_function':df.iloc[0]['prompt_function'],\n",
        "        'text_column':df.iloc[0]['text_column'],\n",
        "        'date': get_datetime(),\n",
        "        'runtime':sum(df['runtime']),\n",
        "        'set':subset,\n",
        "        'support':evaluation.iloc[-1]['support'],\n",
        "\n",
        "        # evaluation\n",
        "        'accuracy': evaluation_dict['accuracy'],\n",
        "\n",
        "        'recall_weighted_avg':evaluation.loc[evaluation.index=='weighted avg']['recall'].values[0],\n",
        "        'precision_weighted_avg': evaluation.loc[evaluation.index=='weighted avg']['precision'].values[0],\n",
        "        'f1_weighted_avg': evaluation.loc[evaluation.index=='weighted avg']['f1-score'].values[0],\n",
        "\n",
        "        'recall_macro_avg':evaluation.loc[evaluation.index=='macro avg']['recall'].values[0],\n",
        "        'precision_macro_avg': evaluation.loc[evaluation.index=='macro avg']['precision'].values[0],\n",
        "        'f1_macro_avg': evaluation.loc[evaluation.index=='macro avg']['f1-score'].values[0],\n",
        "\n",
        "\n",
        "        'recall_classes': dict(zip(evaluation.index[0:-3], evaluation['recall'][0:-3])),\n",
        "        'precision_classes': dict(zip(evaluation.index[0:-3], evaluation['precision'][0:-3])),\n",
        "        'f1_classes': dict(zip(evaluation.index[0:-3], evaluation['f1-score'][0:-3])),\n",
        "        'support_classes': dict(zip(evaluation.index[0:-3], evaluation['support'][0:-3])),\n",
        "\n",
        "        # docs that were predicted\n",
        "        'doc_paths':list(df['path'].values)\n",
        "        \n",
        "    }\n",
        "\n",
        "    # create a new dataframe with the evaluation, each run has one row\n",
        "    results = pd.DataFrame(columns=new_row.keys())\n",
        "    results.loc[len(results)] = new_row\n",
        "   \n",
        "    # if not the first run, get results from previous runs\n",
        "    path = f\"{cf.output_path}/overview_results.pkl\"\n",
        "    if os.path.exists(path):\n",
        "        earlier_results = pd.read_pickle(path)\n",
        "\n",
        "        # combine evaluation of previous runs with current run\n",
        "        results = pd.concat([earlier_results, results])\n",
        "\n",
        "    # save to overview_results.pkl\n",
        "    results.to_pickle(path)\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# update_overview_results(res, 'geitje')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# yeet = pd.read_pickle(f\"{cf.output_path}/overview_results.pkl\")\n",
        "# display(yeet)\n",
        "\n",
        "# yeet = pd.read_pickle(f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\")\n",
        "# display(yeet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "\"\"\" Given a dataframe with txt, return a df with predictions \"\"\"\n",
        "# docs_df = dataframe with the documents that need to be predicted\n",
        "# text_column = name of the column that includes the input_text. Can be different based on the text representation method. \n",
        "# prompt_function = prompt template -> ONLY prompt templates that take doc as input (ZERO SHOT)\n",
        "\n",
        "def zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function):\n",
        "    results_df = pd.DataFrame(columns = ['id', 'path', 'text_column', 'prompt_function', 'response', 'prediction', 'label', 'runtime', 'date'])\n",
        "    \n",
        "    # prompt each document\n",
        "    for index, row in docs_df.iterrows():\n",
        "        start_time = time.time()\n",
        "\n",
        "        # get the prompt, with the doc filled in\n",
        "        txt = row[text_column]\n",
        "        prompt = prompt_function(txt)\n",
        "\n",
        "        # prompt and get the response\n",
        "        converse = chatbot(Conversation(prompt))\n",
        "        response = converse[1]['content']\n",
        "\n",
        "        # extract prediction from response\n",
        "        prediction = get_prediction_from_response(response)\n",
        "\n",
        "        # save results in dataframe\n",
        "        results_df.loc[len(results_df)] = {\n",
        "            'id': row['id'],\n",
        "            'path' : row['path'],\n",
        "            'text_column' : text_column,\n",
        "            'prompt_function': get_promptfunction_name(prompt_function),\n",
        "            'response':response,\n",
        "            'prediction':prediction,\n",
        "            'label':row['label'].lower(),\n",
        "            'runtime':time.time()-start_time,\n",
        "            'date': get_datetime()\n",
        "        }\n",
        "    return results_df\n",
        "\n",
        "\"\"\" Run a prediction function -> can be ZeroShot or FewShot \"\"\"\n",
        "def run_prediction(docs_df, text_column, prompt_function, subset=None, learning='ZeroShot'):\n",
        "    if learning == 'ZeroShot':\n",
        "        # get the predictions\n",
        "        res = zero_shot_predictions_incontextlearning(docs_df, text_column, prompt_function)\n",
        "\n",
        "        # INSERT ELSE STATEMENT HERE FOR FEWSHOT\n",
        "\n",
        "        # get run_id\n",
        "        path = f\"{cf.output_path}/predictions/ICgeitje_predictions.pkl\"\n",
        "        res['run_id'], predictions_df = get_runid(path)\n",
        "\n",
        "        # combine earlier predictions with new ones\n",
        "        all_predictions = pd.concat([predictions_df, res])\n",
        "\n",
        "        # save predictions\n",
        "        all_predictions.to_pickle(path)\n",
        "\n",
        "        # save the evaluation metrics for each run\n",
        "        update_overview_results(res, 'Rijgersberg/GEITje-7B-chat-v2')\n",
        "        return res\n",
        "\n",
        "res = run_prediction(df, 'text_trunc', pt.simple_prompt)\n",
        "display(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "amsterdamincontextlearning"
    },
    "kernelspec": {
      "display_name": "2AmsterdamLLM",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "nl"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
