{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "# import my_secrets as sc\n",
    "# import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "Goal: repair responses for fine-tuned Mistral. The ideal response is: {'categorie': *category of the doc*}. However after training on 2-epochs or 3-epochs, Mistral's responses make the same mistake over again, it misses the openening curly bracket alot. \n",
    "\n",
    "*Previous notebook: GetPredictions*\n",
    "\n",
    "*Next notebook: baseline*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for ALL prediction files if predictions were correctly extracted\n",
    "The Regex pattern that's used to extract the prediction from the response has been updated, after running the experiments. I found out that the regex pattern, which just matches everything within {} was not strict enough, thus the pattern was adjusted to only match when 'categorie' is named within the curly brackets. This means all predictions need to be checked on errors when extracting with previous regex pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/') \n",
    "import prediction_helperfunctions as ph\n",
    "import prompt_template as pt\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from collections import Counter\n",
    "\n",
    "\"\"\" Check for each response if the extracted prediction would be the same for the old regex pattern and the new one. \"\"\"\n",
    "def check_predictions_using_new_regex(list_of_dfs):\n",
    "    errors = 0\n",
    "    for df in list_of_dfs:\n",
    "        df['new_prediction'] = df['response'].apply(ph.get_prediction_from_response)\n",
    "        if len(df.loc[df['new_prediction']!=df['prediction']]) > 0:\n",
    "            print(f\"Following run_id has responses of which the prediction was not correctly extracted using the old regex pattern: {set(df['run_id'])}.\")\n",
    "            print(f\"ACTION REQUIRED: the predictions need to be re-extracted from the response, this time using the new regex pattern. Don't forget to update the evaluation scores in the overview file.\")\n",
    "            errors += 1\n",
    "\n",
    "    if errors == 0:\n",
    "        print('All predictions from all dataframes were already correctly extracted!')\n",
    "\n",
    "    else:\n",
    "        print(f\"There are {errors} dataframes of which not all predictions were not alrady completely correctly extracted.\")\n",
    "\n",
    "\"\"\" Given the response, extract the prediction using the updated regex pattern \"\"\"\n",
    "def extract_predictions_using_new_regex(file_path):\n",
    "    df = pd.read_pickle(file_path)\n",
    "    df['prediction'] = df['response'].apply(ph.get_prediction_from_response)\n",
    "    df.to_pickle(file_path)\n",
    "       \n",
    "def check_multi_predictions(df):\n",
    "    errors = df.loc[df['prediction']=='MultiplePredictionErrorInFormatting']\n",
    "    print(len(errors))\n",
    "    for index, row in errors.iterrows():\n",
    "        print(\"NEW RESPONSE:\")\n",
    "        print(row['response'], '\\n')\n",
    "\n",
    "\"\"\" Check if response contains opening and closing brackets, and 'categorie' is named within them \"\"\"\n",
    "def match_complete_regex(response):\n",
    "    pattern = r'\\{[^{}]*categorie[^{}]*\\}'\n",
    "    matches = re.findall(pattern, response)\n",
    "    return len(matches)\n",
    "\n",
    "\"\"\" Check if response contains closing brackets, and 'categorie' is named \"\"\"\n",
    "def match_adjusted_regex(response):\n",
    "    pattern = r'[^{}]*categorie[^{}]*\\}'\n",
    "    matches = re.findall(pattern, response)\n",
    "    return len(matches)\n",
    "\n",
    "\n",
    "\"\"\" Given the string response, extract the prediction using adjusted regex pattern (no opening bracket) \"\"\"\n",
    "def get_prediction_from_response(response):\n",
    "    # get a list of the possible classes\n",
    "    classes_list = pt.get_class_list()\n",
    "    \n",
    "    # check if part of string matches given output format to prompt\n",
    "    # pattern = r'\\{[^{}]+\\}'\n",
    "    pattern = r'[^{}]*categorie[^{}]*\\}'\n",
    "    matches = re.findall(pattern, response)\n",
    "    if len(matches) == 1:\n",
    "        prediction_output = matches[0]\n",
    "        predictions = [True if category.lower() in prediction_output.lower() else False for category in classes_list]\n",
    "\n",
    "        # check if multiple classes were named, this is a prediction error\n",
    "        if Counter(predictions)[True] > 1:\n",
    "            return \"MultiplePredictionErrorInOutput\"\n",
    "\n",
    "        # check if exactly one class is named, this is the prediction\n",
    "        elif Counter(predictions)[True] == 1:\n",
    "            prediction = [category.lower() for category in classes_list if category.lower() in prediction_output.lower()]\n",
    "            return prediction[0]\n",
    "\n",
    "        # if no class is named, then this is a no prediction error\n",
    "        else:\n",
    "            return 'NoPredictionInOutput'\n",
    "        \n",
    "    elif len(matches) > 1:\n",
    "        return 'MultiplePredictionErrorInFormatting'\n",
    "    else:\n",
    "        return 'NoPredictionFormat'\n",
    "    \n",
    "\"\"\" Calculate evaluation scores give preditions \"\"\"\n",
    "# avrg = either 'weighted' or 'macro'\n",
    "def calculate_evaluation_metrics(predictions, prediction_col, avrg, print_statement=False):\n",
    "    y_true = predictions['label']\n",
    "    y_pred = predictions[prediction_col]\n",
    "\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=avrg)\n",
    "    recall = recall_score(y_true, y_pred, average=avrg)\n",
    "    f1 = f1_score(y_true, y_pred, average=avrg)\n",
    "\n",
    "    if print_statement == True:\n",
    "        print(report)\n",
    "\n",
    "    return accuracy, precision, recall, f1, report\n",
    "\n",
    "\"\"\" Given the new predictions, update the evaluation scores in the overview file \"\"\"\n",
    "def update_scores_in_overview(overview_path, predictions, run_id):\n",
    "    accuracy, weighted_precision, weighted_recall, weighted_f1, report = calculate_evaluation_metrics(predictions, 'prediction', 'weighted')\n",
    "    accuracy, macro_precision, macro_recall, macro_f1, report = calculate_evaluation_metrics(predictions, 'prediction', 'macro')\n",
    "\n",
    "    overview = pd.read_pickle(overview_path)\n",
    "\n",
    "    overview.loc[overview['run_id']==run_id, 'accuracy'] = accuracy\n",
    "    overview.loc[overview['run_id']==run_id, 'macro_avg_precision'] = macro_precision\n",
    "    overview.loc[overview['run_id']==run_id, 'macro_avg_recall'] = macro_recall\n",
    "    overview.loc[overview['run_id']==run_id, 'macro_avg_f1'] = macro_f1\n",
    "    overview.loc[overview['run_id']==run_id, 'weighted_avg_precision'] = weighted_precision\n",
    "    overview.loc[overview['run_id']==run_id, 'weighted_avg_recall'] = weighted_recall\n",
    "    overview.loc[overview['run_id']==run_id, 'weighted_avg_f1'] = weighted_f1\n",
    "    overview.loc[overview['run_id']==run_id, 'classification_report'] = report\n",
    "\n",
    "    overview.to_pickle(overview_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GEITje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All predictions from all dataframes were already correctly extracted!\n",
      "All predictions from all dataframes were already correctly extracted!\n"
     ]
    }
   ],
   "source": [
    "# IC\n",
    "g_fewshot = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/GEITje/fewshot_prompt_no_template/First200Last0Predictions.pkl\")\n",
    "g_ic_100 = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/GEITje/zeroshot_prompt_geitje/First100Last0Predictions.pkl\")\n",
    "g_ic_200 = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/GEITje/zeroshot_prompt_geitje/First200Last0Predictions.pkl\")\n",
    "g_ic_100_100 = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/GEITje/zeroshot_prompt_geitje/First100Last100Predictions.pkl\")\n",
    "check_predictions_using_new_regex([g_fewshot, g_ic_100, g_ic_200, g_ic_100_100])\n",
    "\n",
    "# FT\n",
    "g_1ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/1epochs/GEITjeFirst200Last0Predictions.pkl\")\n",
    "g_2ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/2epochs/GEITjeFirst200Last0Predictions.pkl\")\n",
    "g_3ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/3epochs/GEITjeFirst200Last0Predictions.pkl\")\n",
    "check_predictions_using_new_regex([g_1ep, g_2ep, g_3ep])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All predictions from all dataframes were already correctly extracted!\n",
      "All predictions from all dataframes were already correctly extracted!\n"
     ]
    }
   ],
   "source": [
    "# IC\n",
    "l_fewshot = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/Llama/fewshot_prompt_with_template/First200Last0Predictions.pkl\")\n",
    "l_ic_100 = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/Llama/zeroshot_prompt_mistral_llama/First100Last0Predictions.pkl\")\n",
    "l_ic_200 = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/Llama/zeroshot_prompt_mistral_llama/First200Last0Predictions.pkl\")\n",
    "l_ic_100_100 = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/Llama/zeroshot_prompt_mistral_llama/First100Last100Predictions.pkl\")\n",
    "check_predictions_using_new_regex([l_fewshot, l_ic_100, l_ic_200, l_ic_100_100])\n",
    "\n",
    "# FT\n",
    "l_1ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/1epochs/LlamaFirst200Last0Predictions.pkl\")\n",
    "l_2ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/2epochs/LlamaFirst200Last0Predictions.pkl\")\n",
    "l_3ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/3epochs/LlamaFirst200Last0Predictions.pkl\")\n",
    "check_predictions_using_new_regex([l_1ep, l_2ep, l_3ep])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All predictions from all dataframes were already correctly extracted!\n",
      "Following run_id has responses of which the prediction was not correctly extracted using the old regex pattern: {'FT_AmsterdamDocClassificationMistral200T2Epochszeroshot_prompt_mistral_llamaLlamaTokens200_0traintest_numEx0'}.\n",
      "ACTION REQUIRED: the predictions need to be re-extracted from the response, this time using the new regex pattern. Don't forget to update the evaluation scores in the overview file.\n",
      "Following run_id has responses of which the prediction was not correctly extracted using the old regex pattern: {'FT_AmsterdamDocClassificationMistral200T3Epochszeroshot_prompt_mistral_llamaLlamaTokens200_0traintest_numEx0'}.\n",
      "ACTION REQUIRED: the predictions need to be re-extracted from the response, this time using the new regex pattern. Don't forget to update the evaluation scores in the overview file.\n",
      "There are 2 dataframes of which not all predictions were not alrady completely correctly extracted.\n"
     ]
    }
   ],
   "source": [
    "# IC\n",
    "m_fewshot = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/Mistral/fewshot_prompt_with_template/First200Last0Predictions.pkl\")\n",
    "m_ic_100 = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/Mistral/zeroshot_prompt_mistral_llama/First100Last0Predictions.pkl\")\n",
    "m_ic_200 = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/Mistral/zeroshot_prompt_mistral_llama/First200Last0Predictions.pkl\")\n",
    "m_ic_100_100 = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/Mistral/zeroshot_prompt_mistral_llama/First100Last100Predictions.pkl\")\n",
    "check_predictions_using_new_regex([m_fewshot, m_ic_100, m_ic_200, m_ic_100_100])\n",
    "\n",
    "# FT\n",
    "m_1ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/1epochs/MistralFirst200Last0Predictions.pkl\")\n",
    "m_2ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/2epochs/MistralFirst200Last0Predictions.pkl\")\n",
    "m_3ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/3epochs/MistralFirst200Last0Predictions.pkl\")\n",
    "check_predictions_using_new_regex([m_1ep, m_2ep, m_3ep])\n",
    "\n",
    "# We originally found that m_2ep had incorrectly extracted predictions, below is the code to replace the original predictions column, with the new predictions extracted using the new/correct regex pattern\n",
    "# extract_predictions_using_new_regex(f\"{cf.output_path}/predictionsFinal/finetuning/2epochs/MistralFirst200Last0Predictions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check responses without opening bracket\n",
    "Fine-tuned Mistral has problems with returning the format output, and return the format without the opening bracket, resulting in many prediction errors. \n",
    "\n",
    "Below we can see that the responses for 1epoch were correctly formatted and thus extracted.\n",
    "\n",
    "However, 2 and 3 epoch is very problematic. The response that only miss { will also be extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/1epochs/MistralFirst200Last0Predictions.pkl\")\n",
    "m_2ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/2epochs/ORIGINALMistralFirst200Last0Predictions.pkl\")\n",
    "m_3ep = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/3epochs/ORIGINALMistralFirst200Last0Predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1 has 0 responses that do not start with {\n",
      " Epoch 1 has 0 predictions that are NoPredictionFormat errors.\n",
      " Epoch 2 has 200 responses that do not start with {\n",
      " Epoch 2 has 200 predictions that are NoPredictionFormat errors.\n",
      " Epoch 3 has 1100 responses that do not start with {\n",
      " Epoch 3 has 1097 predictions that are NoPredictionFormat errors.\n"
     ]
    }
   ],
   "source": [
    "print(f\" Epoch 1 has {len(m_1ep[~m_1ep['response'].str.startswith('{')])} responses that do not start with {'{'}\")\n",
    "print(f\" Epoch 1 has {len(m_1ep.loc[m_1ep['prediction']=='NoPredictionFormat'])} predictions that are NoPredictionFormat errors.\")\n",
    "print(f\" Epoch 2 has {len(m_2ep[~m_2ep['response'].str.startswith('{')])} responses that do not start with {'{'}\")\n",
    "print(f\" Epoch 2 has {len(m_2ep.loc[m_2ep['prediction']=='NoPredictionFormat'])} predictions that are NoPredictionFormat errors.\")\n",
    "pred3 = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/3epochs/MistralFirst200Last0Predictions.pkl\")\n",
    "print(f\" Epoch 3 has {len(m_3ep[~m_3ep['response'].str.startswith('{')])} responses that do not start with {'{'}\")\n",
    "print(f\" Epoch 3 has {len(m_3ep.loc[m_3ep['prediction']=='NoPredictionFormat'])} predictions that are NoPredictionFormat errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of responses that include format, but format misses {: 0\n",
      "Amount of formats in responses (adjusted_regex): Counter({1: 1100})\n",
      "Amount of responses that already followed format (complete_regex): 1100\n",
      "Amount of responses that include format, but format misses {: 199\n",
      "Amount of formats in responses (adjusted_regex): Counter({1: 199, 0: 1})\n",
      "Amount of responses that already followed format (complete_regex): 0\n",
      "Amount of responses that include format, but format misses {: 981\n",
      "Amount of formats in responses (adjusted_regex): Counter({1: 982, 0: 116, 2: 2})\n",
      "Amount of responses that already followed format (complete_regex): 3\n"
     ]
    }
   ],
   "source": [
    "m_1ep['matches_complete_regex'] = m_1ep['response'].apply(match_complete_regex)\n",
    "m_1ep['matches_adjusted_regex'] = m_1ep['response'].apply(match_adjusted_regex)\n",
    "print(\"Amount of responses that include format, but format misses {:\",len(m_1ep.loc[(m_1ep['matches_complete_regex']==0) & (m_1ep['matches_adjusted_regex']>0)]))\n",
    "print(f\"Amount of formats in responses (adjusted_regex): {Counter(m_1ep['matches_adjusted_regex'])}\")\n",
    "print(\"Amount of responses that already followed format (complete_regex):\", len(m_1ep.loc[m_1ep['matches_complete_regex']>0]))\n",
    "\n",
    "m_2ep['matches_complete_regex'] = m_2ep['response'].apply(match_complete_regex)\n",
    "m_2ep['matches_adjusted_regex'] = m_2ep['response'].apply(match_adjusted_regex)\n",
    "print(\"Amount of responses that include format, but format misses {:\",len(m_2ep.loc[(m_2ep['matches_complete_regex']==0) & (m_2ep['matches_adjusted_regex']>0)]))\n",
    "print(f\"Amount of formats in responses (adjusted_regex): {Counter(m_2ep['matches_adjusted_regex'])}\")\n",
    "print(\"Amount of responses that already followed format (complete_regex):\", len(m_2ep.loc[m_2ep['matches_complete_regex']>0]))\n",
    "\n",
    "m_3ep['matches_complete_regex'] = m_3ep['response'].apply(match_complete_regex)\n",
    "m_3ep['matches_adjusted_regex'] = m_3ep['response'].apply(match_adjusted_regex)\n",
    "print(\"Amount of responses that include format, but format misses {:\",len(m_3ep.loc[(m_3ep['matches_complete_regex']==0) & (m_3ep['matches_adjusted_regex']>0)]))\n",
    "print(f\"Amount of formats in responses (adjusted_regex): {Counter(m_3ep['matches_adjusted_regex'])}\")\n",
    "print(\"Amount of responses that already followed format (complete_regex):\", len(m_3ep.loc[m_3ep['matches_complete_regex']>0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete regex = {categorie}  -> match opening an closing brackets, that include the word categorie.\n",
    "- Adjusted regex = categorie}  -> match everything between categorie and closing bracket.\n",
    "\n",
    "All models were checked for NoPredictionFormat errors. We found that Llama and Mistral did not have them for any of the epochs. Mistral, however, does for epoch=2&3.\n",
    "Above we can see that the original predictions (extracted using complete regex), results in many NoPredictionFormat errors. Next, we seee that many of those response do match with the adjusted regex, but not with the complete regex, meaning that only the beginning bracket is missing. Thus we create a new column with new prediction, of which the predictions are extarcted using the adjusted regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repair predictions. For this we use the new regex pattern, with a minor adjustment. We match instead of '{categorie }', we match 'categorie }'. \n",
    "# So we do not require the opening bracket.\n",
    "m_2ep.rename(columns={'prediction':'Original_Prediction'}, inplace=True)\n",
    "m_3ep.rename(columns={'prediction':'Original_Prediction'}, inplace=True)\n",
    "\n",
    "m_2ep['prediction'] = m_2ep['response'].apply(get_prediction_from_response)\n",
    "m_3ep['prediction'] = m_3ep['response'].apply(get_prediction_from_response)\n",
    "\n",
    "# save repaired predictions\n",
    "# m_2ep.to_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/2epochs/MistralFirst200Last0Predictions.pkl\")\n",
    "# m_3ep.to_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/3epochs/MistralFirst200Last0Predictions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to adjust the evaluation scores in the overview file to match the new predictions. We save the old scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_path = f\"{cf.output_path}/predictionsFinal/finetuning/3epochs/overview.pkl\"\n",
    "predictions = pd.read_pickle(f\"{cf.output_path}/predictionsFinal/finetuning/3epochs/MistralFirst200Last0Predictions.pkl\")\n",
    "run_id = 'FT_AmsterdamDocClassificationMistral200T3Epochszeroshot_prompt_mistral_llamaLlamaTokens200_0traintest_numEx0'\n",
    "# update_scores_in_overview(overview_path, predictions, run_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2AmsterdamLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
