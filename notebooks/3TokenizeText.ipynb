{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "- Get insight into tokenizer, tokens and doc lengths.\n",
    "- Test different text truncation thresholds on the baseline.\n",
    "\n",
    "*Previous notebook: clean_data*\n",
    "\n",
    "*Next notebook: FinetuningDataFormatting*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text truncation -- overview in tokenizer/doc lengths\n",
    "- tokenize text using tokenizer of mistral, geitje and Llama.\n",
    "- Check if mistral and geitje indeed have the same tokenizer.\n",
    "- After getting the tokens, check distribution.\n",
    "- Truncate text and test multiple thresholds on baseline\n",
    "\n",
    "Results are saved in txtfiles_tokenizer.pkl, so that txtfiles.pkl is a back-up file, in case anything gets messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\" Add column with tokenizes tokens using the the models tokenizer \"\"\"\n",
    "def add_tokenized_tokens_column(model_name, df, text_col, new_col_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    all_texts = list(df[text_col].values)\n",
    "\n",
    "    all_tokens = []\n",
    "    all_tokens_len = []\n",
    "    for txt in all_texts:\n",
    "        tokens = tokenizer.tokenize(txt)\n",
    "        all_tokens.append(tokens)\n",
    "        all_tokens_len.append(len(tokens))\n",
    "\n",
    "    df[new_col_name] = all_tokens\n",
    "    df[f\"count_{new_col_name}\"] = all_tokens_len\n",
    "    # df.to_pickle(save_to_path)\n",
    "    return df\n",
    "\n",
    "\"\"\" Calculate fraction of tokens that exceeds a certain amount of tokens \"\"\"\n",
    "def fraction_token(df, max_token, token_len_col):\n",
    "    for col in token_len_col:\n",
    "        print(f\"{len(df.loc[df[col]>max_token])} out of {len(df)} ({round(len(df.loc[df[col]>max_token])/len(df)*100, 2)}%) docs exceed a token length of {max_token}\")\n",
    "\n",
    "    for col in token_len_col:\n",
    "        print(df[col].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before: ['label', 'path', 'id', 'text', 'tokens', 'token_count', 'clean_tokens', 'clean_tokens_count', 'num_pages', 'clean_text', '4split', '2split', 'MistralTokens', 'count_MistralTokens', 'LlamaTokens', 'count_LlamaTokens', 'old_label', 'md5_hash', 'balanced_split', 'GEITjeTokens', 'count_GEITjeTokens']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after: ['label', 'path', 'id', 'text', 'tokens', 'token_count', 'clean_tokens', 'clean_tokens_count', 'num_pages', 'clean_text', '4split', '2split', 'MistralTokens', 'count_MistralTokens', 'LlamaTokens', 'count_LlamaTokens', 'old_label', 'md5_hash', 'balanced_split', 'GEITjeTokens', 'count_GEITjeTokens']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Columns before: {list(df.columns)}\")\n",
    "\"\"\"GEITje and Mistral \"\"\" # same tokenizer for both models\n",
    "df = add_tokenized_tokens_column('mistralai/Mistral-7B-v0.1', df, 'text', 'MistralTokens')\n",
    "\n",
    "\"\"\"Llama\"\"\"\n",
    "df = add_tokenized_tokens_column('meta-llama/Llama-2-7b-hf', df, 'text', 'LlamaTokens')\n",
    "\n",
    "print(f\"Columns after: {list(df.columns)}\")\n",
    "\n",
    "# df.to_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse token length of model tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2903 out of 20818 (13.94%) docs exceed a token length of 4096\n",
      "2797 out of 20818 (13.44%) docs exceed a token length of 4096\n",
      "count     20818.000000\n",
      "mean       4491.198434\n",
      "std       15975.578413\n",
      "min          75.000000\n",
      "25%         630.000000\n",
      "50%        1058.000000\n",
      "75%        2455.000000\n",
      "max      621995.000000\n",
      "Name: count_MistralTokens, dtype: float64\n",
      "count     20818.000000\n",
      "mean       4340.207897\n",
      "std       15456.312555\n",
      "min          74.000000\n",
      "25%         612.000000\n",
      "50%        1031.000000\n",
      "75%        2378.000000\n",
      "max      618067.000000\n",
      "Name: count_LlamaTokens, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fraction_token(df, 4096, ['count_MistralTokens', 'count_LlamaTokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test text truncation on baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "# load file with baseline function\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import baseline as bf\n",
    "\n",
    "# load file with truncation function\n",
    "from truncation import add_truncation_column\n",
    "\n",
    "# variables for text truncation\n",
    "DATAFRAME = tok\n",
    "TEXT_COL = 'text'\n",
    "TOKENS_COL = 'LlamaTokens'\n",
    "\n",
    "# variables for baseline\n",
    "BASELINE_FUNCTION = MultinomialNB()\n",
    "MODEL_NAME = 'MultinomialNB'\n",
    "TRAIN_SET = 'train' # must be dev or train\n",
    "TEST_SET = 'test' # must be val or test\n",
    "SPLIT_COLUMN = '4split' #must be either 2split or 4split. 2split = data split into train and test. 4split = data split into train, test, dev and val. \n",
    "LABEL_COLUMN = 'label'\n",
    "PREDICTION_PATH = f\"{cf.output_path}/predictions/baselineTruncationPredictions.pkl\"\n",
    "OVERVIEW_PATH = f\"{cf.output_path}/overview/baselineTruncationOverview.pkl\"\n",
    "# PREDICTION_PATH = f\"{cf.output_path}/predictions/tryoutBaselineTruncationPredictions.pkl\"\n",
    "# OVERVIEW_PATH = f\"{cf.output_path}/overview/tryoutBaselineTruncationOverview.pkl\"\n",
    "TRUNC_COLUMN = 'trunc_txt'\n",
    "threshold_combinations =[(100,0), (200,0), (500,0), (1000,0), (2000,0), (100,100),(200,200), (500,500), (1000,1000), (0,100), (0,200), (0,500), (0,1000), (0,2000)]\n",
    "threshold_combinations =[(200,200), (500,500), (1000,1000), (0,100), (0,200), (0,500), (0,1000), (0,2000)]\n",
    "\n",
    "# threshold_combinations = [(100,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all thresholds and save predictions\n",
    "for thresholds in threshold_combinations:\n",
    "    front_threshold = thresholds[0]\n",
    "    back_threshold = thresholds[1]\n",
    "    trunc = add_truncation_column(DATAFRAME, TEXT_COL, TOKENS_COL, front_threshold,back_threshold)\n",
    "    bf.run_baseline(BASELINE_FUNCTION,MODEL_NAME, trunc, SPLIT_COLUMN, TRAIN_SET, TEST_SET, TRUNC_COLUMN, LABEL_COLUMN, PREDICTION_PATH, OVERVIEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "PREDICTION_PATH = f\"{cf.output_path}/predictions/baselineTruncationPredictions.pkl\"\n",
    "OVERVIEW_PATH = f\"{cf.output_path}/overview/baselineTruncationOverview.pkl\"\n",
    "\n",
    "yeet = pd.read_pickle(OVERVIEW_PATH)\n",
    "yeet = yeet.sort_values(by=['macro_avg_f1', 'accuracy'], ascending=False)\n",
    "display(yeet)\n",
    "\n",
    "bl = pd.read_pickle(f\"{cf.output_path}/overview/baselineOverview.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ls = yeet.loc[yeet['model']=='LinearSVC']\n",
    "ls = pd.concat([bl.loc[bl['model']=='LinearSVC'], yeet.loc[yeet['model']=='LinearSVC']]).sort_values(by=['macro_avg_f1', 'accuracy'], ascending=False)\n",
    "ls = ls.loc[~ls['text_col'].isin(['TruncationLlamaTokensFront2000Back0', 'TruncationLlamaTokensFront1000Back0', 'TruncationLlamaTokensFront1000Back1000', 'TruncationLlamaTokensFront0Back2000',''])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = yeet.loc[yeet['model']=='MultinomialNB']\n",
    "# nb = nb.loc[~nb['text_col'].isin(['TruncationLlamaTokensFront2000Back0', 'TruncationLlamaTokensFront1000Back0', 'TruncationLlamaTokensFront1000Back1000', 'TruncationLlamaTokensFront0Back2000',''])]\n",
    "display(nb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best cut-off point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DummyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
