{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "- Get insight into tokenizer, tokens and doc lengths.\n",
    "- Test different text truncation thresholds on the baseline.\n",
    "\n",
    "*Previous notebook: clean_data*\n",
    "\n",
    "*Next notebook: FinetuningDataFormatting*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text truncation -- overview in tokenizer/doc lengths\n",
    "- tokenize text using tokenizer of mistral, geitje and Llama.\n",
    "- Check if mistral and geitje indeed have the same tokenizer.\n",
    "- After getting the tokens, check distribution.\n",
    "- Truncate text and test multiple thresholds on baseline\n",
    "\n",
    "Results are saved in txtfiles_tokenizer.pkl, so that txtfiles.pkl is a back-up file, in case anything gets messed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(f\"{cf.output_path}/txtfiles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(f\"{cf.output_path}/txtfiles_notcleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\" Add column with tokenizes tokens using the the models tokenizer \"\"\"\n",
    "def add_tokenized_tokens_column(model_name, df, text_col, new_col_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    all_texts = list(df[text_col].values)\n",
    "\n",
    "    all_tokens = []\n",
    "    all_tokens_len = []\n",
    "    for txt in all_texts:\n",
    "        tokens = tokenizer.tokenize(txt)\n",
    "        all_tokens.append(tokens)\n",
    "        all_tokens_len.append(len(tokens))\n",
    "\n",
    "    df[new_col_name] = all_tokens\n",
    "    df[f\"count_{new_col_name}\"] = all_tokens_len\n",
    "    # df.to_pickle(save_to_path)\n",
    "    return df\n",
    "\n",
    "\"\"\" Calculate fraction of tokens that exceeds a certain amount of tokens \"\"\"\n",
    "def fraction_token(df, max_token, token_len_col):\n",
    "    for col in token_len_col:\n",
    "        print(f\"{len(df.loc[df[col]>max_token])} out of {len(df)} ({round(len(df.loc[df[col]>max_token])/len(df)*100, 2)}%) docs exceed a token length of {max_token}\")\n",
    "\n",
    "    for col in token_len_col:\n",
    "        print(df[col].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before: ['label', 'path', 'id', 'text', 'tokens', 'token_count', 'clean_tokens', 'clean_tokens_count', 'pdf_path', 'num_pages']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after: ['label', 'path', 'id', 'text', 'tokens', 'token_count', 'clean_tokens', 'clean_tokens_count', 'pdf_path', 'num_pages', 'LlamaTokens', 'count_LlamaTokens']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Columns before: {list(df.columns)}\")\n",
    "\"\"\"GEITje and Mistral \"\"\" # same tokenizer for both models\n",
    "# df = add_tokenized_tokens_column('mistralai/Mistral-7B-v0.1', df, 'text', 'MistralTokens')\n",
    "\n",
    "\"\"\"Llama\"\"\"\n",
    "df = add_tokenized_tokens_column('meta-llama/Llama-2-7b-hf', df, 'text', 'LlamaTokens')\n",
    "\n",
    "print(f\"Columns after: {list(df.columns)}\")\n",
    "\n",
    "# df.to_pickle(f\"{cf.output_path}/txtfiles.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(f\"{cf.output_path}/txtfiles_notcleaned_llama.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse token length of model tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2903 out of 20818 (13.94%) docs exceed a token length of 4096\n",
      "2797 out of 20818 (13.44%) docs exceed a token length of 4096\n",
      "count     20818.000000\n",
      "mean       4491.198434\n",
      "std       15975.578413\n",
      "min          75.000000\n",
      "25%         630.000000\n",
      "50%        1058.000000\n",
      "75%        2455.000000\n",
      "max      621995.000000\n",
      "Name: count_MistralTokens, dtype: float64\n",
      "count     20818.000000\n",
      "mean       4340.207897\n",
      "std       15456.312555\n",
      "min          74.000000\n",
      "25%         612.000000\n",
      "50%        1031.000000\n",
      "75%        2378.000000\n",
      "max      618067.000000\n",
      "Name: count_LlamaTokens, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fraction_token(df, 4096, ['count_MistralTokens', 'count_LlamaTokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DummyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
