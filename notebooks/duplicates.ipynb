{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "goal: check data for duplicates\n",
    "\n",
    "- All old data is moved into duplicates folder in blobfuse.\n",
    "- Save new df with duplicates removed, bases on the MB5 hash. \n",
    "- Data is re-split into subsets (train, test, val and dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles_tokenizer = pd.read_pickle(f\"{cf.output_path}/duplicates/txtfiles_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5886 docs removed. New total: 20818 docs.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import sys\n",
    "\n",
    "# load function to split data into subsets (train,test,val and dev)\n",
    "sys.path.append('../scripts/') \n",
    "from data_split import save_split\n",
    "\n",
    "def calculate_md5(file_path):\n",
    "    \"\"\"Calculate the MD5 hash of a file.\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "\"\"\"Calculate MD5 hash for each file path in a DataFrame.\"\"\"\n",
    "def drop_duplicates(df, file_path_column):\n",
    "    df['md5_hash'] = df[file_path_column].apply(calculate_md5)\n",
    "\n",
    "    # remove rows with duplicate md5_hash\n",
    "    clean_df = df.drop_duplicates(subset=['md5_hash'])\n",
    "    print(f\"{len(df)-len(clean_df)} docs removed. New total: {len(clean_df)} docs.\")\n",
    "\n",
    "    # redo data split\n",
    "    clean_df = save_split(clean_df)\n",
    "    return clean_df\n",
    "\n",
    "# output_file = f\"{cf.output_path}/hash_output.txt\"\n",
    "hash_df = drop_duplicates(txtfiles_tokenizer, 'path')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_df.to_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
