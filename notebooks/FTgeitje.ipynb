{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "# import my_secrets as sc\n",
    "import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# setup environment GEITje-7B Finetuning\n",
    "# - pip install torch\n",
    "# - pip install datasets\n",
    "# - pip install transformers\n",
    "# - pip install trl\n",
    "# - pip install accelerate (restart after)\n",
    "# - switch device_map='auto' to avaoid memory error\n",
    "\n",
    "# - pip install sentencepiece\n",
    "# - pip install jupyter\n",
    "# - pip install protobuf \n",
    "# pip install bitsandbytes\n",
    "# pip install bnb\n",
    "# pip install wandb==0.13.3 --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af99bebaad84ef79e57df3fb7b64013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning GEITje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131e070d88d4492bac23927c40549f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb14b3608c5e4fc3ab8e4a48771b3afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.63M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841707cb32c3436e9a35e85af9ed578b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d386fbb5c3a4706baab0545eaee737c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/251k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63d32eb8c93422ba170db720f181747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/68.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4175a03d50e474da168b4c1ec9b6662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e59bbcbfdd043a8ae71c680a715c78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e655df6ef1c4eabbcc4d0ab81d562d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba79ca3b69a4bd185de0cb9931c9110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split:   0%|          | 0/209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load GEITje formatted data\n",
    "from datasets import load_dataset\n",
    "\n",
    "chat_dataset = load_dataset('FemkeBakker/AmsterdamGEITjeFormat200Tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"\\n    Classificeer het document in één van de categoriën.\\n    Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}\\n    \\n    Categoriën: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']\\n    \\n    Document: \\n    > Gemeente\\nAmsterdam\\nMotie\\nDatum raadsvergadering 19 juli 2023\\nIngekomen onder nummer 456\\nStatus Aangenomen\\nOnderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\\nKabamba en Krom inzake een grote schoonmaak voor iedereen op de\\nwachtlijst Hulp bij Huishouden\\nOnderwerp\\nEen grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\\nAan de gemeenteraad\\nOndergetekenden hebben de eer voor te stellen:\\nDe Raad,\\nGehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\\nConstaterende dat:\\ne \\n\\n    Vul in met de categorie van het document: {'categorie': ??}     \\n    \",\n",
       "  'role': 'user'},\n",
       " {'content': \"{'categorie': Motie}\", 'role': 'assistant'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_dataset['train']['message'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_dataset[\"train\"]=chat_dataset[\"train\"].select(range(2))\n",
    "# chat_dataset[\"test\"]=chat_dataset[\"test\"].select(range(2))\n",
    "# chat_dataset[\"val\"]=chat_dataset[\"val\"].select(range(2))\n",
    "# chat_dataset[\"dev\"]=chat_dataset[\"dev\"].select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77bc6cdcdf240a9aa412d9dbcbf3977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef84e06ba654349bd000c1bd3f43488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "# basemodel_name = 'mistral-community/Mistral-7B-v0.2'\n",
    "# basemodel_name = \"stabilityai/stablelm-2-1_6b\"\n",
    "# basemodel_name = 'meta-llama/Llama-2-7b-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel_name, torch_dtype=torch.bfloat16,\n",
    "                                                low_cpu_mem_usage=True, attn_implementation=\"sdpa\",\n",
    "                                                device_map='cpu')\n",
    "\n",
    "# tokenizer_model = basemodel_name\n",
    "tokenizer_model = 'mistral-community/Mistral-7B-v0.2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "model.config.pad_token_id = tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>[INST] Classificeer het document in één van de categoriën.\\n    Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}\\n    \\n    Categoriën: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']\\n    \\n    Document: \\n    > Gemeente\\nAmsterdam\\nMotie\\nDatum raadsvergadering 19 juli 2023\\nIngekomen onder nummer 456\\nStatus Aangenomen\\nOnderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\\nKabamba en Krom inzake een grote schoonmaak voor iedereen op de\\nwachtlijst Hulp bij Huishouden\\nOnderwerp\\nEen grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\\nAan de gemeenteraad\\nOndergetekenden hebben de eer voor te stellen:\\nDe Raad,\\nGehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\\nConstaterende dat:\\ne \\n\\n    Vul in met de categorie van het document: {'categorie': ??} [/INST] {'categorie': Motie} </s>\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(chat_dataset['train']['message'][0], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff051d3531a748caa3dc53818b3f432b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bcfd123dd74032858da15b673e3b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "basemodel_name = 'mistral-community/Mistral-7B-v0.2'\n",
    "# basemodel_name = \"stabilityai/stablelm-2-1_6b\"\n",
    "# basemodel_name = 'meta-llama/Llama-2-7b-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel_name, torch_dtype=torch.bfloat16,\n",
    "                                                low_cpu_mem_usage=True, attn_implementation=\"sdpa\",\n",
    "                                                device_map='cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(basemodel_name)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "model.config.pad_token_id = tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>[INST] Classificeer het document in één van de categoriën.\\n    Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}\\n    \\n    Categoriën: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']\\n    \\n    Document: \\n    > Gemeente\\nAmsterdam\\nMotie\\nDatum raadsvergadering 19 juli 2023\\nIngekomen onder nummer 456\\nStatus Aangenomen\\nOnderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\\nKabamba en Krom inzake een grote schoonmaak voor iedereen op de\\nwachtlijst Hulp bij Huishouden\\nOnderwerp\\nEen grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\\nAan de gemeenteraad\\nOndergetekenden hebben de eer voor te stellen:\\nDe Raad,\\nGehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\\nConstaterende dat:\\ne \\n\\n    Vul in met de categorie van het document: {'categorie': ??} [/INST] {'categorie': Motie} </s>\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(chat_dataset['train']['message'][0], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def combine_and_save_df(model_df, save_to_path):\n",
    "    \n",
    "    # combine with earlier runs if exists\n",
    "    if os.path.exists(save_to_path):\n",
    "        original = pd.read_pickle(save_to_path)\n",
    "        model_df = pd.concat([original, model_df])\n",
    "\n",
    "    model_df.to_pickle(save_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|user|>\\n\\n    Classificeer het document in één van de categoriën.\\n    Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}\\n    \\n    Categoriën: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']\\n    \\n    Document: \\n    > Gemeente\\nAmsterdam\\nMotie\\nDatum raadsvergadering 19 juli 2023\\nIngekomen onder nummer 456\\nStatus Aangenomen\\nOnderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\\nKabamba en Krom inzake een grote schoonmaak voor iedereen op de\\nwachtlijst Hulp bij Huishouden\\nOnderwerp\\nEen grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\\nAan de gemeenteraad\\nOndergetekenden hebben de eer voor te stellen:\\nDe Raad,\\nGehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\\nConstaterende dat:\\ne \\n\\n    Vul in met de categorie van het document: {'categorie': ??}     \\n    </s>\\n<|assistant|>\\n{'categorie': Motie}</s>\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(chat_dataset['train']['message'][0], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import prediction_helperfunctions as ph\n",
    "\n",
    "def train(model, model_name, tokenizer, chat_dataset, chat_dataset_name, new_model_name, train_set, test_set, run_id='No_id', save_to_hub=True, resume=False):\n",
    "    start_time = time.time()\n",
    "    def format(examples):\n",
    "        return [tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "                for conversation in examples['message']]\n",
    "\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    steps_per_epoch = len(chat_dataset[train_set])\\\n",
    "                // (torch.cuda.device_count() * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = steps_per_epoch // 5\n",
    "\n",
    "    output_directory = f'{cf.output_path}/finetuning_output/GEITjeSmallData200Tokens_finetuning_output'\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        optim='adamw_bnb_8bit',\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy='epoch',\n",
    "        bf16=False, #bf16=True require CUDA 11 -> original code bf16=True\n",
    "        output_dir=output_directory,\n",
    "        report_to=[\"tensorboard\", 'wandb'],\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        hub_model_id=new_model_name,\n",
    "        push_to_hub=True,\n",
    "        hub_private_repo=True,\n",
    "        hub_strategy='all_checkpoints',\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=8192,\n",
    "        train_dataset=chat_dataset[train_set],\n",
    "        eval_dataset=chat_dataset[test_set],\n",
    "        formatting_func=format,\n",
    "        neftune_noise_alpha=5,\n",
    "    )\n",
    "\n",
    "  \n",
    "    dict_info = {\n",
    "        'model':new_model_name,\n",
    "        'base_model':model_name,\n",
    "        'chat_dataset':chat_dataset_name,\n",
    "        'train_set':train_set,\n",
    "        'test_set': test_set,\n",
    "        'training_args': training_args,\n",
    "        'resume_from_checkpoint':resume,\n",
    "        'date':ph.get_datetime(),\n",
    "        'runtime': time.time()-start_time,\n",
    "        'Error': False,\n",
    "        'run_id':run_id,\n",
    "        'save_to_hub':save_to_hub,\n",
    "        'output_dir': output_directory\n",
    "        }\n",
    "\n",
    "    data = pd.DataFrame(columns=dict_info.keys())\n",
    "\n",
    "    # trainer.train(resume_from_checkpoint=resume)\n",
    "    # if save_to_hub == True:\n",
    "    #     trainer.push_to_hub()\n",
    "    # return trainer\n",
    "\n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=resume)\n",
    "        if save_to_hub == True:\n",
    "            trainer.push_to_hub()\n",
    "        data.loc[len(data)] = dict_info\n",
    "        combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "        print(\"Finished without error!\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        dict_info['Error'] = 'KeyboardInterrupt'\n",
    "        data.loc[len(data)] = dict_info\n",
    "        combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "    except Exception  as e:\n",
    "        print(e)\n",
    "        dict_info['Error'] = e\n",
    "        data.loc[len(data)] = dict_info\n",
    "        combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "\n",
    "        model_df = pd.DataFrame(dict_info)\n",
    "        combine_and_save_df(model_df, f'{cf.output_path}/overview_models.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "resume_from_checkpoitn, gives error if last epoch was not fully run, because then files are missing but the folder exists, thus it throws an error. Removing the last checkpoint folder solves this, does mean the epoch need to restart completely. If this is a viable solution  depends on how long an epoch takes to train. Might be quite long. However if all files are in folder then it is fine. How long does it take before all files are saved in folder? Does this need the epoch to be completed?\n",
    "\n",
    "\n",
    "MAKE SURE: no previous checkpints of runs unrelated to current run are in output folder!!\n",
    "\n",
    "MAKE SURE: run_id is unique, for each seperate run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:257: UserWarning: You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override the one in the `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5176558839734927ab5b06fa8bcc9896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfemkebakker\u001b[0m (\u001b[33mthesisamsterdam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/femke-gpu-24cores-220ram/code/Users/f.bakker/document-classification-using-large-language-models/notebooks/wandb/run-20240429_095704-2p1lal6n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thesisamsterdam/huggingface/runs/2p1lal6n' target=\"_blank\">wobbly-capybara-30</a></strong> to <a href='https://wandb.ai/thesisamsterdam/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thesisamsterdam/huggingface' target=\"_blank\">https://wandb.ai/thesisamsterdam/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thesisamsterdam/huggingface/runs/2p1lal6n' target=\"_blank\">https://wandb.ai/thesisamsterdam/huggingface/runs/2p1lal6n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [52/52 06:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.658400</td>\n",
       "      <td>0.750424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.544800</td>\n",
       "      <td>0.658864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.470300</td>\n",
       "      <td>0.631221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.619414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.618062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, basemodel_name, tokenizer, chat_dataset, 'FemkeBakker/AmsterdamGEITjeFormat200Tokens',\n",
    "          'FemkeBakker/GEITjeSmallData200Tokens', 'dev', 'val',  run_id=4, save_to_hub=True, resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"{'categorie': Motie}\", 'role': 'assistant'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_dataset['train']['message'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb31bbf778b941788be42516085ea1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b695fe457b54b4f930d49be59f033c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680982e656cc4ea1b6609b7039b126e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382b2d5549784653a99d07d56a110d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4224de55204ddca384673b21ef4165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e3053e5dc7446b98631fc680d0df30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a464ea0cb3f48f39605faa96de958a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a600580fae4d2b87d9c3856fa37403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4842ac1d08e14c81bf3272df3d6a483c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c7d4e046c64b64bc3dd9e36e1c00c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eba7306715d48598d1b08796b6094b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af9ea3769c942fd9bfcf0db7ac8f009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 01465b34-332a-4929-8ded-979522c5e4bb\n",
      "user: \n",
      "    Classificeer het document in één van de categoriën.\n",
      "    Geef de output in de vorm van een JSON file: {'categorie': categorie van het document}\n",
      "    \n",
      "    Categoriën: ['Voordracht', 'Besluit', 'Schriftelijke Vraag', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheet']\n",
      "    \n",
      "    Document: \n",
      "    > Gemeente\n",
      "Amsterdam\n",
      "Motie\n",
      "Datum raadsvergadering 19 juli 2023\n",
      "Ingekomen onder nummer 456\n",
      "Status Aangenomen\n",
      "Onderwerp Motie van de leden Van Pijpen, Runderkamp, Alberts, Wehkamp,\n",
      "Kabamba en Krom inzake een grote schoonmaak voor iedereen op de\n",
      "wachtlijst Hulp bij Huishouden\n",
      "Onderwerp\n",
      "Een grote schoonmaak voor iedereen op de wachtlijst Hulp bij het Huishouden\n",
      "Aan de gemeenteraad\n",
      "Ondergetekenden hebben de eer voor te stellen:\n",
      "De Raad,\n",
      "Gehoord de discussie over Plan van Aanpak Wachtlijsten Hulp bij het Huishouden\n",
      "Constaterende dat:\n",
      "e \n",
      "\n",
      "    Vul in met de categorie van het document: {'categorie': ??}     \n",
      "    \n",
      "assistant: {'categorie': Motie}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "\n",
    "chatbot = pipeline(task='conversational', model='FemkeBakker/GEITjeSmallData200Tokens',\n",
    "                   device_map='auto', model_kwargs={'offload_buffers':True})\n",
    "\n",
    "## EXAMPLE PROMPT\n",
    "response = chatbot(\n",
    "    Conversation(chat_dataset['train']['message'][0][0]['content'])\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>base_model</th>\n",
       "      <th>chat_dataset</th>\n",
       "      <th>train_set</th>\n",
       "      <th>test_set</th>\n",
       "      <th>training_args</th>\n",
       "      <th>resume_from_checkpoint</th>\n",
       "      <th>date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>Error</th>\n",
       "      <th>run_id</th>\n",
       "      <th>save_to_hub</th>\n",
       "      <th>output_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-25 13:21:46.201696+02:00</td>\n",
       "      <td>0.876791</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-25 13:24:59.884223+02:00</td>\n",
       "      <td>0.274652</td>\n",
       "      <td>Error(s) in loading state_dict for MistralForC...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-25 13:25:57.527833+02:00</td>\n",
       "      <td>0.362344</td>\n",
       "      <td>No valid checkpoint found in output directory ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-25 13:26:24.369492+02:00</td>\n",
       "      <td>0.319212</td>\n",
       "      <td>CUDA out of memory. Tried to allocate 112.00 M...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 08:47:21.945291+02:00</td>\n",
       "      <td>0.392476</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryOutFinetuningGeitje</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 08:47:57.151755+02:00</td>\n",
       "      <td>0.600505</td>\n",
       "      <td>string longer than 2147483647 bytes</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryoutGeitje2</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-26 08:56:52.283214+02:00</td>\n",
       "      <td>0.449522</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryoutGeitje2</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 09:26:12.329799+02:00</td>\n",
       "      <td>3.251497</td>\n",
       "      <td>[Errno 2] No such file or directory: '/home/az...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/TryoutGeitje2</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-04-26 09:29:06.636546+02:00</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 09:46:03.908296+02:00</td>\n",
       "      <td>0.364137</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/tryoutstablelm</td>\n",
       "      <td>stabilityai/stablelm-2-1_6b</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 09:48:18.644406+02:00</td>\n",
       "      <td>0.791530</td>\n",
       "      <td>Expected input batch_size (2) to match target ...</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 11:28:02.756504+02:00</td>\n",
       "      <td>1.342644</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 11:48:12.624523+02:00</td>\n",
       "      <td>4.499735</td>\n",
       "      <td>KeyboardInterrupt</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FemkeBakker/GEITjeSmallData200Tokens</td>\n",
       "      <td>Rijgersberg/GEITje-7B</td>\n",
       "      <td>FemkeBakker/AmsterdamGEITjeFormat200Tokens</td>\n",
       "      <td>dev</td>\n",
       "      <td>val</td>\n",
       "      <td>TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-29 11:53:35.436951+02:00</td>\n",
       "      <td>0.519014</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>/home/azureuser/cloudfiles/code/blobfuse/raads...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model                   base_model  \\\n",
       "0    FemkeBakker/TryOutFinetuningGeitje        Rijgersberg/GEITje-7B   \n",
       "0    FemkeBakker/TryOutFinetuningGeitje        Rijgersberg/GEITje-7B   \n",
       "0    FemkeBakker/TryOutFinetuningGeitje        Rijgersberg/GEITje-7B   \n",
       "0    FemkeBakker/TryOutFinetuningGeitje        Rijgersberg/GEITje-7B   \n",
       "0    FemkeBakker/TryOutFinetuningGeitje        Rijgersberg/GEITje-7B   \n",
       "0    FemkeBakker/TryOutFinetuningGeitje        Rijgersberg/GEITje-7B   \n",
       "0             FemkeBakker/TryoutGeitje2        Rijgersberg/GEITje-7B   \n",
       "0             FemkeBakker/TryoutGeitje2        Rijgersberg/GEITje-7B   \n",
       "0             FemkeBakker/TryoutGeitje2        Rijgersberg/GEITje-7B   \n",
       "0            FemkeBakker/tryoutstablelm  stabilityai/stablelm-2-1_6b   \n",
       "0            FemkeBakker/tryoutstablelm  stabilityai/stablelm-2-1_6b   \n",
       "0  FemkeBakker/GEITjeSmallData200Tokens        Rijgersberg/GEITje-7B   \n",
       "0  FemkeBakker/GEITjeSmallData200Tokens        Rijgersberg/GEITje-7B   \n",
       "0  FemkeBakker/GEITjeSmallData200Tokens        Rijgersberg/GEITje-7B   \n",
       "\n",
       "                                 chat_dataset train_set test_set  \\\n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "0  FemkeBakker/AmsterdamGEITjeFormat200Tokens       dev      val   \n",
       "\n",
       "                                       training_args  resume_from_checkpoint  \\\n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                    True   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "0  TrainingArguments(\\n_n_gpu=1,\\naccelerator_con...                   False   \n",
       "\n",
       "                              date   runtime  \\\n",
       "0 2024-04-25 13:21:46.201696+02:00  0.876791   \n",
       "0 2024-04-25 13:24:59.884223+02:00  0.274652   \n",
       "0 2024-04-25 13:25:57.527833+02:00  0.362344   \n",
       "0 2024-04-25 13:26:24.369492+02:00  0.319212   \n",
       "0 2024-04-26 08:47:21.945291+02:00  0.392476   \n",
       "0 2024-04-26 08:47:57.151755+02:00  0.600505   \n",
       "0 2024-04-26 08:56:52.283214+02:00  0.449522   \n",
       "0 2024-04-26 09:26:12.329799+02:00  3.251497   \n",
       "0 2024-04-26 09:29:06.636546+02:00  0.543515   \n",
       "0 2024-04-29 09:46:03.908296+02:00  0.364137   \n",
       "0 2024-04-29 09:48:18.644406+02:00  0.791530   \n",
       "0 2024-04-29 11:28:02.756504+02:00  1.342644   \n",
       "0 2024-04-29 11:48:12.624523+02:00  4.499735   \n",
       "0 2024-04-29 11:53:35.436951+02:00  0.519014   \n",
       "\n",
       "                                               Error  run_id save_to_hub  \\\n",
       "0                                  KeyboardInterrupt       0         NaN   \n",
       "0  Error(s) in loading state_dict for MistralForC...       0         NaN   \n",
       "0  No valid checkpoint found in output directory ...       0         NaN   \n",
       "0  CUDA out of memory. Tried to allocate 112.00 M...       0         NaN   \n",
       "0                                  KeyboardInterrupt       0         NaN   \n",
       "0                string longer than 2147483647 bytes       0         NaN   \n",
       "0                                              False       1         NaN   \n",
       "0  [Errno 2] No such file or directory: '/home/az...       1         NaN   \n",
       "0                                              False       1         NaN   \n",
       "0                                  KeyboardInterrupt       2         NaN   \n",
       "0  Expected input batch_size (2) to match target ...       2       False   \n",
       "0                                  KeyboardInterrupt       3        True   \n",
       "0                                  KeyboardInterrupt       4        True   \n",
       "0                                              False       4        True   \n",
       "\n",
       "                                          output_dir  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0                                                NaN  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  \n",
       "0  /home/azureuser/cloudfiles/code/blobfuse/raads...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yeet = pd.read_pickle(f'{cf.output_path}/overview_models.pkl')\n",
    "display(yeet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt_id', 'message'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt_id', 'message'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['prompt_id', 'message'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['prompt_id', 'message'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465606243ff44d5e9d5112229a278628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 299M/299M [00:01<00:00, 201MB/s]  \n",
      "Downloading data: 100%|██████████| 23.5M/23.5M [00:00<00:00, 103MB/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7459d3cb3634db5aaa4f267b76dc80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0030c415361041a597ef217576240238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import prediction_helperfunctions as ph\n",
    "\n",
    "def train(model, model_name, tokenizer, dataset, chat_dataset_name, new_model_name, train_set, test_set, run_id='No_id', save_to_hub=True, resume=False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    # tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    train_data = dataset[train_set].shuffle(seed=42).select(range(100))\n",
    "    train_data = train_data.map(tokenize_function, batched=True)\n",
    "    test_data = dataset[test_set].shuffle(seed=42).select(range(100))    \n",
    "    test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    steps_per_epoch = len(train_data)\\\n",
    "                // (torch.cuda.device_count() * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = steps_per_epoch // 5\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        optim='adamw_bnb_8bit',\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy='epoch',\n",
    "        bf16=False, #bf16=True require CUDA 11 -> original code bf16=True\n",
    "        output_dir=f'{cf.output_path}/finetuning_output/stablelm_finetuning_output',\n",
    "        report_to=[\"tensorboard\", 'wandb'],\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        hub_model_id=new_model_name,\n",
    "        push_to_hub=True,\n",
    "        hub_private_repo=True,\n",
    "        hub_strategy='all_checkpoints',\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=8192,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        packing=True\n",
    "        # formatting_func=format,\n",
    "        # neftune_noise_alpha=5,\n",
    "    )\n",
    "\n",
    "  \n",
    "    dict_info = {\n",
    "        'model':new_model_name,\n",
    "        'base_model':model_name,\n",
    "        'chat_dataset':chat_dataset_name,\n",
    "        'train_set':train_set,\n",
    "        'test_set': test_set,\n",
    "        'training_args': training_args,\n",
    "        'resume_from_checkpoint':resume,\n",
    "        'date':ph.get_datetime(),\n",
    "        'runtime': time.time()-start_time,\n",
    "        'Error': False,\n",
    "        'run_id':run_id,\n",
    "        'save_to_hub':save_to_hub\n",
    "        }\n",
    "\n",
    "    data = pd.DataFrame(columns=dict_info.keys())\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume)\n",
    "    if save_to_hub == True:\n",
    "        trainer.push_to_hub()\n",
    "    return trainer\n",
    "\n",
    "    # try:\n",
    "    #     trainer.train(resume_from_checkpoint=resume)\n",
    "    #     if save_to_hub == True:\n",
    "    #         trainer.push_to_hub()\n",
    "    #     data.loc[len(data)] = dict_info\n",
    "    #     combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "    # except KeyboardInterrupt:\n",
    "    #     dict_info['Error'] = 'KeyboardInterrupt'\n",
    "    #     data.loc[len(data)] = dict_info\n",
    "    #     combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "    # except Exception  as e:\n",
    "    #     print(e)\n",
    "    #     dict_info['Error'] = e\n",
    "    #     data.loc[len(data)] = dict_info\n",
    "    #     combine_and_save_df(data, f'{cf.output_path}/overview_models.pkl')\n",
    "\n",
    "\n",
    "    #     model_df = pd.DataFrame(dict_info)\n",
    "    #     combine_and_save_df(model_df, f'{cf.output_path}/overview_models.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of StableLmForSequenceClassification were not initialized from the model checkpoint at stabilityai/stablelm-2-1_6b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "basemodel_name = \"stabilityai/stablelm-2-1_6b\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(basemodel_name,num_labels=5, torch_dtype=torch.bfloat16,\n",
    "                                                low_cpu_mem_usage=True, attn_implementation=\"sdpa\",\n",
    "                                                device_map='cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(basemodel_name)\n",
    "# tokenizer.pad_token = tokenizer.unk_token\n",
    "# tokenizer.padding_side = 'right'\n",
    "\n",
    "# model.config.pad_token_id = tokenizer.unk_token_id\n",
    "model.config.problem_type = \"multi_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48d9c9024e541589000e711d0f029c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b90406c98c425cadaeef6f0a87a0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_class \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasemodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFemkeBakker/AmsterdamGEITjeFormat200Tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFemkeBakker/tryoutstablelm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 82\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, model_name, tokenizer, dataset, chat_dataset_name, new_model_name, train_set, test_set, run_id, save_to_hub, resume)\u001b[0m\n\u001b[1;32m     65\u001b[0m dict_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m:new_model_name,\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m'\u001b[39m:model_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_to_hub\u001b[39m\u001b[38;5;124m'\u001b[39m:save_to_hub\n\u001b[1;32m     78\u001b[0m     }\n\u001b[1;32m     80\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mdict_info\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 82\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_to_hub \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mpush_to_hub()\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/trainer.py:1771\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1769\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1770\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1778\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/trainer.py:2085\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2086\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/accelerate/data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3329\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3326\u001b[0m         encoded_inputs[key] \u001b[38;5;241m=\u001b[39m to_py_obj(value)\n\u001b[1;32m   3328\u001b[0m \u001b[38;5;66;03m# Convert padding_strategy in PaddingStrategy\u001b[39;00m\n\u001b[0;32m-> 3329\u001b[0m padding_strategy, _, max_length, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m   3331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3333\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   3334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(required_input[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamInContextLearning/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2777\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 2777\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2778\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2779\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2780\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2781\u001b[0m     )\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2785\u001b[0m     truncation_strategy \u001b[38;5;241m!=\u001b[39m TruncationStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2786\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2789\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (max_length \u001b[38;5;241m%\u001b[39m pad_to_multiple_of \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2790\u001b[0m ):\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "trained_class = train(model, basemodel_name, tokenizer, dataset, 'FemkeBakker/AmsterdamGEITjeFormat200Tokens',\n",
    "          'FemkeBakker/tryoutstablelm', 'train', 'test',  run_id=2, save_to_hub=False, resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f2dfe7d5ec49da91adfe5948ab7bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee265553dc9f4221943074a474c3c53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18b5078836b4e85aa7c26b7515c1346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0efc3fa4412479fa7c3ef6832ad177b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf612709f66e4f118ab25c0c873cf162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dad046e6ef240cbba454ae4df5b1b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2197de49a6cc413aa1988723b6e091b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c320287d77024910bb24f89bdf8967ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6955b3631c164fd7a5649d1071607bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e57e26e1884b77b76bcf741dfb4c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697d1f602ba2478eb1a865b93d43e728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at FemkeBakker/TryoutGeitje2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"FemkeBakker/TryoutGeitje2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize input text\n",
    "input_text = \"This is a sample sentence.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Model inference\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=1)\n",
    "\n",
    "# Post-processing (e.g., convert predictions to labels)\n",
    "predicted_label = predictions.item()\n",
    "print(\"Predicted label:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tryout finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdd68318ef94eedaba0871f47521bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bd69a5f1ee44ae93a22ac3a81fb185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'torch.storage.UntypedStorage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRijgersberg/GEITje-7B-chat-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(conversation, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000\u001b[39m):\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/transformers/modeling_utils.py:3531\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3523\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3524\u001b[0m     (\n\u001b[1;32m   3525\u001b[0m         model,\n\u001b[1;32m   3526\u001b[0m         missing_keys,\n\u001b[1;32m   3527\u001b[0m         unexpected_keys,\n\u001b[1;32m   3528\u001b[0m         mismatched_keys,\n\u001b[1;32m   3529\u001b[0m         offload_index,\n\u001b[1;32m   3530\u001b[0m         error_msgs,\n\u001b[0;32m-> 3531\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3538\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3542\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3543\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3549\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3550\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/transformers/modeling_utils.py:3938\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shard_file \u001b[38;5;129;01min\u001b[39;00m disk_only_shard_files:\n\u001b[1;32m   3937\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 3938\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3940\u001b[0m \u001b[38;5;66;03m# Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\u001b[39;00m\n\u001b[1;32m   3941\u001b[0m \u001b[38;5;66;03m# matching the weights in the model.\u001b[39;00m\n\u001b[1;32m   3942\u001b[0m mismatched_keys \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3943\u001b[0m     state_dict,\n\u001b[1;32m   3944\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3948\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3949\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/transformers/modeling_utils.py:513\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlx\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe safetensors archive passed at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not contain the valid metadata. Make sure \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou save your model with the `save_pretrained` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m         )\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msafe_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    516\u001b[0m         (is_deepspeed_zero3_enabled() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mget_rank() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0())\n\u001b[1;32m    518\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized:\n",
      "File \u001b[0;32m/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/safetensors/torch.py:313\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m safe_open(filename, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 313\u001b[0m         result[k] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'torch.storage.UntypedStorage'"
     ]
    }
   ],
   "source": [
    "\n",
    "## WORKING VERSION OF GEITJE-Chat using AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\n",
    "                                             low_cpu_mem_usage=True, attn_implementation=\"eager\",\n",
    "                                             device_map='balanced')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate(conversation, temperature=0.2, top_k=50, max_new_tokens=1_000):\n",
    "    tokenized = tokenizer.apply_chat_template(conversation, add_generation_prompt=True,\n",
    "                                              return_tensors='pt').to(device)\n",
    "    outputs = model.generate(tokenized, do_sample=True, temperature=temperature,\n",
    "                             top_k=top_k, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Xx Gemeente Amsterdam F l N\\n% Raadscommissie voor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies,\\nAanpak Belastingen, Waterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\n% Agenda, woensdag 3 juni 2015\\nHierbij wordt u uitgenodigd voor de openbare vergadering van de Raadscommissie\\nvoor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies, Aanpak Belastingen,\\nWaterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\nTijd 09.00 tot 12.30 uur en van 13.30 tot 17',\n",
    "    }\n",
    "]\n",
    "print(generate(conversation))\n",
    "# <|user|>\n",
    "# Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"? \n",
    "# <|assistant|>\n",
    "# Het woord dat niet op zijn plaats staat is 'geit'. Een geit zou niet tussen een lijst van vervoersmiddelen moeten staan. Het past beter bij een boerderijthema of dierenlijst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "\n",
      "         Classificeer het document in één van de categoriën.\n",
      "        Houd het kort, geef enkel de naam van de categorie als response.\n",
      "    \n",
      "    Categoriën:   ['Voordracht', 'Besluit', 'Schriftelijke Vragen', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheets']\n",
      "    \n",
      "    Document: \n",
      "\n",
      "        'Xx Gemeente Amsterdam F l N\n",
      "% Raadscommissie voor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies,\n",
      "Aanpak Belastingen, Waterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\n",
      "\n",
      "% Agenda, woensdag 3 juni 2015\n",
      "Hierbij wordt u uitgenodigd voor de openbare vergadering van de Raadscommissie\n",
      "voor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies, Aanpak Belastingen,\n",
      "Waterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\n",
      "\n",
      "Tijd 09.00 tot 12.30 uur en van 13.30 tot 17'\n",
      "         \n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation = [\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': f\"\"\"\n",
    "         Classificeer het document in één van de categoriën.\n",
    "        Houd het kort, geef enkel de naam van de categorie als response.\n",
    "    \n",
    "    Categoriën:   ['Voordracht', 'Besluit', 'Schriftelijke Vragen', 'Brief', 'Raadsadres', 'Onderzoeksrapport', 'Termijnagenda', 'Raadsnotulen', 'Agenda', 'Motie', 'Actualiteit', 'Factsheets']\n",
    "    \n",
    "    Document: \n",
    "\n",
    "        'Xx Gemeente Amsterdam F l N\\n% Raadscommissie voor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies,\\nAanpak Belastingen, Waterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\n% Agenda, woensdag 3 juni 2015\\nHierbij wordt u uitgenodigd voor de openbare vergadering van de Raadscommissie\\nvoor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies, Aanpak Belastingen,\\nWaterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\nTijd 09.00 tot 12.30 uur en van 13.30 tot 17'\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "print(generate(conversation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xx Gemeente Amsterdam F l N\\n% Raadscommissie voor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies,\\nAanpak Belastingen, Waterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\n% Agenda, woensdag 3 juni 2015\\nHierbij wordt u uitgenodigd voor de openbare vergadering van de Raadscommissie\\nvoor Financiën, Coördinatie 3d, Coördinatie Aanpak Subsidies, Aanpak Belastingen,\\nWaterbeheer, Vastgoed, Inkoop en Personeel en Organisatie\\n\\nTijd 09.00 tot 12.30 uur en van 13.30 tot 17'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_dataset['val'][0]['message'][0]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code works for GEITje example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel_name, torch_dtype=torch.bfloat16,\n",
    "                                                low_cpu_mem_usage=True, attn_implementation=\"sdpa\",\n",
    "                                                device_map='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:257: UserWarning: You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override the one in the `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfemkebakker\u001b[0m (\u001b[33mthesisamsterdam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/femke-gpu-24cores-220ram/code/Users/f.bakker/document-classification-using-large-language-models/notebooks/wandb/run-20240416_113125-40z5iowj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thesisamsterdam/huggingface/runs/40z5iowj' target=\"_blank\">breezy-dust-12</a></strong> to <a href='https://wandb.ai/thesisamsterdam/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thesisamsterdam/huggingface' target=\"_blank\">https://wandb.ai/thesisamsterdam/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thesisamsterdam/huggingface/runs/40z5iowj' target=\"_blank\">https://wandb.ai/thesisamsterdam/huggingface/runs/40z5iowj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 09:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.373000</td>\n",
       "      <td>1.369126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.371100</td>\n",
       "      <td>1.075798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>1.051754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/AmsterdamLLM/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8216f038d4e4ea899a0c9dadb346523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d162eab9724d959445299bf9416674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f31e89c052347feb1e02b7853abbdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549fd8b1bc0641bea3ad46a899fc21f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31424e0043b14debafbbcc1e37f5b2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1713267084.femke-gpu-24cores-220ram:   0%|          | 0.00/6.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "def train(model, tokenizer, chat_dataset, new_model_name):\n",
    "\n",
    "    def format(examples):\n",
    "        return [tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "                for conversation in examples['messages_nl']]\n",
    "\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = 8\n",
    "    steps_per_epoch = len(chat_dataset['train_sft'])\\\n",
    "                 // (torch.cuda.device_count() * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "    eval_steps = steps_per_epoch // 5\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        optim='adamw_bnb_8bit',\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        output_dir=f'{cf.output_path}/geitje_finetuning_output',\n",
    "        report_to=[\"tensorboard\", 'wandb'],\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        hub_model_id=new_model_name,\n",
    "        push_to_hub=True,\n",
    "        hub_private_repo=True,\n",
    "        hub_strategy='all_checkpoints',\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=8192,\n",
    "        train_dataset=chat_dataset['train_sft'],\n",
    "        eval_dataset=chat_dataset['test_sft'],\n",
    "        formatting_func=format,\n",
    "        neftune_noise_alpha=5,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.push_to_hub()\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# basemodel_name = 'Rijgersberg/GEITje-7B'\n",
    "# model = AutoModelForCausalLM.from_pretrained(basemodel_name, torch_dtype=torch.bfloat16,\n",
    "#                                                 low_cpu_mem_usage=True, attn_implementation=\"eager\",\n",
    "#                                                 device_map='balanced')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(basemodel_name)\n",
    "\n",
    "# Mistral 7B is missing a padding token by default, so we need to assign\n",
    "# another token to the padding job during training.\n",
    "# Unfortunately we cannot use the </s> token, because we need the model to\n",
    "# learn to output </s> at the end of its turn, so that we can stop generating\n",
    "# when it emits it. If we were to also use it as the padding token,\n",
    "# any loss computed on </s> would then be discarded, nothing would be learned\n",
    "# and the model would never stop generating.\n",
    "# Trust me, I learned this the hard way ;).\n",
    "# Therefore, we take the least bad alternative action and assign\n",
    "# the rarely used <UNK> token to the padding role.\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'right'\n",
    "model.config.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "no_robots_nl = load_dataset('Rijgersberg/no_robots_nl')\n",
    "no_robots_nl[\"train_sft\"]=no_robots_nl[\"train_sft\"].select(range(2))\n",
    "no_robots_nl[\"test_sft\"]=no_robots_nl[\"test_sft\"].select(range(2))\n",
    "\n",
    "ultrachat_nl = load_dataset('Rijgersberg/ultrachat_10k_nl')\n",
    "ultrachat_nl[\"train_sft\"]=ultrachat_nl[\"train_sft\"].select(range(2))\n",
    "ultrachat_nl[\"test_sft\"]=ultrachat_nl[\"test_sft\"].select(range(2))\n",
    "\n",
    "chat_dataset = DatasetDict({\n",
    "    'train_sft': concatenate_datasets([no_robots_nl['train_sft'],\n",
    "                                        ultrachat_nl['train_sft']]).shuffle(seed=42),\n",
    "    'test_sft': concatenate_datasets([no_robots_nl['test_sft'],\n",
    "                                        ultrachat_nl['test_sft']]).shuffle(seed=42),\n",
    "})\n",
    "\n",
    "chat_dataset = chat_dataset.filter(lambda row: all(turn['content'] != '<TRANSLATION FAILED>'\n",
    "                                                    for turn in row['messages_nl']))\n",
    "\n",
    "trained_model = train(model, tokenizer, chat_dataset,\n",
    "        new_model_name='FemkeBakker/TryOutFinetuningGeitje')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## WORKING VERSION OF GEITJE-Chat using AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'Rijgersberg/GEITje-7B-chat-v2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,\n",
    "                                             low_cpu_mem_usage=True, attn_implementation=\"eager\",\n",
    "                                             device_map='balanced')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate(conversation, temperature=0.2, top_k=50, max_new_tokens=1_000):\n",
    "    tokenized = tokenizer.apply_chat_template(conversation, add_generation_prompt=True,\n",
    "                                              return_tensors='pt').to(device)\n",
    "    outputs = model.generate(tokenized, do_sample=True, temperature=temperature,\n",
    "                             top_k=top_k, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"?'\n",
    "    }\n",
    "]\n",
    "print(generate(conversation))\n",
    "# <|user|>\n",
    "# Welk woord hoort er niet in dit rijtje thuis: \"auto, vliegtuig, geitje, bus\"? \n",
    "# <|assistant|>\n",
    "# Het woord dat niet op zijn plaats staat is 'geit'. Een geit zou niet tussen een lijst van vervoersmiddelen moeten staan. Het past beter bij een boerderijthema of dierenlijst."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AmsterdamLLM",
   "language": "python",
   "name": "amsterdamllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
