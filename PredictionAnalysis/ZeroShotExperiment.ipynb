{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Select where to run notebook: \"azure\" or \"local\"\n",
    "my_run = \"azure\"\n",
    "\n",
    "# import my_secrets as sc\n",
    "# import settings as st\n",
    "\n",
    "if my_run == \"azure\":\n",
    "    import config_azure as cf\n",
    "elif my_run == \"local\":\n",
    "    import config as cf\n",
    "\n",
    "\n",
    "import os\n",
    "if my_run == \"azure\":\n",
    "    if not os.path.exists(cf.HUGGING_CACHE):\n",
    "        os.mkdir(cf.HUGGING_CACHE)\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# set-up environment - GEITje-7b-chat InContextLearning:\n",
    "# - install blobfuse -> sudo apt-get install blobfuse\n",
    "# - pip install transformers\n",
    "# - pip install torch\n",
    "# - pip install accelerate\n",
    "# - pip install jupyter\n",
    "# - pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "Goal: gain insight into predictions of fewshot experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = pd.read_pickle(f\"{cf.output_path}/txtfiles_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/2AmsterdamLLM/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "sys.path.append('../scripts/') \n",
    "import prompt_template as pt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "def get_tokens(model_name, df, text_col, new_col_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    all_texts = list(df[text_col].values)\n",
    "\n",
    "    all_tokens = []\n",
    "    all_tokens_len = []\n",
    "    for txt in all_texts:\n",
    "        tokens = tokenizer.tokenize(txt)\n",
    "        all_tokens.append(tokens)\n",
    "        all_tokens_len.append(len(tokens))\n",
    "\n",
    "    df[new_col_name] = all_tokens\n",
    "    df[f\"count_{new_col_name}\"] = all_tokens_len\n",
    "    return df\n",
    "\n",
    "def format_label(label):\n",
    "    format = f\"\"\"{{'categorie': {label}}}\"\"\"\n",
    "    return format\n",
    "    \n",
    "\n",
    "def get_response_length(df, model_name):\n",
    "    # convert response column into tokens, using models tokenizer (so either geitje, mistral or Llama)\n",
    "    df = get_tokens(model_name, df, 'response', 'responseTokens')\n",
    "\n",
    "    # convert label into ideal format\n",
    "    df['label_formatted'] = df['label'].apply(format_label)\n",
    "    # convert ideal format into tokens, so that we know the ideal length of the responses\n",
    "    df_ideal = get_tokens(model_name, df.loc[df['run_id']==df.iloc[0]['run_id']], 'label_formatted', 'label_formattedTokens')\n",
    "\n",
    "    # for each run, describe the response lengths\n",
    "    description = df.groupby('run_id')['count_responseTokens'].describe()\n",
    "    description.loc[len(description)] = df_ideal['count_label_formattedTokens'].describe()\n",
    "    \n",
    "    print(\"RESPONSE LENGTH\")\n",
    "    description = description.rename(index={description.index[-1]: 'IDEAL'}).round(1)\n",
    "    display(description)\n",
    "    return df\n",
    "   \n",
    "\n",
    "def prediction_errors(df):\n",
    "    print('PREDICTION ERRORS')\n",
    "    error_names = ['NoPredictionInOutput', 'MultiplePredictionErrorInFormatting','NoPredictionFormat', 'MultiplePredictionErrorInOutput']\n",
    "\n",
    "    # only select row that have prediction error -> response of which a prediction could not be extracted. \n",
    "    errors_df = df.loc[df['prediction'].isin(error_names)]\n",
    "\n",
    "    df_errors_count = pd.DataFrame(columns=error_names)\n",
    "\n",
    "    for runid in set(errors_df['run_id']):\n",
    "        subdf = errors_df.loc[errors_df['run_id']==runid]\n",
    "        # count for each error the instances\n",
    "        count = dict(Counter(subdf['prediction']))\n",
    "\n",
    "        # check if all errors are included, else set that error to 0\n",
    "        for error in error_names:\n",
    "            if error not in count.keys():\n",
    "                count[error]=0\n",
    "\n",
    "        df_errors_count.loc[len(df_errors_count)] = count\n",
    "\n",
    "    df_errors_count.index = list(set(errors_df['run_id']))\n",
    "    df_errors_count['total'] = df_errors_count.sum(axis=1)\n",
    "\n",
    "    print(\"Count of prediction errors for each run:\")\n",
    "    display(df_errors_count)\n",
    "\n",
    "    df_classes_in_response_count = pd.DataFrame(columns=['responses with 0 classes', 'responses with 1 classes', 'responses with 2 classes', 'Correct label in response'])\n",
    "\n",
    "    for run_id in list(set(errors_df['run_id'])):\n",
    "        subdf = errors_df.loc[errors_df['run_id']==run_id]\n",
    "\n",
    "        classes_in_responses = []\n",
    "        correct_class_in_response = []\n",
    "        for index, row in subdf.iterrows():\n",
    "            # for each response, return list with all labels that are named in response\n",
    "            classes_in_response = [category.lower() for category in pt.get_class_list() if category.lower() in row['response'].lower()]\n",
    "            classes_in_responses.append(classes_in_response)\n",
    "\n",
    "            # for each response, check if true label is named in response.\n",
    "            if row['label'].lower() in classes_in_response:\n",
    "                correct_class_in_response.append(True)\n",
    "            else:\n",
    "                correct_class_in_response.append(False)\n",
    "\n",
    "        # count how many classes are named in a response\n",
    "        amount_of_classes = dict(Counter([len(response) for response in classes_in_responses]))\n",
    "        amount_of_classes = {f\"responses with {k} classes\":v for k,v in amount_of_classes.items()}\n",
    "        amount_of_classes['Correct label in response'] = f\"{correct_class_in_response.count(True)} out of {len(subdf)} prediction errors\" \n",
    "        df_classes_in_response_count.loc[len(df_classes_in_response_count)] = amount_of_classes\n",
    "\n",
    "        \n",
    "    df_classes_in_response_count.index = list(set(errors_df['run_id']))\n",
    "\n",
    "    print('amount of class in responses:')\n",
    "    display(df_classes_in_response_count)\n",
    "\n",
    "    # for each run count the errors per class\n",
    "    count_error_class_df_list = []\n",
    "    for run_id in set(errors_df['run_id']):\n",
    "        # select errors for current runid\n",
    "        subdf = errors_df.loc[errors_df['run_id']==run_id]\n",
    "        errors_count_per_class = dict()\n",
    "        # for each class count the errors\n",
    "        for category in pt.get_class_list():\n",
    "            class_df = subdf.loc[subdf['label']==category.lower()]\n",
    "            count_errors = dict(Counter(class_df['prediction']))\n",
    "            # sort the errors from highest to lowest for each class\n",
    "            count_errors = dict(sorted(count_errors.items(), key=lambda item: item[1], reverse=True))\n",
    "            errors_count_per_class[category] = count_errors\n",
    "\n",
    "        # create df\n",
    "        df_errors_count = pd.DataFrame(list(errors_count_per_class.items()), columns=['Class', run_id])\n",
    "        # add df to list with the dfs of other runs\n",
    "        count_error_class_df_list.append(df_errors_count)\n",
    "    \n",
    "    # combine df into one\n",
    "    df_errors_count = count_error_class_df_list[0]\n",
    "    for count_df in count_error_class_df_list[1:]:\n",
    "        df_errors_count = df_errors_count.merge(count_df, on='Class', how='outer')\n",
    "    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print('Amount of errors type per class per run:')\n",
    "    display(df_errors_count)\n",
    "\n",
    "\n",
    "\n",
    "def evaluation_metrics(df):\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    report = classification_report(df['label'], df['prediction'])\n",
    "    print('EVALUATION METRICS')\n",
    "    print(report)\n",
    "\n",
    "\n",
    "def percentage_mistakes(count):\n",
    "    return f\"{round(count/1100*100,1)}%\"\n",
    "\n",
    "def mistakes(df, detailed=False):\n",
    "    print('MISTAKES. INCLUDES PREDICTIONS ERRORS.')\n",
    "    run_ids = sorted(list(set(df['run_id'])))\n",
    "\n",
    "    # select all response where the prediction was not correct\n",
    "    mistakes = df.loc[df['label'] != df['prediction']]\n",
    "\n",
    "    # groupby run_id so we get the amount of mistakes per run\n",
    "    mistakes_per_run = mistakes.groupby('run_id').size().reset_index(name='count')\n",
    "    mistakes_per_run['percentage of total predictions'] = mistakes_per_run['count'].apply(percentage_mistakes)\n",
    "    print(\"Total amount of mistakes per run:\")\n",
    "    display(mistakes_per_run)\n",
    "\n",
    "    # for each run get the amount of mistakes per class\n",
    "    df_count_class = pd.DataFrame(columns=[category.lower() for category in pt.get_class_list()])\n",
    "    for run_id in run_ids:\n",
    "        # select mistakes of current runid\n",
    "        subdf = mistakes.loc[mistakes['run_id']==run_id]\n",
    "        # count mistakes per class\n",
    "        class_count = dict(Counter(subdf['label']))\n",
    "\n",
    "        # if a class is not included in mistakes, set counter to 0\n",
    "        for category in pt.get_class_list():\n",
    "            if category.lower() not in class_count.keys():\n",
    "                class_count[category.lower()] = 0\n",
    "\n",
    "        # add count of class to df\n",
    "        df_count_class.loc[len(df_count_class)] = class_count\n",
    "\n",
    "    # reset index to runids\n",
    "    df_count_class.index = run_ids\n",
    "    print(\"Amount of mistakes per class for each run:\")\n",
    "    display(df_count_class)\n",
    "\n",
    "    # For each run get for each class the highest mistakes\n",
    "    df_highest_class = pd.DataFrame(columns=[category.lower() for category in pt.get_class_list()])\n",
    "    for run_id in run_ids:\n",
    "        # select mistakes of current runid\n",
    "        subdf = mistakes.loc[mistakes['run_id']==run_id]\n",
    "        # count mistakes of runid\n",
    "        count_mistakes_per_class = dict(Counter(subdf['label']))\n",
    "\n",
    "        class_count = subdf.groupby('label')['prediction'].value_counts().reset_index(name='count')\n",
    "        highest_per_class = dict()\n",
    "        for label in set(class_count['label']):\n",
    "            # for each class select the class for which it made the most mistakes\n",
    "            highest_count_row = class_count.iloc[class_count[class_count['label'] == label]['count'].idxmax()]\n",
    "            highest_per_class[label] = f\"{highest_count_row['prediction']} ({highest_count_row['count']} out of {count_mistakes_per_class[label]})\"\n",
    "\n",
    "        # add to df\n",
    "        df_highest_class.loc[len(df_highest_class)] = highest_per_class\n",
    "    # reset index to run_id\n",
    "    df_highest_class.index = run_ids\n",
    "    print(\"Highest mistakes per class for each run:\")\n",
    "    display(df_highest_class.transpose())\n",
    "   \n",
    "\n",
    "def runtime(df):\n",
    "    print(\"RUNTIME\")\n",
    "\n",
    "    # select all docs that have runtime longer than 75th percentile.\n",
    "    percentile_75 = df['runtime'].quantile(0.75)\n",
    "\n",
    "    # count how many docs for each class take longer than 75th percentile\n",
    "    df_count_long_runtimes = pd.DataFrame(columns=[category.lower() for category in pt.get_class_list()])\n",
    "\n",
    "    # calculate average runtime per doc for each class\n",
    "    df_average_runtime = pd.DataFrame(columns=[category.lower() for category in pt.get_class_list()])\n",
    "\n",
    "    # for each run_id, calculate average runtime and count how many docs exceed 75th percentile\n",
    "    for run_id in set(df['run_id']):\n",
    "        subdf = df.loc[df['run_id']==run_id]\n",
    "\n",
    "        # get very long runtimes\n",
    "        percentile_75 = subdf['runtime'].quantile(0.75)\n",
    "        above_75th_percentile = subdf[subdf['runtime'] > percentile_75]\n",
    "        count_long_runtimes_per_class = dict(Counter(above_75th_percentile['label']))\n",
    "        df_count_long_runtimes.loc[len(df_count_long_runtimes)] = count_long_runtimes_per_class\n",
    "\n",
    "        # average runtime per class\n",
    "        average_runtime_per_class = subdf.groupby('label')['runtime'].mean()\n",
    "        df_average_runtime.loc[len(df_average_runtime)] = average_runtime_per_class\n",
    "\n",
    "    df_count_long_runtimes.index = list(set(df['run_id']))\n",
    "    df_average_runtime.index = list(set(df['run_id']))\n",
    "\n",
    "    print('Description of runtime per doc:')\n",
    "    display(df.groupby('run_id')['runtime'].describe().round(1))\n",
    "\n",
    "    print(f\"Amount of docs that took longer than 75th percentile per class:\")\n",
    "    display(df_count_long_runtimes)\n",
    "\n",
    "    print('Average runtime per doc for each class')\n",
    "    display(df_average_runtime.transpose().sort_values(by=list(set(df['run_id']))).round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncation experiment\n",
    "predictions =  pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/GEITje/zeroshot_prompt_geitje/First200Last0Predictions.pkl\")\n",
    "predictions2 =  pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/Mistral/zeroshot_prompt_mistral_llama/First200Last0Predictions.pkl\")\n",
    "predictions3 =  pd.read_pickle(f\"{cf.output_path}/predictionsFinal/in_context/Llama/zeroshot_prompt_mistral_llama/First200Last0Predictions.pkl\")\n",
    "combined = pd.concat([predictions, predictions2, predictions3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE LENGTH\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IC_GEITje-7B-chat-v2zeroshot_prompt_geitjeLlamaTokens200_0traintest_numEx0</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>7.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IC_Llama-2-7b-chat-hfzeroshot_prompt_mistral_llamaLlamaTokens200_0traintest_numEx0</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>90.2</td>\n",
       "      <td>51.1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>59.5</td>\n",
       "      <td>132.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IC_Mistral-7B-Instruct-v0.2zeroshot_prompt_mistral_llamaLlamaTokens200_0traintest_numEx0</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>44.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDEAL</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     count  mean   std   min  \\\n",
       "run_id                                                                         \n",
       "IC_GEITje-7B-chat-v2zeroshot_prompt_geitjeLlama...  1100.0  16.1   7.3   3.0   \n",
       "IC_Llama-2-7b-chat-hfzeroshot_prompt_mistral_ll...  1100.0  90.2  51.1  11.0   \n",
       "IC_Mistral-7B-Instruct-v0.2zeroshot_prompt_mist...  1100.0  37.7  44.5   9.0   \n",
       "IDEAL                                               1100.0   9.1   1.4   7.0   \n",
       "\n",
       "                                                     25%   50%    75%    max  \n",
       "run_id                                                                        \n",
       "IC_GEITje-7B-chat-v2zeroshot_prompt_geitjeLlama...  15.0  16.0   17.0  227.0  \n",
       "IC_Llama-2-7b-chat-hfzeroshot_prompt_mistral_ll...  51.0  59.5  132.0  256.0  \n",
       "IC_Mistral-7B-Instruct-v0.2zeroshot_prompt_mist...  11.0  13.0   58.0  257.0  \n",
       "IDEAL                                                8.0   9.0   10.0   12.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "description = get_response_length(combined, 'meta-llama/Llama-2-7b-chat-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2AmsterdamLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
