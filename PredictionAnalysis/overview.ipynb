{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "Goal: an summary of all insights into the predictions of the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response Length:**\n",
    "- Ideal response = 9.1\n",
    "- 100/100 tends to have on average longer responses. Especially, for Mistral (44.8) and Llama (109.7). Does not really hold for GEITje (16.2)\n",
    "- 100 and 200 are mostly of similar length, especially for geitje (100=15.9; 200=16.1). Though Llama (100=97.2; 200=90.2) and especially Mistral (100=39.3; 200=37.7) have a difference between them. Just not as big of a difference with 100/100. \n",
    "- For GEITje: truncation does not matter for response lenght, minor differences.\n",
    "- For Mistral: truncation does matter, from smallest to longest response: 200, 100, 100/100. Though the difference not as big for Llama.\n",
    "- For llama: truncation matters the most for response length: 200,100,100/100. Same order as Mistral.\n",
    "\n",
    "*Conclusion: 200 most ideal response length*\n",
    "\n",
    "**Runtime:**\n",
    "- For GEITje: minor difference in runtime. From shortest to longest runtime: 100 (=29.1sec), 100/100 (=30.8sec), 200 (=32.8sec). \n",
    "- For Llama:  Big difference between runtime. From shortest to longest runtime: 200 (=120sec), 100 (=127.5sec), 100/100 (=149.2sec). \n",
    "- For Mistral: quite a difference between runtime, more than Geitje, less than Llama. From shortest to longest runtime: 200 (=59.8sec), 100 (=63sec), 100/100 (=74.2sec). \n",
    "\n",
    "Difference between 100/100 and other truncation is biggest, 100/100 adds more runtime. \n",
    "\n",
    "*Conclusion: 200 most optimal runtime, except for GEITje, adds 3 seconds for each doc compared to optimal option (100).*\n",
    "\n",
    "**Errors:**\n",
    "- For GEITje: Big difference in how many errors. From least to most: 100/100 (=33 of which 39% contains correct label), 200(=83 of which 46% contains correct label), 100 (=118 of which 71% contains correct label). For all truncation methods, geitje has the most NoPredictionFormat errors. \n",
    "- For Llama: Quite a difference in how many errors, less than GEITje. From least to most: 100/100 (=107 of which 23% contains correct label), 100(=135 of which 16% contains correct label), 200 (=141 of which 22% contains correct label). For all truncation methods, llama has the most NoPredictionInOutput errors. \n",
    "- For Mistral: Quite a difference in how many errors, less than GEITje, more than Llama. From least to most: 100/100 (=78 of which 41% contains correct label), 100(=100 of which 23% contains correct label), 200 (=131 of which 34% contains correct label). Does not seem to be a pattern in which prediction error.\n",
    "\n",
    "*Conclusion: 100/100 best for less errors.*\n",
    "\n",
    "**Mistakes:**\n",
    "- For GEITje: Minor difference between 100/100 (33.9%) and 200(34.5%), bigger diffence with 100 (42.4%).\n",
    "- For Llama: Small difference between 200 (52.5%) and the other truncations (100/100=57.5%; 100=57.7%).\n",
    "- For Mistral: Small difference between all truncations. From least to most mistakes: 100 (=38.8%), 100/100 (=39.5%), 200 (=41.7%).\n",
    "\n",
    "*Conclusion: each model different optimal truncation.*\n",
    "\n",
    "\n",
    "**Experiment Conclusion**: Only looking at accuracy, then each model has optimal option. However, 200 is the best option, when looking at mistakes and runtime. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
